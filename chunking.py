#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Standalone script ƒë·ªÉ chunk t·∫•t c·∫£ vƒÉn b·∫£n lu·∫≠t th√†nh chunks
T∆∞∆°ng t·ª± nh∆∞ find_law_files.py nh∆∞ng cho vi·ªác chunking
"""

import os
import json
import sys
import argparse
from pathlib import Path
from datetime import datetime

# For AI functionality
try:
    from dotenv import load_dotenv
    DOTENV_AVAILABLE = True
except ImportError:
    DOTENV_AVAILABLE = False

# Import t·ª´ modules
from modules.chunking import read_docx, chunk_law_document, generate_law_id
from modules.data_loader import save_chunks_to_json

# ====== C·∫§U H√åNH AGENT ======
GEMINI_MODEL_NAME = "gemini-1.5-flash"  # c·ªë ƒë·ªãnh theo y√™u c·∫ßu

# ===================== GEMINI EVALUATION AGENT =====================

GEMINI_PROMPT = """B·∫°n l√† chuy√™n gia ph√°p ƒëi·ªÉn ho√° & ki·ªÉm th·ª≠ d·ªØ li·ªáu lu·∫≠t.
T√¥i g·ª≠i cho b·∫°n:
1) Summary th·ªëng k√™ & c·∫£nh b√°o t·ª´ b·ªô chunking.
2) M·ªôt s·ªë "excerpts" (tr√≠ch ƒëo·∫°n nguy√™n vƒÉn) ƒë·∫°i di·ªán.
3) Danh m·ª•c chunks (id + metadata + content r√∫t g·ªçn).

Nhi·ªám v·ª•:
- PH√ÅT HI·ªÜN B·∫§T TH∆Ø·ªúNG (anomalies) trong chunking, v√≠ d·ª•:
  * Sai th·ª© t·ª±/ch∆∞a "strict" (nh·∫£y c√≥c Ch∆∞∆°ng/ƒêi·ªÅu/Kho·∫£n/ƒêi·ªÉm).
  * Nh·∫≠n di·ªán nh·∫ßm "Kho·∫£n" (kh√¥ng ph·∫£i d·∫°ng `1.`) ho·∫∑c "ƒêi·ªÉm" (kh√¥ng ph·∫£i `a)`).
  * Kh√¥ng ti√™m intro kho·∫£n v√†o ƒëi·ªÉm khi ƒë√£ c√≥ chu·ªói ƒëi·ªÉm.
  * Thi·∫øu/b·ªè s√≥t n·ªôi dung so v·ªõi excerpts.
  * Metadata kh√¥ng kh·ªõp: article_no/clause_no/point_letter/exact_citation.
  * ƒê√≥ng m·ªü chu·ªói ƒëi·ªÉm sai (b·∫Øt ƒë·∫ßu kh√¥ng ph·∫£i "a)", ch√®n n·ªôi dung th∆∞·ªùng v√†o gi·ªØa).
  * N·ªôi dung "ƒêi·ªÅu" kh√¥ng c√≥ kho·∫£n nh∆∞ng kh√¥ng sinh chunk intro.
- G·ª¢I √ù S·ª¨A: ch·ªâ r√µ v·ªã tr√≠ (id / exact_citation), m√¥ t·∫£ v·∫•n ƒë·ªÅ, c√°ch kh·∫Øc ph·ª•c.
- N·∫øu KH√îNG th·∫•y v·∫•n ƒë·ªÅ, x√°c nh·∫≠n "ok" v√† n√™u ng·∫Øn g·ªçn c∆° s·ªü k·∫øt lu·∫≠n.

H√£y TR·∫¢ L·ªúI **CH·ªà** ·ªü d·∫°ng JSON theo schema:
{
  "status": "ok" | "issues_found",
  "confidence": 0.0-1.0,
  "issues": [
    {
      "id": "chu·ªói id chunk ho·∫∑c m√¥ t·∫£ v·ªã tr√≠",
      "citation": "ƒêi·ªÅu ... kho·∫£n ... ƒëi·ªÉm ...",
      "severity": "low|medium|high",
      "category": "ordering|regex|metadata|omission|points_chain|format|other",
      "message": "M√¥ t·∫£ ng·∫Øn g·ªçn v·∫•n ƒë·ªÅ",
      "suggestion": "C√°ch s·ª≠a ng·∫Øn g·ªçn"
    }
  ],
  "notes": "Ghi ch√∫ ng·∫Øn (tu·ª≥ ch·ªçn)"
}
Tr·∫£ JSON h·ª£p l·ªá. Kh√¥ng gi·∫£i th√≠ch ngo√†i JSON.
"""

def _shorten_text(s: str, max_len: int = 500) -> str:
    if len(s) <= max_len:
        return s
    head = s[: max_len // 2].rstrip()
    tail = s[- max_len // 2 :].lstrip()
    return head + "\n...\n" + tail

def build_review_payload(chunks: list, summary: dict, raw_texts: list, sample_excerpts_chars: int = 2000, max_chunks_sample: int = 50):
    """
    X√¢y d·ª±ng payload cho AI review v·ªõi sampling th√¥ng minh ƒë·ªÉ tr√°nh v∆∞·ª£t token limit.

    Args:
        chunks: Danh s√°ch t·∫•t c·∫£ chunks
        summary: Th·ªëng k√™ t·ªïng quan
        raw_texts: List c√°c raw text t·ª´ files m·∫´u
        sample_excerpts_chars: T·ªïng k√Ω t·ª± excerpts (chia ƒë·ªÅu cho c√°c files)
        max_chunks_sample: S·ªë l∆∞·ª£ng chunks t·ªëi ƒëa ƒë·ªÉ sample
    """

    # 1. Sample excerpts t·ª´ raw texts (chia ƒë·ªÅu chars cho t·ª´ng file)
    excerpts_per_file = sample_excerpts_chars // max(len(raw_texts), 1)
    excerpts_list = []

    for i, raw_text in enumerate(raw_texts[:3]):  # T·ªëi ƒëa 3 files
        if len(raw_text) <= excerpts_per_file:
            excerpts_list.append(f"File {i+1}:\n{raw_text}")
        else:
            # Sample 3 ph·∫ßn: ƒë·∫ßu, gi·ªØa, cu·ªëi
            k = excerpts_per_file // 3
            n = len(raw_text)
            excerpt = raw_text[:k] + "\n...\n" + raw_text[n//2 - k//2 : n//2 + k//2] + "\n...\n" + raw_text[-k:]
            excerpts_list.append(f"File {i+1}:\n{excerpt}")

    combined_excerpts = "\n\n".join(excerpts_list)

    # 2. Sample chunks th√¥ng minh (∆∞u ti√™n chunks c√≥ v·∫•n ƒë·ªÅ ti·ªÅm ·∫©n)
    def lite(c):
        return {
            "id": c.get("id"),
            "metadata": c.get("metadata"),
            "content_preview": _shorten_text(c.get("content",""), 500)  # Gi·∫£m t·ª´ 700 xu·ªëng 500
        }

    # Sampling strategy: l·∫•y mix c·ªßa c√°c lo·∫°i chunks
    sampled_chunks = []
    article_chunks = []
    clause_chunks = []
    point_chunks = []

    for c in chunks:
        metadata = c.get('metadata', {})
        if metadata.get('point_letter'):
            point_chunks.append(c)
        elif metadata.get('clause_no'):
            clause_chunks.append(c)
        elif metadata.get('article_no'):
            article_chunks.append(c)

    # Sample ƒë·ªÅu t·ª´ m·ªói lo·∫°i (t·ªëi ƒëa max_chunks_sample)
    samples_per_type = max_chunks_sample // 3

    import random
    sampled_chunks.extend(random.sample(article_chunks, min(samples_per_type, len(article_chunks))))
    sampled_chunks.extend(random.sample(clause_chunks, min(samples_per_type, len(clause_chunks))))
    sampled_chunks.extend(random.sample(point_chunks, min(samples_per_type, len(point_chunks))))

    # N·∫øu v·∫´n ch∆∞a ƒë·ªß, th√™m random t·ª´ t·∫•t c·∫£
    remaining = max_chunks_sample - len(sampled_chunks)
    if remaining > 0:
        other_chunks = [c for c in chunks if c not in sampled_chunks]
        sampled_chunks.extend(random.sample(other_chunks, min(remaining, len(other_chunks))))

    # Shuffle ƒë·ªÉ tr√°nh bias
    random.shuffle(sampled_chunks)
    chunks_lite = [lite(c) for c in sampled_chunks]

    return {
        "summary": summary,
        "excerpts": combined_excerpts,
        "chunks_preview": chunks_lite,
        "note": f"Sampled {len(chunks_lite)}/{len(chunks)} chunks from {len(raw_texts)} files for AI review"
    }

def call_gemini_review(payload: dict, api_key: str = None) -> dict:
    """
    G·ªçi Gemini (model c·ªë ƒë·ªãnh GEMINI_MODEL_NAME) v√† √©p tr·∫£ JSON.
    API key l·∫•y t·ª´ .env (GEMINI_API_KEY).
    """
    if api_key is None:
        api_key = os.getenv("GEMINI_API_KEY", "").strip()
    if not api_key:
        raise RuntimeError("Thi·∫øu GEMINI_API_KEY trong m√¥i tr∆∞·ªùng (.env).")

    # import t·∫°i ch·ªó, ch·ªâ khi --AI b·∫≠t
    import google.generativeai as genai
    genai.configure(api_key=api_key)

    generation_config = {
        "temperature": 0.2,
        "top_p": 0.9,
        "top_k": 40,
        "response_mime_type": "application/json",
    }
    model = genai.GenerativeModel(GEMINI_MODEL_NAME, generation_config=generation_config)
    prompt_parts = [
        {"role": "user", "parts": [{"text": GEMINI_PROMPT}]},
        {"role": "user", "parts": [{"text": json.dumps(payload, ensure_ascii=False)}]},
    ]
    resp = model.generate_content(prompt_parts)
    raw = getattr(resp, "text", None) or (
        resp.candidates and resp.candidates[0].content.parts[0].text
    )
    if not raw:
        raise RuntimeError("Gemini kh√¥ng tr·∫£ ra n·ªôi dung.")
    try:
        data = json.loads(raw)
        if not isinstance(data, dict):
            raise ValueError("JSON kh√¥ng ph·∫£i object")
        data.setdefault("status", "issues_found")
        data.setdefault("issues", [])
        data.setdefault("confidence", 0.0)
        return data
    except Exception as e:
        return {
            "status": "issues_found",
            "confidence": 0.0,
            "issues": [{
                "id": "PARSER",
                "citation": "",
                "severity": "high",
                "category": "other",
                "message": f"Kh√¥ng parse ƒë∆∞·ª£c JSON t·ª´ Gemini: {e}",
                "suggestion": "Ch·∫°y l·∫°i ho·∫∑c gi·∫£m excerpts."
            }],
            "notes": raw[:2000]
        }

def load_law_file_paths(json_path="data_files/law_file_paths.json"):
    """Load danh s√°ch file lu·∫≠t t·ª´ JSON"""
    if not os.path.exists(json_path):
        print(f"‚ùå Law file paths JSON not found: {json_path}")
        print("   Please run find_law_files.py first")
        return []

    print(f"üìñ Loading law file paths from: {json_path}")

    try:
        with open(json_path, 'r', encoding='utf-8') as f:
            law_file_paths = json.load(f)
        print(f"‚úÖ Loaded {len(law_file_paths)} law file paths")
        return law_file_paths
    except Exception as e:
        print(f"‚ùå Error loading {json_path}: {e}")
        return []

def chunk_all_law_documents(law_file_paths):
    """Chunk t·∫•t c·∫£ vƒÉn b·∫£n lu·∫≠t t·ª´ danh s√°ch file paths"""

    print(f"\nüîç Starting to chunk {len(law_file_paths)} law documents...")

    all_chunks = []
    successful_chunks = 0
    failed_files = 0
    warnings = []
    articles_count = 0
    article_intro_count = 0
    clauses_count = 0
    points_count = 0

    for i, file_info in enumerate(law_file_paths):
        file_path = file_info['path']
        category = file_info['category']
        file_name = file_info['file_name']

        print(f"\nüìÑ [{i+1}/{len(law_file_paths)}] Processing: {file_name}")
        print(f"   Category: {category}")
        print(f"   Path: {file_path}")

        if not os.path.exists(file_path):
            print(f"   ‚ùå File not found: {file_path}")
            failed_files += 1
            continue

        try:
            # B∆∞·ªõc 1: ƒê·ªçc file
            print("   üìñ Reading file...")
            raw_text = read_docx(file_path)

            if not raw_text or len(raw_text.strip()) < 100:
                print(f"   ‚ö†Ô∏è File seems empty or too short: {len(raw_text)} characters")
                print("   ‚è≠Ô∏è Skipping file (cannot read content)")
                failed_files += 1
                continue

            # B∆∞·ªõc 2: T·∫°o law_id t·ª´ t√™n file
            law_id = generate_law_id(file_name)
            print(f"   üìã Generated law_id: {law_id}")

            # B∆∞·ªõc 3: Chunk document
            print("   üî® Chunking document...")
            chunks = chunk_law_document(
                text=raw_text,
                law_id=law_id,
                law_no="",
                law_title=file_name
            )

            if not chunks:
                print("   ‚ö†Ô∏è No chunks created from file")
                failed_files += 1
                continue

            # B∆∞·ªõc 4: Th√™m metadata v√† chu·∫©n b·ªã cho save
            print("   üóÇÔ∏è Preparing chunks...")
            for j, chunk in enumerate(chunks):
                # Th√™m th√¥ng tin v·ªÅ file g·ªëc v√†o metadata
                chunk_metadata = chunk.get('metadata', {}).copy()
                chunk_metadata.update({
                    'source_file': file_path,
                    'source_category': category,
                    'source_file_name': file_name,
                    'chunk_index': j
                })

                # T·∫°o document theo format chunks.json
                all_chunks.append({
                    'id': chunk['id'],
                    'content': chunk['content'],
                    'metadata': chunk_metadata
                })

                # ƒê·∫øm lo·∫°i chunk
                metadata = chunk.get('metadata', {})
                if metadata.get('point_letter'):
                    points_count += 1
                elif metadata.get('clause_no'):
                    clauses_count += 1
                elif metadata.get('article_no') and not metadata.get('clause_no'):
                    if 'intro' in chunk['id'].lower():
                        article_intro_count += 1
                    else:
                        articles_count += 1

            print(f"   ‚úÖ Successfully processed {len(chunks)} chunks")
            successful_chunks += len(chunks)

        except Exception as e:
            print(f"   ‚ùå Error processing file: {e}")
            failed_files += 1
            continue

    print(f"\nüìä Chunking Summary:")
    print(f"   ‚úÖ Successfully chunked: {len(law_file_paths) - failed_files} files")
    print(f"   ‚ùå Failed to process: {failed_files} files")
    print(f"   üìÑ Total chunks created: {len(all_chunks)}")

    if all_chunks:
        print(f"   üìä Average chunk length: {sum(len(c['content']) for c in all_chunks) // len(all_chunks):.0f} characters")

        # Th·ªëng k√™ theo category
        category_counts = {}
        for chunk in all_chunks:
            category = chunk.get('metadata', {}).get('source_category', 'Unknown')
            category_counts[category] = category_counts.get(category, 0) + 1

        print("\nüìà Chunks distribution by category:")
        for category, count in category_counts.items():
            print(f"   - {category}: {count} chunks")

    # T·∫°o summary dictionary
    summary = {
        "chapters_seen": [],  # Kh√¥ng c√≥ th√¥ng tin chapters trong batch processing
        "articles": articles_count,
        "article_intro": article_intro_count,
        "clauses": clauses_count,
        "points": points_count,
        "citations": [],  # Kh√¥ng c√≥ citations trong batch processing
        "warnings": warnings,
        "halted_reason": None,
        "total_chunks": len(all_chunks)
    }

    return all_chunks, summary

def main():
    """Main function ƒë·ªÉ ch·∫°y chunking standalone"""

    # Parse arguments
    parser = argparse.ArgumentParser(
        description="Chunk lu·∫≠t Vi·ªát Nam + (tu·ª≥ ch·ªçn) g·ªçi Gemini ƒë·ªÉ th·∫©m ƒë·ªãnh (--AI).\n"
        "AI review s·ª≠ d·ª•ng sampling th√¥ng minh ƒë·ªÉ tr√°nh v∆∞·ª£t token limit.",
        formatter_class=argparse.RawDescriptionHelpFormatter
    )
    parser.add_argument("--AI", action="store_true", help="B·∫¨T g·ªçi Gemini (m·∫∑c ƒë·ªãnh KH√îNG g·ªçi).")
    parser.add_argument("--sample-excerpts", type=int, default=2000, help="T·ªïng k√Ω t·ª± excerpts t·ª´ T·∫§T C·∫¢ files g·ª≠i Gemini (m·∫∑c ƒë·ªãnh 2000, gi·∫£m t·ª´ 8000).")
    parser.add_argument("--max-chunks-sample", type=int, default=50, help="S·ªë chunks t·ªëi ƒëa sample ƒë·ªÉ AI review (m·∫∑c ƒë·ªãnh 50, gi·∫£m t·ª´ 1200).")
    parser.add_argument("--max-files-sample", type=int, default=2, help="S·ªë files t·ªëi ƒëa l·∫•y raw text ƒë·ªÉ AI review (m·∫∑c ƒë·ªãnh 2, gi·∫£m t·ª´ 5).")
    parser.add_argument("--strict-ok-only", action="store_true", help="Ch·ªâ ghi chunks n·∫øu Gemini tr·∫£ 'ok' (ch·ªâ khi --AI).")
    parser.add_argument("--api-key", help="Gemini API key (ho·∫∑c set GEMINI_API_KEY env var).")

    args = parser.parse_args()

    print("=" * 80)
    print("üî® VIETNAMESE LAW DOCUMENT CHUNKING")
    if args.AI:
        print("ü§ñ WITH AI REVIEW ENABLED")
    print("=" * 80)

    # Load .env if available
    if DOTENV_AVAILABLE:
        load_dotenv()

    # 1. Load danh s√°ch file lu·∫≠t
    law_file_paths = load_law_file_paths()

    if not law_file_paths:
        print("‚ùå Cannot proceed without law file paths!")
        print("   Run find_law_files.py first")
        return

    # 2. Chunk t·∫•t c·∫£ documents
    all_chunks, summary = chunk_all_law_documents(law_file_paths)

    # Thu th·∫≠p raw text cho AI review (n·∫øu b·∫≠t AI)
    raw_texts = []
    if args.AI:
        print(f"\nüìñ Collecting raw texts for AI review (max {args.max_files_sample} files)...")
        files_sampled = 0
        for file_info in law_file_paths:
            if files_sampled >= args.max_files_sample:
                break
            file_path = file_info['path']
            if os.path.exists(file_path):
                try:
                    raw_text = read_docx(file_path)
                    if raw_text and len(raw_text.strip()) > 100:
                        # Gi·ªõi h·∫°n 5k chars m·ªói file thay v√¨ 10k
                        raw_texts.append(raw_text[:5000])
                        files_sampled += 1
                        print(f"   üìÑ Sampled file {files_sampled}: {file_info['file_name']} ({len(raw_text)} chars)")
                except Exception as e:
                    print(f"   ‚ö†Ô∏è Failed to read {file_info['file_name']}: {e}")
                    continue
        print(f"   ‚úÖ Collected raw text excerpts from {len(raw_texts)} files (total ~{sum(len(t) for t in raw_texts)} chars)")

    if not all_chunks:
        print("‚ùå No chunks were created!")
        return

    # 3. Save chunks ra JSON
    # T·∫°o t√™n file theo format: chunk_Gi·ªùph√∫tgi√¢y_ng√†yth√°ngnƒÉm(2 s·ªë cu·ªëi)
    now = datetime.now()
    timestamp = now.strftime("%H%M%S_%d%m%y")  # HHMMSS_DDMMYY
    chunks_json_path = f"data/chunk_{timestamp}.json"
    print(f"\nüíæ Saving {len(all_chunks)} chunks to {chunks_json_path}...")

    try:
        # T·∫°o th∆∞ m·ª•c data n·∫øu ch∆∞a c√≥
        Path("data").mkdir(exist_ok=True)

        save_chunks_to_json(all_chunks, chunks_json_path)
        print(f"‚úÖ Successfully saved {len(all_chunks)} chunks to {chunks_json_path}")
        print("   (Similar to hn2014_chunks.json format)")

    except Exception as e:
        print(f"‚ùå Error saving chunks: {e}")
        return

    # N·∫øu KH√îNG b·∫≠t --AI: k·∫øt th√∫c t·∫°i ƒë√¢y
    if not args.AI:
        print(f"\nüéâ Chunking completed successfully!")
        print(f"   üìÑ Total chunks: {len(all_chunks)}")
        print(f"   üíæ Output file: {chunks_json_path}")
        print(f"\nNext: Run main_refactored.py to evaluate with chunks from {chunks_json_path}")
        return

    # ===== Khi --AI b·∫≠t: g·ªçi Gemini ƒë·ªÉ th·∫©m ƒë·ªãnh =====
    payload = build_review_payload(
        chunks=all_chunks,
        summary=summary,
        raw_texts=raw_texts,
        sample_excerpts_chars=args.sample_excerpts,
        max_chunks_sample=args.max_chunks_sample
    )

    print(f"\nü§ñ G·ªåI GEMINI (gemini-1.5-flash) ƒê√ÅNH GI√Å CHUNKING =====")
    print(f"   üìä Sending: {len(payload['chunks_preview'])} sampled chunks + {len(raw_texts)} file excerpts")
    print(f"   üíæ Payload size estimate: ~{len(str(payload)) // 1000}KB")
    try:
        review = call_gemini_review(payload, args.api_key)
    except Exception as e:
        print(f"‚ùå L·ªói g·ªçi Gemini: {e}", file=sys.stderr)
        if args.strict_ok_only:
            print("‚ùå --strict-ok-only: Kh√¥ng th·ªÉ th·∫©m ƒë·ªãnh, d·ª´ng l·∫°i.")
            return
        else:
            print(f"‚ö†Ô∏è Kh√¥ng th·∫©m ƒë·ªãnh ƒë∆∞·ª£c. ƒê√£ l∆∞u chunks v√†o: {chunks_json_path}")
            print(f"\nüéâ Chunking completed (without AI review)!")
            print(f"   üìÑ Total chunks: {len(all_chunks)}")
            print(f"   üíæ Output file: {chunks_json_path}")
            return

    status = review.get("status", "issues_found")
    confidence = review.get("confidence", 0.0)
    issues = review.get("issues", []) or []
    notes = review.get("notes", "")

    print(f"- Tr·∫°ng th√°i: {status} | Confidence: {confidence:.2f}")
    if notes:
        print(f"- Ghi ch√∫: {notes[:4000]}")

    if issues:
        print("\n‚ö†Ô∏è B√ÅO C√ÅO V·∫§N ƒê·ªÄ T·ª™ AI:")
        for i, it in enumerate(issues, 1):
            print(f"{i:02d}. [{it.get('severity','?')}] ({it.get('category','other')}) "
                  f"{it.get('citation') or it.get('id') or ''}")
            print(f"    - {it.get('message','(no message)')}")
            if it.get('suggestion'):
                print(f"    ‚Üí G·ª£i √Ω: {it['suggestion']}")
        issues_path = chunks_json_path.replace('.json', '.issues.json')
        with open(issues_path, 'w', encoding='utf-8') as f:
            json.dump(review, f, ensure_ascii=False, indent=2)
        print(f"\nüíæ ƒê√£ ghi b√°o c√°o v·∫•n ƒë·ªÅ: {issues_path}")

        if args.strict_ok_only:
            print("‚ùå --strict-ok-only: Kh√¥ng ghi chunks v√¨ AI ch∆∞a x√°c nh·∫≠n 'ok'.")
            return

    print(f"\nüéâ Chunking completed successfully!")
    print(f"   üìÑ Total chunks: {len(all_chunks)}")
    print(f"   üíæ Output file: {chunks_json_path}")
    if args.AI and status == "ok":
        print("   ‚úÖ AI ƒë√£ x√°c nh·∫≠n: Chunking OK!")
    print(f"\nNext: Run main_refactored.py to evaluate with chunks from {chunks_json_path}")

if __name__ == "__main__":
    main()
