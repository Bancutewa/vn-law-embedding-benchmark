#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Standalone script Ä‘á»ƒ chunk táº¥t cáº£ vÄƒn báº£n luáº­t thÃ nh chunks
TÆ°Æ¡ng tá»± nhÆ° find_law_files.py nhÆ°ng cho viá»‡c chunking
"""

import os
import json
import sys
import argparse
from pathlib import Path
from datetime import datetime

# For AI functionality
try:
    from dotenv import load_dotenv
    DOTENV_AVAILABLE = True
except ImportError:
    DOTENV_AVAILABLE = False

# Import tá»« modules
from modules.chunking import read_docx, chunk_law_document, generate_law_id
from modules.data_loader import save_chunks_to_json

# ====== Cáº¤U HÃŒNH AGENT ======
GEMINI_MODEL_NAME = "gemini-1.5-flash"  # cá»‘ Ä‘á»‹nh theo yÃªu cáº§u

# ===================== GEMINI EVALUATION AGENT =====================

GEMINI_PROMPT = """Báº¡n lÃ  chuyÃªn gia phÃ¡p Ä‘iá»ƒn hoÃ¡ & kiá»ƒm thá»­ dá»¯ liá»‡u luáº­t.
TÃ´i gá»­i cho báº¡n:
1) Summary thá»‘ng kÃª & cáº£nh bÃ¡o tá»« bá»™ chunking.
2) Má»™t sá»‘ "excerpts" (trÃ­ch Ä‘oáº¡n nguyÃªn vÄƒn) Ä‘áº¡i diá»‡n.
3) Danh má»¥c chunks (id + metadata + content rÃºt gá»n).

Nhiá»‡m vá»¥:
- PHÃT HIá»†N Báº¤T THÆ¯á»œNG (anomalies) trong chunking, vÃ­ dá»¥:
  * Sai thá»© tá»±/chÆ°a "strict" (nháº£y cÃ³c ChÆ°Æ¡ng/Äiá»u/Khoáº£n/Äiá»ƒm).
  * Nháº­n diá»‡n nháº§m "Khoáº£n" (khÃ´ng pháº£i dáº¡ng `1.`) hoáº·c "Äiá»ƒm" (khÃ´ng pháº£i `a)`).
  * KhÃ´ng tiÃªm intro khoáº£n vÃ o Ä‘iá»ƒm khi Ä‘Ã£ cÃ³ chuá»—i Ä‘iá»ƒm.
  * Thiáº¿u/bá» sÃ³t ná»™i dung so vá»›i excerpts.
  * Metadata khÃ´ng khá»›p: article_no/clause_no/point_letter/exact_citation.
  * ÄÃ³ng má»Ÿ chuá»—i Ä‘iá»ƒm sai (báº¯t Ä‘áº§u khÃ´ng pháº£i "a)", chÃ¨n ná»™i dung thÆ°á»ng vÃ o giá»¯a).
  * Ná»™i dung "Äiá»u" khÃ´ng cÃ³ khoáº£n nhÆ°ng khÃ´ng sinh chunk intro.
- Gá»¢I Ã Sá»¬A: chá»‰ rÃµ vá»‹ trÃ­ (id / exact_citation), mÃ´ táº£ váº¥n Ä‘á», cÃ¡ch kháº¯c phá»¥c.
- Náº¿u KHÃ”NG tháº¥y váº¥n Ä‘á», xÃ¡c nháº­n "ok" vÃ  nÃªu ngáº¯n gá»n cÆ¡ sá»Ÿ káº¿t luáº­n.

HÃ£y TRáº¢ Lá»œI **CHá»ˆ** á»Ÿ dáº¡ng JSON theo schema:
{
  "status": "ok" | "issues_found",
  "confidence": 0.0-1.0,
  "issues": [
    {
      "id": "chuá»—i id chunk hoáº·c mÃ´ táº£ vá»‹ trÃ­",
      "citation": "Äiá»u ... khoáº£n ... Ä‘iá»ƒm ...",
      "severity": "low|medium|high",
      "category": "ordering|regex|metadata|omission|points_chain|format|other",
      "message": "MÃ´ táº£ ngáº¯n gá»n váº¥n Ä‘á»",
      "suggestion": "CÃ¡ch sá»­a ngáº¯n gá»n"
    }
  ],
  "notes": "Ghi chÃº ngáº¯n (tuá»³ chá»n)"
}
Tráº£ JSON há»£p lá»‡. KhÃ´ng giáº£i thÃ­ch ngoÃ i JSON.
"""

def _shorten_text(s: str, max_len: int = 500) -> str:
    if len(s) <= max_len:
        return s
    head = s[: max_len // 2].rstrip()
    tail = s[- max_len // 2 :].lstrip()
    return head + "\n...\n" + tail

def build_review_payload(chunks: list, summary: dict, raw_texts: list, sample_excerpts_chars: int = 2000, max_chunks_sample: int = 50):
    """
    XÃ¢y dá»±ng payload cho AI review vá»›i sampling thÃ´ng minh Ä‘á»ƒ trÃ¡nh vÆ°á»£t token limit.

    Args:
        chunks: Danh sÃ¡ch táº¥t cáº£ chunks
        summary: Thá»‘ng kÃª tá»•ng quan
        raw_texts: List cÃ¡c raw text tá»« files máº«u
        sample_excerpts_chars: Tá»•ng kÃ½ tá»± excerpts (chia Ä‘á»u cho cÃ¡c files)
        max_chunks_sample: Sá»‘ lÆ°á»£ng chunks tá»‘i Ä‘a Ä‘á»ƒ sample
    """

    # 1. Sample excerpts tá»« raw texts (chia Ä‘á»u chars cho tá»«ng file)
    excerpts_per_file = sample_excerpts_chars // max(len(raw_texts), 1)
    excerpts_list = []

    for i, raw_text in enumerate(raw_texts[:3]):  # Tá»‘i Ä‘a 3 files
        if len(raw_text) <= excerpts_per_file:
            excerpts_list.append(f"File {i+1}:\n{raw_text}")
        else:
            # Sample 3 pháº§n: Ä‘áº§u, giá»¯a, cuá»‘i
            k = excerpts_per_file // 3
            n = len(raw_text)
            excerpt = raw_text[:k] + "\n...\n" + raw_text[n//2 - k//2 : n//2 + k//2] + "\n...\n" + raw_text[-k:]
            excerpts_list.append(f"File {i+1}:\n{excerpt}")

    combined_excerpts = "\n\n".join(excerpts_list)

    # 2. Sample chunks thÃ´ng minh (Æ°u tiÃªn chunks cÃ³ váº¥n Ä‘á» tiá»m áº©n)
    def lite(c):
        return {
            "id": c.get("id"),
            "metadata": c.get("metadata"),
            "content_preview": _shorten_text(c.get("content",""), 500)  # Giáº£m tá»« 700 xuá»‘ng 500
        }

    # Sampling strategy: láº¥y mix cá»§a cÃ¡c loáº¡i chunks
    sampled_chunks = []
    article_chunks = []
    clause_chunks = []
    point_chunks = []

    for c in chunks:
        metadata = c.get('metadata', {})
        if metadata.get('point_letter'):
            point_chunks.append(c)
        elif metadata.get('clause_no'):
            clause_chunks.append(c)
        elif metadata.get('article_no'):
            article_chunks.append(c)

    # Sample Ä‘á»u tá»« má»—i loáº¡i (tá»‘i Ä‘a max_chunks_sample)
    samples_per_type = max_chunks_sample // 3

    import random
    sampled_chunks.extend(random.sample(article_chunks, min(samples_per_type, len(article_chunks))))
    sampled_chunks.extend(random.sample(clause_chunks, min(samples_per_type, len(clause_chunks))))
    sampled_chunks.extend(random.sample(point_chunks, min(samples_per_type, len(point_chunks))))

    # Náº¿u váº«n chÆ°a Ä‘á»§, thÃªm random tá»« táº¥t cáº£
    remaining = max_chunks_sample - len(sampled_chunks)
    if remaining > 0:
        other_chunks = [c for c in chunks if c not in sampled_chunks]
        sampled_chunks.extend(random.sample(other_chunks, min(remaining, len(other_chunks))))

    # Shuffle Ä‘á»ƒ trÃ¡nh bias
    random.shuffle(sampled_chunks)
    chunks_lite = [lite(c) for c in sampled_chunks]

    return {
        "summary": summary,
        "excerpts": combined_excerpts,
        "chunks_preview": chunks_lite,
        "note": f"Sampled {len(chunks_lite)}/{len(chunks)} chunks from {len(raw_texts)} files for AI review"
    }

def call_gemini_review(payload: dict, api_key: str = None) -> dict:
    """
    Gá»i Gemini (model cá»‘ Ä‘á»‹nh GEMINI_MODEL_NAME) vÃ  Ã©p tráº£ JSON.
    API key láº¥y tá»« .env (GEMINI_API_KEY).
    """
    if api_key is None:
        api_key = os.getenv("GEMINI_API_KEY", "").strip()
    if not api_key:
        raise RuntimeError("Thiáº¿u GEMINI_API_KEY trong mÃ´i trÆ°á»ng (.env).")

    # import táº¡i chá»—, chá»‰ khi --AI báº­t
    import google.generativeai as genai
    genai.configure(api_key=api_key)

    generation_config = {
        "temperature": 0.2,
        "top_p": 0.9,
        "top_k": 40,
        "response_mime_type": "application/json",
    }
    model = genai.GenerativeModel(GEMINI_MODEL_NAME, generation_config=generation_config)
    prompt_parts = [
        {"role": "user", "parts": [{"text": GEMINI_PROMPT}]},
        {"role": "user", "parts": [{"text": json.dumps(payload, ensure_ascii=False)}]},
    ]
    resp = model.generate_content(prompt_parts)
    raw = getattr(resp, "text", None) or (
        resp.candidates and resp.candidates[0].content.parts[0].text
    )
    if not raw:
        raise RuntimeError("Gemini khÃ´ng tráº£ ra ná»™i dung.")
    try:
        data = json.loads(raw)
        if not isinstance(data, dict):
            raise ValueError("JSON khÃ´ng pháº£i object")
        data.setdefault("status", "issues_found")
        data.setdefault("issues", [])
        data.setdefault("confidence", 0.0)
        return data
    except Exception as e:
        return {
            "status": "issues_found",
            "confidence": 0.0,
            "issues": [{
                "id": "PARSER",
                "citation": "",
                "severity": "high",
                "category": "other",
                "message": f"KhÃ´ng parse Ä‘Æ°á»£c JSON tá»« Gemini: {e}",
                "suggestion": "Cháº¡y láº¡i hoáº·c giáº£m excerpts."
            }],
            "notes": raw[:2000]
        }

def load_law_file_paths(json_path="data_files/law_file_paths.json"):
    """Load danh sÃ¡ch file luáº­t tá»« JSON"""
    if not os.path.exists(json_path):
        print(f"âŒ Law file paths JSON not found: {json_path}")
        print("   Please run find_law_files.py first")
        return []

    print(f"ğŸ“– Loading law file paths from: {json_path}")

    try:
        with open(json_path, 'r', encoding='utf-8') as f:
            law_file_paths = json.load(f)
        print(f"âœ… Loaded {len(law_file_paths)} law file paths")
        return law_file_paths
    except Exception as e:
        print(f"âŒ Error loading {json_path}: {e}")
        return []

def chunk_all_law_documents(law_file_paths):
    """Chunk táº¥t cáº£ vÄƒn báº£n luáº­t tá»« danh sÃ¡ch file paths"""

    print(f"\nğŸ” Starting to chunk {len(law_file_paths)} law documents...")

    all_chunks = []
    successful_chunks = 0
    failed_files = 0
    warnings = []
    articles_count = 0
    article_intro_count = 0
    clauses_count = 0
    points_count = 0

    for i, file_info in enumerate(law_file_paths):
        file_path = file_info['path']
        category = file_info['category']
        file_name = file_info['file_name']

        print(f"\nğŸ“„ [{i+1}/{len(law_file_paths)}] Processing: {file_name}")
        print(f"   Category: {category}")
        print(f"   Path: {file_path}")

        if not os.path.exists(file_path):
            print(f"   âŒ File not found: {file_path}")
            failed_files += 1
            continue

        try:
            # BÆ°á»›c 1: Äá»c file
            print("   ğŸ“– Reading file...")
            raw_text = read_docx(file_path)

            if not raw_text or len(raw_text.strip()) < 100:
                print(f"   âš ï¸ File seems empty or too short: {len(raw_text)} characters")
                print("   â­ï¸ Skipping file (cannot read content)")
                failed_files += 1
                continue

            # BÆ°á»›c 2: Táº¡o law_id tá»« tÃªn file
            law_id = generate_law_id(file_name)
            print(f"   ğŸ“‹ Generated law_id: {law_id}")

            # BÆ°á»›c 3: Chunk document
            print("   ğŸ”¨ Chunking document...")
            chunks = chunk_law_document(
                text=raw_text,
                law_id=law_id,
                law_no="",
                law_title=file_name
            )

            if not chunks:
                print("   âš ï¸ No chunks created from file")
                failed_files += 1
                continue

            # BÆ°á»›c 4: ThÃªm metadata vÃ  chuáº©n bá»‹ cho save
            print("   ğŸ—‚ï¸ Preparing chunks...")
            for j, chunk in enumerate(chunks):
                # ThÃªm thÃ´ng tin vá» file gá»‘c vÃ o metadata
                chunk_metadata = chunk.get('metadata', {}).copy()
                chunk_metadata.update({
                    'source_file': file_path,
                    'source_category': category,
                    'source_file_name': file_name,
                    'chunk_index': j
                })

                # Táº¡o document theo format chunks.json
                all_chunks.append({
                    'id': chunk['id'],
                    'content': chunk['content'],
                    'metadata': chunk_metadata
                })

                # Äáº¿m loáº¡i chunk
                metadata = chunk.get('metadata', {})
                if metadata.get('point_letter'):
                    points_count += 1
                elif metadata.get('clause_no'):
                    clauses_count += 1
                elif metadata.get('article_no') and not metadata.get('clause_no'):
                    if 'intro' in chunk['id'].lower():
                        article_intro_count += 1
                    else:
                        articles_count += 1

            print(f"   âœ… Successfully processed {len(chunks)} chunks")
            successful_chunks += len(chunks)

        except Exception as e:
            print(f"   âŒ Error processing file: {e}")
            failed_files += 1
            continue

    print(f"\nğŸ“Š Chunking Summary:")
    print(f"   âœ… Successfully chunked: {len(law_file_paths) - failed_files} files")
    print(f"   âŒ Failed to process: {failed_files} files")
    print(f"   ğŸ“„ Total chunks created: {len(all_chunks)}")

    if all_chunks:
        print(f"   ğŸ“Š Average chunk length: {sum(len(c['content']) for c in all_chunks) // len(all_chunks):.0f} characters")

        # Thá»‘ng kÃª theo category
        category_counts = {}
        for chunk in all_chunks:
            category = chunk.get('metadata', {}).get('source_category', 'Unknown')
            category_counts[category] = category_counts.get(category, 0) + 1

        print("\nğŸ“ˆ Chunks distribution by category:")
        for category, count in category_counts.items():
            print(f"   - {category}: {count} chunks")

    # Táº¡o summary dictionary
    summary = {
        "chapters_seen": [],  # KhÃ´ng cÃ³ thÃ´ng tin chapters trong batch processing
        "articles": articles_count,
        "article_intro": article_intro_count,
        "clauses": clauses_count,
        "points": points_count,
        "citations": [],  # KhÃ´ng cÃ³ citations trong batch processing
        "warnings": warnings,
        "halted_reason": None,
        "total_chunks": len(all_chunks)
    }

    return all_chunks, summary

def main():
    """Main function Ä‘á»ƒ cháº¡y chunking standalone"""

    # Parse arguments
    parser = argparse.ArgumentParser(
        description="Chunk luáº­t Viá»‡t Nam + (tuá»³ chá»n) gá»i Gemini Ä‘á»ƒ tháº©m Ä‘á»‹nh (--AI).\n"
        "AI review sá»­ dá»¥ng sampling thÃ´ng minh Ä‘á»ƒ trÃ¡nh vÆ°á»£t token limit.",
        formatter_class=argparse.RawDescriptionHelpFormatter
    )
    parser.add_argument("--AI", action="store_true", help="Báº¬T gá»i Gemini (máº·c Ä‘á»‹nh KHÃ”NG gá»i).")
    parser.add_argument("--sample-excerpts", type=int, default=2000, help="Tá»•ng kÃ½ tá»± excerpts tá»« Táº¤T Cáº¢ files gá»­i Gemini (máº·c Ä‘á»‹nh 2000, giáº£m tá»« 8000).")
    parser.add_argument("--max-chunks-sample", type=int, default=50, help="Sá»‘ chunks tá»‘i Ä‘a sample Ä‘á»ƒ AI review (máº·c Ä‘á»‹nh 50, giáº£m tá»« 1200).")
    parser.add_argument("--max-files-sample", type=int, default=2, help="Sá»‘ files tá»‘i Ä‘a láº¥y raw text Ä‘á»ƒ AI review (máº·c Ä‘á»‹nh 2, giáº£m tá»« 5).")
    parser.add_argument("--strict-ok-only", action="store_true", help="Chá»‰ ghi chunks náº¿u Gemini tráº£ 'ok' (chá»‰ khi --AI).")
    parser.add_argument("--api-key", help="Gemini API key (hoáº·c set GEMINI_API_KEY env var).")

    args = parser.parse_args()

    print("=" * 80)
    print("ğŸ”¨ VIETNAMESE LAW DOCUMENT CHUNKING")
    if args.AI:
        print("ğŸ¤– WITH AI REVIEW ENABLED")
    print("=" * 80)

    # Load .env if available
    if DOTENV_AVAILABLE:
        load_dotenv()

    # 1. Load danh sÃ¡ch file luáº­t
    law_file_paths = load_law_file_paths()

    if not law_file_paths:
        print("âŒ Cannot proceed without law file paths!")
        print("   Run find_law_files.py first")
        return

    # 2. Chunk táº¥t cáº£ documents
    all_chunks, summary = chunk_all_law_documents(law_file_paths)

    # Thu tháº­p raw text cho AI review (náº¿u báº­t AI)
    raw_texts = []
    if args.AI:
        print(f"\nğŸ“– Collecting raw texts for AI review (max {args.max_files_sample} files)...")
        files_sampled = 0
        for file_info in law_file_paths:
            if files_sampled >= args.max_files_sample:
                break
            file_path = file_info['path']
            if os.path.exists(file_path):
                try:
                    raw_text = read_docx(file_path)
                    if raw_text and len(raw_text.strip()) > 100:
                        # Giá»›i háº¡n 5k chars má»—i file thay vÃ¬ 10k
                        raw_texts.append(raw_text[:5000])
                        files_sampled += 1
                        print(f"   ğŸ“„ Sampled file {files_sampled}: {file_info['file_name']} ({len(raw_text)} chars)")
                except Exception as e:
                    print(f"   âš ï¸ Failed to read {file_info['file_name']}: {e}")
                    continue
        print(f"   âœ… Collected raw text excerpts from {len(raw_texts)} files (total ~{sum(len(t) for t in raw_texts)} chars)")

    if not all_chunks:
        print("âŒ No chunks were created!")
        return

    # 3. Save chunks ra JSON
    # Táº¡o tÃªn file theo format: chunk_Giá»phÃºtgiÃ¢y_ngÃ ythÃ¡ngnÄƒm(2 sá»‘ cuá»‘i)
    now = datetime.now()
    timestamp = now.strftime("%H%M%S_%d%m%y")  # HHMMSS_DDMMYY
    chunks_json_path = f"data/chunk_{timestamp}.json"
    print(f"\nğŸ’¾ Saving {len(all_chunks)} chunks to {chunks_json_path}...")

    try:
        # Táº¡o thÆ° má»¥c data náº¿u chÆ°a cÃ³
        Path("data").mkdir(exist_ok=True)

        save_chunks_to_json(all_chunks, chunks_json_path)
        print(f"âœ… Successfully saved {len(all_chunks)} chunks to {chunks_json_path}")
        print("   (Similar to hn2014_chunks.json format)")

    except Exception as e:
        print(f"âŒ Error saving chunks: {e}")
        return

    # Náº¿u KHÃ”NG báº­t --AI: káº¿t thÃºc táº¡i Ä‘Ã¢y
    if not args.AI:
        print(f"\nğŸ‰ Chunking completed successfully!")
        print(f"   ğŸ“„ Total chunks: {len(all_chunks)}")
        print(f"   ğŸ’¾ Output file: {chunks_json_path}")
        print(f"\nNext: Run main_refactored.py to evaluate with chunks from {chunks_json_path}")
        return

    # ===== Khi --AI báº­t: gá»i Gemini Ä‘á»ƒ tháº©m Ä‘á»‹nh =====
    payload = build_review_payload(
        chunks=all_chunks,
        summary=summary,
        raw_texts=raw_texts,
        sample_excerpts_chars=args.sample_excerpts,
        max_chunks_sample=args.max_chunks_sample
    )

    print(f"\nğŸ¤– Gá»ŒI GEMINI (gemini-1.5-flash) ÄÃNH GIÃ CHUNKING =====")
    print(f"   ğŸ“Š Sending: {len(payload['chunks_preview'])} sampled chunks + {len(raw_texts)} file excerpts")
    print(f"   ğŸ’¾ Payload size estimate: ~{len(str(payload)) // 1000}KB")
    try:
        review = call_gemini_review(payload, args.api_key)
    except Exception as e:
        print(f"âŒ Lá»—i gá»i Gemini: {e}", file=sys.stderr)
        if args.strict_ok_only:
            print("âŒ --strict-ok-only: KhÃ´ng thá»ƒ tháº©m Ä‘á»‹nh, dá»«ng láº¡i.")
            return
        else:
            print(f"âš ï¸ KhÃ´ng tháº©m Ä‘á»‹nh Ä‘Æ°á»£c. ÄÃ£ lÆ°u chunks vÃ o: {chunks_json_path}")
            print(f"\nğŸ‰ Chunking completed (without AI review)!")
            print(f"   ğŸ“„ Total chunks: {len(all_chunks)}")
            print(f"   ğŸ’¾ Output file: {chunks_json_path}")
            return

    status = review.get("status", "issues_found")
    confidence = review.get("confidence", 0.0)
    issues = review.get("issues", []) or []
    notes = review.get("notes", "")

    print(f"- Tráº¡ng thÃ¡i: {status} | Confidence: {confidence:.2f}")
    if notes:
        print(f"- Ghi chÃº: {notes[:4000]}")

    if issues:
        print("\nâš ï¸ BÃO CÃO Váº¤N Äá»€ Tá»ª AI:")
        for i, it in enumerate(issues, 1):
            print(f"{i:02d}. [{it.get('severity','?')}] ({it.get('category','other')}) "
                  f"{it.get('citation') or it.get('id') or ''}")
            print(f"    - {it.get('message','(no message)')}")
            if it.get('suggestion'):
                print(f"    â†’ Gá»£i Ã½: {it['suggestion']}")
        issues_path = chunks_json_path.replace('.json', '.issues.json')
        with open(issues_path, 'w', encoding='utf-8') as f:
            json.dump(review, f, ensure_ascii=False, indent=2)
        print(f"\nğŸ’¾ ÄÃ£ ghi bÃ¡o cÃ¡o váº¥n Ä‘á»: {issues_path}")

        if args.strict_ok_only:
            print("âŒ --strict-ok-only: KhÃ´ng ghi chunks vÃ¬ AI chÆ°a xÃ¡c nháº­n 'ok'.")
            return

    print(f"\nğŸ‰ Chunking completed successfully!")
    print(f"   ğŸ“„ Total chunks: {len(all_chunks)}")
    print(f"   ğŸ’¾ Output file: {chunks_json_path}")
    if args.AI and status == "ok":
        print("   âœ… AI Ä‘Ã£ xÃ¡c nháº­n: Chunking OK!")
    print(f"\nNext: Run main_refactored.py to evaluate with chunks from {chunks_json_path}")

if __name__ == "__main__":
    main()
