# Vietnamese Law Embedding Benchmark â€“ Onboarding Guide

## 1. Project mission
This repository compares multiple embedding models on Vietnamese legal corpora, aiming to identify which model retrieves the most relevant statute passages for benchmark questions. It also includes tools to prepare raw DOC/DOCX laws, sample benchmark questions, and visualize top-scoring retrievals via Qdrant. ã€F:README.mdâ€ L1-L43ã€‘ã€F:README_refactored.mdâ€ L1-L88ã€‘

## 2. Repository layout
The project ships with both the original monolithic workflow and a refactored, modular pipeline. Key top-level assets include:

- **`embedding_evaluation.py`** â€“ legacy end-to-end script that still works but packs ~1,900 lines of logic into a single file. ã€F:README_refactored.mdâ€ L101-L136ã€‘
- **`main_refactored.py`** â€“ entry point for the modular pipeline; it sets up the environment, loads prepared chunks and benchmark queries, runs evaluation across the registered models, and prints a detailed report. ã€F:main_refactored.pyâ€ L1-L191ã€‘
- **`modules/`** â€“ refactored building blocks that encapsulate chunking, loading, embedding, Qdrant operations, and evaluation routines. ã€F:README_refactored.mdâ€ L89-L136ã€‘
- **`data_files/`** â€“ cached metadata such as discovered law file paths and extracted benchmark questions. These JSON files are produced by discovery scripts and reused during chunking/evaluation. ã€F:README.mdâ€ L45-L63ã€‘
- **`data/`** â€“ generated chunk files (per category or full corpus) ready for embedding and evaluation. ã€F:README_refactored.mdâ€ L89-L176ã€‘
- **`results/`** â€“ evaluation artifacts such as aggregated metrics and top-query analyses. ã€F:README.mdâ€ L45-L63ã€‘

Support scripts like `find_law_files.py`, `find_question_files.py`, `chunking.py`, and `embed_and_upload.py` orchestrate the data preparation pipeline and model upload steps. ã€F:README.mdâ€ L45-L63ã€‘ã€F:README_refactored.mdâ€ L1-L176ã€‘

## 3. Data preparation workflow
1. **Catalog source statutes:** Run `find_law_files.py` to build `data_files/law_file_paths.json` and, in the refactored flow, category-specific JSON manifests (BDS, DN, TM, QDS). ã€F:README.mdâ€ L45-L63ã€‘ã€F:README_refactored.mdâ€ L9-L40ã€‘
2. **Extract benchmark questions:** Run `find_question_files.py` to transform Excel sheets into `data_files/law_questions.json`, which holds 511 labeled questions. ã€F:README.mdâ€ L7-L43ã€‘ã€F:README_refactored.mdâ€ L41-L68ã€‘
3. **Chunk the statutes:** Use `chunking.py` to convert DOC/DOCX laws into structured chunks. The module handles Unicode normalization, clause-aware heuristics, law ID generation, and optional Gemini-based quality review. Outputs land in `data/` with filenames scoped by category and timestamp. ã€F:modules/chunking.pyâ€ L1-L120ã€‘ã€F:modules/chunking.pyâ€ L121-L240ã€‘ã€F:README_refactored.mdâ€ L41-L176ã€‘
4. **(Optional) Upload to Qdrant:** `embed_and_upload.py` encodes a chosen chunk file with a specific model and stores vectors in Qdrant collections named by model/category. ã€F:README_refactored.mdâ€ L129-L176ã€‘ã€F:modules/embedding_models.pyâ€ L1-L120ã€‘ã€F:modules/qdrant_manager.pyâ€ L1-L120ã€‘

## 4. Evaluation workflow
- **Load data:** `modules/data_loader` reads the law manifests, reuses chunk metadata, and samples benchmark questions (with automatic fallbacks if JSON is missing). It enriches chunk metadata with source file provenance for downstream inspection. ã€F:modules/data_loader.pyâ€ L1-L200ã€‘
- **Encode & store vectors:** `modules/embedding_models` wraps both `transformers` and `sentence-transformers` models with batching, mean pooling, and normalization, while `modules/qdrant_manager` recreates collections, manages payload indexes, and upserts vectors with retry logic. ã€F:modules/embedding_models.pyâ€ L1-L160ã€‘ã€F:modules/qdrant_manager.pyâ€ L1-L160ã€‘
- **Score retrieval quality:** `modules/evaluation` connects to Qdrant, encodes queries, performs vector search, collects per-query metrics (max, top-k averages, threshold counts), and aggregates model-level summaries. It also prints rich console reports and optionally reuses existing Qdrant collections to skip re-encoding. ã€F:modules/evaluation.pyâ€ L1-L200ã€‘
- **Run orchestrator:** `main_refactored.py` calls the above modules end-to-end, then formats a ranking table, highlights best-performing models, and surfaces top queries with supporting metadata excerpts. ã€F:main_refactored.pyâ€ L1-L191ã€‘

## 5. Key concepts & conventions
- **Law IDs:** Automatically generated identifiers distinguish primary statutes from amendments (e.g., `LKBDS`, `LSÄBSLKBDS`) to keep metadata consistent across chunks, embeddings, and Qdrant indexes. ã€F:README_refactored.mdâ€ L69-L110ã€‘ã€F:modules/chunking.pyâ€ L121-L240ã€‘
- **Chunk metadata:** Every chunk carries citation fields (law ID, article, clause) plus source provenance so analysts can trace retrieval hits back to original documents. Qdrant payload indexes are created for these metadata fields to enable filtering. ã€F:modules/chunking.pyâ€ L160-L240ã€‘ã€F:modules/qdrant_manager.pyâ€ L33-L120ã€‘
- **Model registry:** `get_models_to_evaluate()` centralizes the list of supported embedding checkpoints, ensuring both pipelines evaluate the same baseline models unless you modify the registry. ã€F:modules/embedding_models.pyâ€ L121-L180ã€‘

## 6. Recommended next steps for newcomers
1. **Run the refactored pipeline:** Follow the workflow in `README_refactored.md` (setup â†’ discovery â†’ chunking â†’ `main_refactored.py`) to observe the end-to-end evaluation output locally. ã€F:README_refactored.mdâ€ L101-L176ã€‘
2. **Inspect sample results:** Explore `results/embedding_evaluation_results.json` and `results/top_queries_analysis.json` to understand how metrics and retrieved passages are stored. ã€F:README.mdâ€ L45-L63ã€‘
3. **Experiment with models:** Add a new entry to `get_models_to_evaluate()` and rerun the evaluation to see how alternative embeddings behave. ã€F:modules/embedding_models.pyâ€ L121-L180ã€‘
4. **Deep-dive into chunking heuristics:** Review `modules/chunking.py` to learn how Vietnamese statute structure is parsed; tweaking heuristics can improve retrieval quality. ã€F:modules/chunking.pyâ€ L1-L200ã€‘
5. **Integrate with Qdrant filters:** Use the payload indexes created in `qdrant_manager.py` to build filtered search or analytics tools that target specific laws, chapters, or categories. ã€F:modules/qdrant_manager.pyâ€ L33-L160ã€‘

Happy benchmarking! ğŸš€
