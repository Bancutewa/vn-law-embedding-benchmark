#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ƒê√°nh Gi√° M√¥ H√¨nh Embedding Cho Lu·∫≠t Ti·∫øng Vi·ªát
Chuy·ªÉn ƒë·ªïi t·ª´ notebook th√†nh script ƒë·ªÉ ch·∫°y m·ªôt l·∫ßn
"""

import warnings
warnings.filterwarnings('ignore')

import torch
import numpy as np
import pandas as pd
import os
import sys
import re
import json
import time
import gc
from tqdm import tqdm
from sklearn.metrics.pairwise import cosine_similarity
from qdrant_client import QdrantClient
from qdrant_client.http.models import Distance, VectorParams, PointStruct
from dotenv import load_dotenv
load_dotenv()

def check_version(package_name, expected_version=None):
    """Ki·ªÉm tra phi√™n b·∫£n package"""
    try:
        import importlib.metadata
        version = importlib.metadata.version(package_name)
        if expected_version and version != expected_version:
            print(f"‚ö†Ô∏è  {package_name}: expected {expected_version}, got {version}")
        else:
            print(f"‚úÖ {package_name}: {version}")
        return True, version
    except Exception as e:
        print(f"‚ùå {package_name}: not found or error ({e})")
        return False, None

def setup_environment():
    """Thi·∫øt l·∫≠p m√¥i tr∆∞·ªùng v√† import c√°c th∆∞ vi·ªán c·∫ßn thi·∫øt"""
    print("üîÑ Importing core libraries...")
    
    # Check critical versions
    check_version("huggingface_hub", "0.16.4")
    check_version("tokenizers", "0.13.3")
    check_version("transformers", "4.32.1")
    check_version("sentence-transformers", "2.2.2")
    
    # Import transformers
    print("\nüîÑ Importing transformers...")
    try:
        from transformers import AutoTokenizer, AutoModel
        print("‚úÖ transformers imported successfully")
        
        # Quick functionality test
        test_tokenizer = AutoTokenizer.from_pretrained("distilbert-base-uncased")
        test_tokens = test_tokenizer("test", return_tensors="pt")
        print("‚úÖ transformers functionality test passed")
        del test_tokenizer, test_tokens
        
    except Exception as e:
        print(f"‚ùå transformers failed: {e}")
        raise
    
    # Import sentence-transformers
    print("\nüîÑ Importing sentence-transformers...")
    try:
        from sentence_transformers import SentenceTransformer
        print("‚úÖ sentence-transformers imported successfully")
        
        # Quick functionality test
        test_model = SentenceTransformer("all-MiniLM-L6-v2")
        test_embedding = test_model.encode(["test sentence"])
        print(f"‚úÖ sentence-transformers functionality test passed (embedding shape: {test_embedding.shape})")
        del test_model, test_embedding
        
    except Exception as e:
        print(f"‚ùå sentence-transformers failed: {e}")
        raise
    
    # Import document processing
    print("\nüîÑ Importing document processing...")
    try:
        from docx import Document
        print("‚úÖ python-docx imported successfully")
    except ImportError:
        print("üì¶ Installing python-docx...")
        import subprocess
        subprocess.check_call([sys.executable, "-m", "pip", "install", "python-docx==0.8.11"])
        from docx import Document
        print("‚úÖ python-docx installed and imported")
    
    # Device configuration
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"\nüñ•Ô∏è  Using device: {device}")
    if device == "cuda":
        print(f"   GPU: {torch.cuda.get_device_name()}")
        print(f"   GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB")
    
    print("\nüéâ All libraries loaded successfully! Ready for evaluation.")
    return device

def get_qdrant_client():
    """Kh·ªüi t·∫°o Qdrant client t·ª´ bi·∫øn m√¥i tr∆∞·ªùng.
    """
    url = os.getenv("QDRANT_URL")
    api_key = os.getenv("QDRANT_API_KEY")

    try:
        client = QdrantClient(
            url=url, 
            api_key=api_key or None, 
            timeout=300.0,
            grpc_port=6334,
        )
        print("üóÑÔ∏è  Qdrant connected successfully")
        return client
    except Exception as e:
        print(f"‚ùå Cannot connect to Qdrant: {e}")
        raise

def ensure_collection(client: QdrantClient, collection_name: str, vector_size: int):
    """T·∫°o m·ªõi (recreate) collection v·ªõi c·∫•u h√¨nh cosine v√† k√≠ch th∆∞·ªõc vector t∆∞∆°ng ·ª©ng."""
    client.recreate_collection(
        collection_name=collection_name,
        vectors_config=VectorParams(size=vector_size, distance=Distance.COSINE)
    )
    print(f"‚úÖ Collection ready: {collection_name} (dim={vector_size})")

    try:
        from qdrant_client.http.models import PayloadSchemaType

        index_fields = {
            # C·∫•u tr√∫c ph√°p ƒëi·ªÉn
            "metadata.law_id": PayloadSchemaType.keyword,
            "metadata.law_title": PayloadSchemaType.keyword,
            "metadata.law_no": PayloadSchemaType.keyword,
            "metadata.chapter": PayloadSchemaType.keyword,
            "metadata.section": PayloadSchemaType.keyword,
            "metadata.article_no": PayloadSchemaType.integer,
            "metadata.article_title": PayloadSchemaType.keyword,
            "metadata.clause_no": PayloadSchemaType.integer,
            "metadata.point_letter": PayloadSchemaType.keyword,
            "metadata.exact_citation": PayloadSchemaType.keyword,
            # Ngu·ªìn
            "metadata.source_category": PayloadSchemaType.keyword,
            "metadata.source_file_name": PayloadSchemaType.keyword,
            "metadata.source_file": PayloadSchemaType.keyword,
            "metadata.chunk_index": PayloadSchemaType.integer,
        }

        for field_name, schema_type in index_fields.items():
            try:
                client.create_payload_index(
                    collection_name=collection_name,
                    field_name=field_name,
                    field_schema=schema_type,
                )
                print(f"   üîé Indexed payload field: {field_name} ({schema_type})")
            except Exception as ie:
                # C√≥ th·ªÉ index ƒë√£ t·ªìn t·∫°i sau khi recreate; ch·ªâ log c·∫£nh b√°o nh·∫π
                print(f"   ‚ö†Ô∏è Could not index '{field_name}': {ie}")
    except Exception as e:
        print(f"‚ö†Ô∏è Skipped creating payload indexes: {e}")


def upsert_embeddings_to_qdrant(client: QdrantClient, collection_name: str, embeddings: np.ndarray, law_docs: list, batch_size=100):
    """Upsert to√†n b·ªô embeddings v√† payload v√†o Qdrant theo batch nh·ªè."""
    total_points = len(embeddings)
    print(f"üì§ Upserting {total_points} vectors in batches of {batch_size}...")
    
    for i in range(0, total_points, batch_size):
        batch_end = min(i + batch_size, total_points)
        batch_points = []
        
        for idx in range(i, batch_end):
            payload = law_docs[idx]
            batch_points.append(PointStruct(id=idx, vector=embeddings[idx].tolist(), payload=payload))
        
        # Retry mechanism
        max_retries = 3
        for retry in range(max_retries):
            try:
                client.upsert(collection_name=collection_name, points=batch_points)
                print(f"   ‚úÖ Batch {i//batch_size + 1}: upserted {len(batch_points)} vectors ({batch_end}/{total_points})")
                break
            except Exception as e:
                if retry < max_retries - 1:
                    print(f"   ‚ö†Ô∏è Batch {i//batch_size + 1} failed (attempt {retry + 1}/{max_retries}): {e}")
                    print(f"   üîÑ Retrying in 2 seconds...")
                    time.sleep(2)
                else:
                    print(f"   ‚ùå Batch {i//batch_size + 1} failed after {max_retries} attempts: {e}")
                    raise
    
    print(f"‚úÖ Successfully upserted {total_points} vectors into Qdrant collection '{collection_name}'")

def search_qdrant(client: QdrantClient, collection_name: str, query_embedding: np.ndarray, top_k: int):
    """Search top-k b·∫±ng Qdrant, tr·∫£ v·ªÅ (indices, scores)."""
    hits = client.search(
        collection_name=collection_name,
        query_vector=query_embedding.tolist(),
        limit=top_k
    )
    top_indices = [hit.id for hit in hits]
    top_scores = [float(hit.score) for hit in hits]
    return np.array(top_indices), np.array(top_scores, dtype=float)

def count_collection_points(client: QdrantClient, collection_name: str) -> int:
    try:
        ct = client.count(collection_name=collection_name, exact=True)
        return int(getattr(ct, 'count', 0))
    except Exception:
        return 0

# Roman numeral converter
ROMAN_MAP = {'I':1,'V':5,'X':10,'L':50,'C':100,'D':500,'M':1000}

def roman_to_int(s: str):
    """Chuy·ªÉn ƒë·ªïi s·ªë La M√£ sang s·ªë nguy√™n"""
    s = s.upper().strip()
    if not s or any(ch not in ROMAN_MAP for ch in s):
        return None
    total = 0
    prev = 0
    for ch in reversed(s):
        val = ROMAN_MAP[ch]
        if val < prev:
            total -= val
        else:
            total += val
            prev = val
    return total

def mean_pooling(model_output, attention_mask):
    """Mean pooling ƒë·ªÉ t·∫°o sentence embeddings t·ª´ token embeddings"""
    token_embeddings = model_output[0]
    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()
    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)

def encode_with_transformers(texts, model_name, max_length=512, batch_size=32, device="cuda"):
    """Encode texts using transformers library v·ªõi mean pooling"""
    from transformers import AutoTokenizer, AutoModel
    
    print(f"Loading model: {model_name}")
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModel.from_pretrained(model_name).to(device)
    model.eval()
    
    all_embeddings = []
    
    with torch.no_grad():
        for i in tqdm(range(0, len(texts), batch_size), desc="Encoding"):
            batch = texts[i:i+batch_size]
            
            # Tokenize
            encoded = tokenizer(
                batch, 
                padding=True, 
                truncation=True, 
                max_length=max_length,
                return_tensors='pt'
            ).to(device)
            
            # Get model output
            model_output = model(**encoded)
            
            # Mean pooling
            sentence_embeddings = mean_pooling(model_output, encoded['attention_mask'])
            
            # Normalize embeddings
            sentence_embeddings = torch.nn.functional.normalize(sentence_embeddings, p=2, dim=1)
            
            # Move to CPU and convert to numpy
            embeddings = sentence_embeddings.cpu().numpy()
            all_embeddings.append(embeddings)
            
            # Clear GPU memory
            del encoded, model_output, sentence_embeddings, embeddings
            torch.cuda.empty_cache() if device == "cuda" else None
    
    # Delete model to free memory
    del model, tokenizer
    torch.cuda.empty_cache() if device == "cuda" else None
    
    # Combine all embeddings
    final_embeddings = np.vstack(all_embeddings)
    print(f"   ‚úÖ Generated {final_embeddings.shape[0]} embeddings")
    
    return final_embeddings

def encode_with_sentence_transformers(texts, model_name, batch_size=32, device="cuda"):
    """Encode texts using sentence-transformers library"""
    from sentence_transformers import SentenceTransformer
    
    print(f"Loading model: {model_name}")
    model = SentenceTransformer(model_name)
    model.to(device)
    
    # Encode all texts
    embeddings = model.encode(
        texts,
        batch_size=batch_size,
        show_progress_bar=True,
        convert_to_numpy=True,
        normalize_embeddings=True
    )
    
    # Delete model to free memory
    del model
    torch.cuda.empty_cache() if device == "cuda" else None
    
    print(f"   ‚úÖ Generated {embeddings.shape[0]} embeddings")
    
    return embeddings

def read_docx(file_path):
    """ƒê·ªçc file docx v√† tr·∫£ v·ªÅ text (h·ªó tr·ª£ c·∫£ .doc v√† .docx)"""
    print(f"   Reading file: {file_path}")
    
    try:
        # Th·ª≠ ƒë·ªçc b·∫±ng python-docx tr∆∞·ªõc (cho file .docx th·ª±c s·ª±)
        from docx import Document
        doc = Document(file_path)
        text = "\n".join((p.text or "").strip() for p in doc.paragraphs)
        print(f"   ‚úÖ Successfully read {len(text):,} characters using python-docx")
        return text
    except Exception as e1:
        print(f"   ‚ö†Ô∏è python-docx failed: {e1}")
        
        # N·∫øu file c√≥ extension .docx nh∆∞ng th·ª±c t·∫ø l√† .doc, th·ª≠ d√πng docx2txt
        try:
            import docx2txt
            text = docx2txt.process(file_path)
            if text and len(text.strip()) > 0:
                print(f"   ‚úÖ Successfully read {len(text):,} characters using docx2txt")
                return text
            else:
                print(f"   ‚ùå docx2txt returned empty content")
                return ""
        except Exception as e2:
            print(f"   ‚ùå docx2txt also failed: {e2}")
            return ""

def normalize_lines(text: str):
    """Chu·∫©n h√≥a c√°c d√≤ng text - lo·∫°i b·ªè whitespace th·ª´a"""
    lines = [re.sub(r'\s+$', '', ln) for ln in text.splitlines()]
    print(f"   ‚úÖ Normalized {len(lines):,} lines")
    return lines

# ===================== GEMINI EVALUATION AGENT (optional) =====================
# Env controls:
#   CHUNK_AI_REVIEW=true|1        ‚Üí enable Gemini review for chunks
#   CHUNK_STRICT_OK_ONLY=true|1   ‚Üí only proceed when Gemini returns status=ok
#   GEMINI_API_KEY                ‚Üí required when review is enabled

GEMINI_MODEL_NAME = "gemini-2.5-flash"

GEMINI_PROMPT = """B·∫°n l√† chuy√™n gia ph√°p ƒëi·ªÉn ho√° & ki·ªÉm th·ª≠ d·ªØ li·ªáu lu·∫≠t.
T√¥i g·ª≠i cho b·∫°n:
1) Summary th·ªëng k√™ & c·∫£nh b√°o t·ª´ b·ªô chunking.
2) M·ªôt s·ªë "excerpts" (tr√≠ch ƒëo·∫°n nguy√™n vƒÉn) ƒë·∫°i di·ªán.
3) Danh m·ª•c chunks (id + metadata + content r√∫t g·ªçn).

Nhi·ªám v·ª•:
- PH√ÅT HI·ªÜN B·∫§T TH∆Ø·ªúNG (anomalies) trong chunking, v√≠ d·ª•:
  * Sai th·ª© t·ª±/ch∆∞a ‚Äústrict‚Äù (nh·∫£y c√≥c Ch∆∞∆°ng/ƒêi·ªÅu/Kho·∫£n/ƒêi·ªÉm).
  * Nh·∫≠n di·ªán nh·∫ßm ‚ÄúKho·∫£n‚Äù (kh√¥ng ph·∫£i d·∫°ng `1.`) ho·∫∑c ‚Äúƒêi·ªÉm‚Äù (kh√¥ng ph·∫£i `a)`).
  * Kh√¥ng ti√™m intro kho·∫£n v√†o ƒëi·ªÉm khi ƒë√£ c√≥ chu·ªói ƒëi·ªÉm.
  * Thi·∫øu/b·ªè s√≥t n·ªôi dung so v·ªõi excerpts.
  * Metadata kh√¥ng kh·ªõp: article_no/clause_no/point_letter/exact_citation.
  * ƒê√≥ng m·ªü chu·ªói ƒëi·ªÉm sai (b·∫Øt ƒë·∫ßu kh√¥ng ph·∫£i ‚Äúa)‚Äù, ch√®n n·ªôi dung th∆∞·ªùng v√†o gi·ªØa).
  * N·ªôi dung ‚Äúƒêi·ªÅu‚Äù kh√¥ng c√≥ kho·∫£n nh∆∞ng kh√¥ng sinh chunk intro.
- G·ª¢I √ù S·ª¨A: ch·ªâ r√µ v·ªã tr√≠ (id / exact_citation), m√¥ t·∫£ v·∫•n ƒë·ªÅ, c√°ch kh·∫Øc ph·ª•c.
- N·∫øu KH√îNG th·∫•y v·∫•n ƒë·ªÅ, x√°c nh·∫≠n ‚Äúok‚Äù v√† n√™u ng·∫Øn g·ªçn c∆° s·ªü k·∫øt lu·∫≠n.

H√£y TR·∫¢ L·ªúI CH·ªà ·ªü d·∫°ng JSON theo schema:
{
  "status": "ok" | "issues_found",
  "confidence": 0.0-1.0,
  "issues": [
    {
      "id": "chu·ªói id chunk ho·∫∑c m√¥ t·∫£ v·ªã tr√≠",
      "citation": "ƒêi·ªÅu ... kho·∫£n ... ƒëi·ªÉm ...",
      "severity": "low|medium|high",
      "category": "ordering|regex|metadata|omission|points_chain|format|other",
      "message": "M√¥ t·∫£ ng·∫Øn g·ªçn v·∫•n ƒë·ªÅ",
      "suggestion": "C√°ch s·ª≠a ng·∫Øn g·ªçn"
    }
  ],
  "notes": "Ghi ch√∫ ng·∫Øn (tu·ª≥ ch·ªçn)"
}
Tr·∫£ JSON h·ª£p l·ªá. Kh√¥ng gi·∫£i th√≠ch ngo√†i JSON.
"""

def _shorten_text(s: str, max_len: int = 500) -> str:
    if len(s) <= max_len:
        return s
    head = s[: max_len // 2].rstrip()
    tail = s[- max_len // 2 :].lstrip()
    return head + "\n...\n" + tail

def build_review_payload(chunks, summary, raw_text: str, sample_excerpts_chars: int = 8000):
    n = len(raw_text)
    if n <= sample_excerpts_chars:
        excerpts = raw_text
    else:
        k = sample_excerpts_chars // 3
        excerpts = raw_text[:k] + "\n...\n" + raw_text[n//2 - k//2 : n//2 + k//2] + "\n...\n" + raw_text[-k:]

    def lite(c):
        return {
            "id": c.get("metadata", {}).get("exact_citation", ""),
            "metadata": c.get("metadata"),
            "content_preview": _shorten_text(c.get("content", ""), 700)
        }

    chunks_lite = [lite(c) for c in chunks]
    return {
        "summary": summary,
        "excerpts": excerpts,
        "chunks_preview": chunks_lite[:1200]
    }

def _env_truthy(v: str) -> bool:
    return str(v or "").strip().lower() in {"1", "true", "yes", "y", "on"}

def call_gemini_review(payload: dict, api_key: str = None) -> dict:
    api_key = api_key or os.getenv("GEMINI_API_KEY", "").strip()
    if not api_key:
        raise RuntimeError("Thi·∫øu GEMINI_API_KEY trong m√¥i tr∆∞·ªùng (.env).")
    try:
        # import at call time
        import google.generativeai as genai
    except Exception as e:
        raise RuntimeError(f"Thi·∫øu th∆∞ vi·ªán google-generativeai: {e}")

    genai.configure(api_key=api_key)
    generation_config = {
        "temperature": 0.2,
        "top_p": 0.9,
        "top_k": 40,
        "response_mime_type": "application/json",
    }
    model = genai.GenerativeModel(GEMINI_MODEL_NAME, generation_config=generation_config)
    prompt_parts = [
        {"role": "user", "parts": [{"text": GEMINI_PROMPT}]},
        {"role": "user", "parts": [{"text": json.dumps(payload, ensure_ascii=False)}]},
    ]
    resp = model.generate_content(prompt_parts)
    raw = getattr(resp, "text", None) or (resp.candidates and resp.candidates[0].content.parts[0].text)
    if not raw:
        raise RuntimeError("Gemini kh√¥ng tr·∫£ ra n·ªôi dung.")
    try:
        data = json.loads(raw)
        if not isinstance(data, dict):
            raise ValueError("JSON kh√¥ng ph·∫£i object")
        data.setdefault("status", "issues_found")
        data.setdefault("issues", [])
        data.setdefault("confidence", 0.0)
        return data
    except Exception as e:
        return {
            "status": "issues_found",
            "confidence": 0.0,
            "issues": [{
                "id": "PARSER",
                "citation": "",
                "severity": "high",
                "category": "other",
                "message": f"Kh√¥ng parse ƒë∆∞·ª£c JSON t·ª´ Gemini: {e}",
                "suggestion": "Ch·∫°y l·∫°i ho·∫∑c gi·∫£m excerpts."
            }],
            "notes": raw[:2000]
        }

def chunk_law_document(text, law_id="LAW", law_no="", law_title=""):
    """Chia vƒÉn b·∫£n lu·∫≠t th√†nh chunks theo ƒë·ªãnh d·∫°ng hn2014_chunks.json"""
    print("   üîç Chunking law document with strict parsing...")

    lines = normalize_lines(text)
    
    # Regex patterns
    ARTICLE_RE = re.compile(r'^ƒêi·ªÅu\s+(\d+)\s*[\.:]?\s*(.*)$', re.UNICODE)
    CHAPTER_RE = re.compile(r'^Ch∆∞∆°ng\s+([IVXLCDM]+|\d+)\s*(.*)$', re.UNICODE | re.IGNORECASE)
    SECTION_RE = re.compile(r'^M·ª•c\s+(\d+)\s*[:\-]?\s*(.*)$', re.UNICODE | re.IGNORECASE)
    CLAUSE_RE = re.compile(r'^\s*(\d+)\.\s*(.*)$', re.UNICODE)
    POINT_RE = re.compile(r'^\s*([a-zA-Zƒëƒê])\)\s+(.*)$', re.UNICODE)

    # PASS 1: Pre-scan
    chapters_nums, articles_nums = [], []
    chapters_labels, article_titles_seen = [], []

    for line in lines:
            if not line:
                continue
            m_ch = CHAPTER_RE.match(line)
            if m_ch:
                n = roman_to_int(m_ch.group(1))
                if n:
                    chapters_nums.append(n)
                    title = (m_ch.group(2) or "").strip()
                    chapters_labels.append(
                        f"Ch∆∞∆°ng {m_ch.group(1).strip()}" + (f" ‚Äì {title}" if title else "")
                    )
                continue
            m_art = ARTICLE_RE.match(line)
            if m_art:
                articles_nums.append(int(m_art.group(1)))
                continue

    chapters_set, articles_set = set(chapters_nums), set(articles_nums)

    # PASS 2: Chunking
    chunks = []
    chapter_label = None
    section_label = None
    article_no = None
    article_title = ""
    expecting_article_title = False
    article_intro_buf = ""
    article_has_any_chunk = False
    clause_no = None
    clause_buf = ""
    clause_intro_current = None
    in_points = False
    point_letter = None
    point_buf = ""
    expected_chapter = None
    expected_article = None
    seeking_article = False

    def build_article_header(article_no: int, article_title: str) -> str:
        t = (article_title or "").strip()
        return f"ƒêi·ªÅu {article_no}" + (f" {t}" if t else "")

    def flush_article_intro():
        nonlocal article_intro_buf, article_has_any_chunk
        content = article_intro_buf.strip()
        if not content:
            return
        cid = f"{law_id}-D{article_no}"
        exact = f"ƒêi·ªÅu {article_no}"
        meta = {
            "law_no": law_no,
            "law_title": law_title,
            "law_id": law_id,
            "chapter": chapter_label,
            "section": section_label,
            "article_no": article_no,
            "article_title": article_title,
            "exact_citation": exact
        }
        title_line = f"ƒêi·ªÅu {article_no}. {article_title}".strip() if article_title else f"ƒêi·ªÅu {article_no}"
        chunks.append({
            "id": cid,
            "content": f"{title_line}\n{content}",
            "metadata": meta
        })
        article_intro_buf = ""
        article_has_any_chunk = True

    def flush_clause():
        nonlocal clause_buf
        content = clause_buf.strip()
        if not content:
            return
        cid = f"{law_id}-D{article_no}-K{clause_no}"
        exact = f"ƒêi·ªÅu {article_no} kho·∫£n {clause_no}"
        meta = {
            "law_no": law_no,
            "law_title": law_title,
            "law_id": law_id,
            "chapter": chapter_label,
            "section": section_label,
            "article_no": article_no,
            "article_title": article_title,
            "clause_no": clause_no,
            "exact_citation": exact
        }
        art_hdr = build_article_header(article_no, article_title)
        full_content = f"{art_hdr} Kho·∫£n {clause_no}. {content}"
        chunks.append({
            "id": cid,
            "content": full_content,
            "metadata": meta
        })

    def flush_point():
        nonlocal point_buf
        content = point_buf.strip()
        if not content:
            return
        letter = point_letter.lower()
        cid = f"{law_id}-D{article_no}-K{clause_no}-{letter}"
        exact = f"ƒêi·ªÅu {article_no} kho·∫£n {clause_no} ƒëi·ªÉm {letter})"
        meta = {
            "law_no": law_no,
            "law_title": law_title,
            "law_id": law_id,
            "chapter": chapter_label,
            "section": section_label,
            "article_no": article_no,
            "article_title": article_title,
            "clause_no": clause_no,
            "point_letter": letter,
            "exact_citation": exact
        }
        if clause_intro_current:
            meta["clause_intro"] = clause_intro_current

        art_hdr = build_article_header(article_no, article_title)
        if clause_intro_current:
            intro = clause_intro_current.rstrip().rstrip(':')
            full_content = f"{art_hdr} Kho·∫£n {clause_no} {intro}, ƒëi·ªÉm {letter}): {content}"
        else:
            full_content = f"{art_hdr} Kho·∫£n {clause_no}, ƒëi·ªÉm {letter}) {content}"

        chunks.append({
            "id": cid,
            "content": full_content,
            "metadata": meta
        })

    def close_clause():
        nonlocal clause_no, clause_buf, in_points, point_letter, point_buf, article_has_any_chunk, clause_intro_current
        if clause_no is None:
            return
        if in_points and point_letter:
            flush_point()
        elif clause_buf.strip():
            flush_clause()
        article_has_any_chunk = True
        clause_no, clause_buf, in_points, point_letter, point_buf = None, "", False, None, ""
        clause_intro_current = None

    def close_article_if_needed():
        nonlocal article_intro_buf, article_has_any_chunk
        if not article_has_any_chunk and article_intro_buf.strip():
            flush_article_intro()

    print(f"   üìÑ Processing {len(lines):,} lines...")

    for line in lines:
        if not line:
            continue

        if seeking_article:
            m_art_seek = ARTICLE_RE.match(line)
            if m_art_seek:
                a_no = int(m_art_seek.group(1))
                if a_no == expected_article:
                    seeking_article = False
                    close_clause()
                    if article_no is not None:
                        close_article_if_needed()
                    article_no = a_no
                    article_title = (m_art_seek.group(2) or "").strip()
                    if not article_title:
                        expecting_article_title = True
                    expected_article = a_no + 1
                    clause_no = None
                    clause_buf = ""
                    in_points = False
                    point_letter = None
                    point_buf = ""
                    clause_intro_current = None
                    continue
            continue

        if expecting_article_title:
            if not any(regex.match(line) for regex in [CHAPTER_RE, SECTION_RE, CLAUSE_RE, POINT_RE, ARTICLE_RE]):
                article_title = line
                expecting_article_title = False
                continue
            else:
                expecting_article_title = False

        # CH∆Ø∆†NG
        m_ch = CHAPTER_RE.match(line)
        if m_ch:
            close_clause()
            if article_no is not None:
                close_article_if_needed()
            article_no = None
            article_title = ""
            article_intro_buf = ""
            expecting_article_title = False

            roman = m_ch.group(1).strip()
            ch_num = roman_to_int(roman) or 0
            ch_title = (m_ch.group(2) or "").strip()
            lbl = f"Ch∆∞∆°ng {roman}" + (f" ‚Äì {ch_title}" if ch_title else "")

            if expected_chapter is None:
                expected_chapter = ch_num + 1
            elif ch_num == expected_chapter:
                    expected_chapter = ch_num + 1
            elif ch_num > expected_chapter:
                if expected_chapter not in chapters_set:
                    break
                continue
            else:
                    continue

            chapter_label = lbl
            section_label = None
            continue

        # M·ª§C
        m_sec = SECTION_RE.match(line)
        if m_sec:
            close_clause()
            if article_no is not None:
                close_article_if_needed()
            article_no = None
            article_title = ""
            article_intro_buf = ""
            expecting_article_title = False

            sec_no = m_sec.group(1).strip()
            sec_title = (m_sec.group(2) or "").strip()
            section_label = f"M·ª•c {sec_no}" + (f" ‚Äì {sec_title}" if sec_title else "")
            continue

        # ƒêI·ªÄU
        m_art = ARTICLE_RE.match(line)
        if m_art:
            a_no = int(m_art.group(1))
            a_title = (m_art.group(2) or "").strip()

            if expected_article is None:
                expected_article = a_no + 1
                close_clause()
                if article_no is not None:
                    close_article_if_needed()
                article_no = a_no
                article_title = a_title
                if not article_title:
                    expecting_article_title = True
                clause_no = None
                clause_buf = ""
                in_points = False
                point_letter = None
                point_buf = ""
                clause_intro_current = None
                continue
            elif a_no == expected_article:
                expected_article = a_no + 1
                close_clause()
                if article_no is not None:
                    close_article_if_needed()
                article_no = a_no
                article_title = a_title
                if not article_title:
                    expecting_article_title = True
                clause_no = None
                clause_buf = ""
                in_points = False
                point_letter = None
                point_buf = ""
                clause_intro_current = None
                continue
            elif a_no > expected_article:
                if expected_article not in articles_set:
                    break
                else:
                    seeking_article = True
                    continue
            else:
                    continue

        if article_no is None:
            continue

        # KHO·∫¢N
        m_k = CLAUSE_RE.match(line)
        if m_k and m_k.group(1).isdigit():
            if article_intro_buf.strip():
                flush_article_intro()
                article_has_any_chunk = True
            close_clause()
            clause_no = int(m_k.group(1))
            clause_buf = (m_k.group(2) or "").strip()
            in_points = False
            point_letter = None
            point_buf = ""
            clause_intro_current = None
            continue

        # ƒêI·ªÇM
        m_p = POINT_RE.match(line)
        if m_p and clause_no is not None:
            letter = m_p.group(1).lower()
            text = (m_p.group(2) or "").strip()

            if not in_points:
                if letter != 'a':
                    clause_buf += ("\n" if clause_buf else "") + f"{letter}) {text}"
                    continue
                clause_intro_current = clause_buf.strip() if clause_buf.strip() else None
                clause_buf = ""
                in_points = True
                point_letter = letter
                point_buf = text
                continue

            if point_letter:
                flush_point()
            in_points = True
            point_letter = letter
            point_buf = text
            continue

        # N·ªôi dung k√©o d√†i
        if clause_no is not None:
            if in_points and point_letter:
                point_buf += ("\n" if point_buf else "") + line
            else:
                clause_buf += ("\n" if clause_buf else "") + line
        else:
            article_intro_buf += ("\n" if article_intro_buf else "") + line

    # K·∫øt th√∫c
    close_clause()
    if article_no is not None:
        close_article_if_needed()

    # Filter chunks
    valid_chunks = []
    for chunk in chunks:
        content = chunk['content'].strip()
        if len(content) > 50:
            valid_chunks.append(chunk)

    print(f"   ‚úÖ Created {len(valid_chunks)} chunks")
    return valid_chunks

def generate_law_id(file_name: str) -> str:
    """T·ª± ƒë·ªông sinh law_id t·ª´ t√™n file"""
    # Lo·∫°i b·ªè extension v√† chu·∫©n h√≥a
    name = file_name.replace('.docx', '').replace('.doc', '').strip()

    # T·ª´ ƒëi·ªÉn mapping cho c√°c lo·∫°i lu·∫≠t ph·ªï bi·∫øn
    law_mappings = {
        'kinh doanh b·∫•t ƒë·ªông s·∫£n': 'LKBDS',
        'nh√† ·ªü': 'LNHAO',
        'ƒë·∫•t ƒëai': 'LDATDAI',
        'ƒë·∫ßu t∆∞': 'LDAUTU',
        'ƒë·∫ßu t∆∞ c√¥ng': 'LDAUTUCONG',
        'ƒë·∫ßu t∆∞ theo ph∆∞∆°ng th·ª©c ƒë·ªëi t√°c c√¥ng t∆∞': 'LDAUTUPPPCT',
        'thu·∫ø s·ª≠ d·ª•ng ƒë·∫•t n√¥ng nghi·ªáp': 'LTSDDNONGNGHIEP',
        'thu·∫ø s·ª≠ d·ª•ng ƒë·∫•t phi n√¥ng nghi·ªáp': 'LTSDDPHINONGNGHIEP',
        'x√¢y d·ª±ng': 'LXAYDUNG',
        'h√¥n nh√¢n v√† gia ƒë√¨nh': 'LHNVDG',
    }

    # Chu·∫©n h√≥a t√™n ƒë·ªÉ matching
    name_lower = name.lower()

    # T√¨m mapping ph√π h·ª£p
    for key, value in law_mappings.items():
        if key in name_lower:
            return value

    # X·ª≠ l√Ω c√°c tr∆∞·ªùng h·ª£p ƒë·∫∑c bi·ªát
    if 'lu·∫≠t s·ªë' in name_lower and 'qh' in name_lower:
        # Lu·∫≠t s·ªë XX_YYYY_QHZZ -> LXAYDUNG (lu·∫≠t x√¢y d·ª±ng)
        if 'x√¢y d·ª±ng' in name_lower:
            return 'LXAYDUNG'
        # C√°c lu·∫≠t kh√°c c√≥ th·ªÉ th√™m mapping

    if 'vƒÉn b·∫£n h·ª£p nh·∫•t' in name_lower:
        # VƒÉn b·∫£n h·ª£p nh·∫•t Lu·∫≠t XXX -> LDAUTU
        if 'ƒë·∫ßu t∆∞' in name_lower:
            return 'LDAUTU'

    # X·ª≠ l√Ω c√°c file VBHN (VƒÉn b·∫£n h·ª£p nh·∫•t) c√≥ th·ªÉ l√† lu·∫≠t x√¢y d·ª±ng
    if name_lower.startswith('vbhn') or 'vbhn' in name_lower:
        # Th∆∞·ªùng l√† lu·∫≠t x√¢y d·ª±ng
        return 'LXAYDUNG'

    # X·ª≠ l√Ω lu·∫≠t s·ªë kh√¥ng c√≥ t·ª´ kh√≥a r√µ r√†ng
    if 'lu·∫≠t s·ªë' in name_lower:
        # C√≥ th·ªÉ l√† lu·∫≠t x√¢y d·ª±ng n·∫øu kh√¥ng match g√¨ kh√°c
        return 'LXAYDUNG'

    # N·∫øu kh√¥ng t√¨m th·∫•y, t·∫°o ID t·ª´ ch·ªØ c√°i ƒë·∫ßu c·ªßa c√°c t·ª´ quan tr·ªçng
    words = name.split()

    # L·ªçc b·ªè c√°c t·ª´ kh√¥ng quan tr·ªçng
    stop_words = {'s·ªë', 'v√†', 'theo', 'ph∆∞∆°ng', 'th·ª©c', 'ƒë·ªëi', 't√°c', 'c√¥ng', 't∆∞', 'lu·∫≠t', 'vƒÉn', 'b·∫£n', 'h·ª£p', 'nh·∫•t', 'nƒÉm', 'qƒë', 'tt', 'bh', 'vbh', 'vbhn', 'vpqh'}

    important_words = []
    for word in words:
        word_lower = word.lower()
        # B·ªè qua s·ªë, t·ª´ d·ª´ng, v√† t·ª´ qu√° ng·∫Øn
        if len(word) <= 1 or word_lower in stop_words or word.isdigit():
            continue
        # B·ªè qua c√°c pattern s·ªë nh∆∞ 2020_QH14
        if '_' in word and any(part.isdigit() for part in word.split('_')):
            continue
        important_words.append(word)

    if important_words:
        # L·∫•y ch·ªØ c√°i ƒë·∫ßu c·ªßa 2-4 t·ª´ quan tr·ªçng ƒë·∫ßu ti√™n
        first_letters = ''.join(w[0].upper() for w in important_words[:4])
        result = f"L{first_letters}"
        # Gi·ªõi h·∫°n ƒë·ªô d√†i
        return result[:8]  # T·ªëi ƒëa 8 k√Ω t·ª±

    # Fallback cu·ªëi c√πng
    first_letters = ''.join(w[0].upper() for w in words[:3] if len(w) > 1 and not w.isdigit())
    return f"L{first_letters[:6]}"  # Gi·ªõi h·∫°n 6 k√Ω t·ª±


def load_all_law_documents():
    """Load v√† chunk t·∫•t c·∫£ vƒÉn b·∫£n lu·∫≠t t·ª´ th∆∞ m·ª•c law_content"""
    print("üìö Loading ALL law documents from law_content folder...")
    
    # Load danh s√°ch file t·ª´ JSON
    law_file_paths = []
    try:
        with open("data_files/law_file_paths.json", 'r', encoding='utf-8') as f:
            law_file_paths = json.load(f)
        print(f"‚úÖ Loaded {len(law_file_paths)} law file paths from JSON")
    except FileNotFoundError:
        print("‚ùå data_files/law_file_paths.json not found! Please run find_law_files.py first.")
        print("üîÑ Running find_law_files.py to generate the file...")
        try:
            import subprocess
            result = subprocess.run([sys.executable, "find_law_files.py"], 
                                  capture_output=True, text=True, cwd=os.getcwd())
            if result.returncode == 0:
                print("‚úÖ Successfully generated data_files/law_file_paths.json")
                with open("data_files/law_file_paths.json", 'r', encoding='utf-8') as f:
                    law_file_paths = json.load(f)
                print(f"‚úÖ Loaded {len(law_file_paths)} law file paths from generated JSON")
            else:
                print(f"‚ùå Failed to generate data_files/law_file_paths.json: {result.stderr}")
                return []
        except Exception as e:
            print(f"‚ùå Error running find_law_files.py: {e}")
            return []
    except Exception as e:
        print(f"‚ùå Error loading data_files/law_file_paths.json: {e}")
        return []
    
    all_law_docs = []
    successful_files = 0
    failed_files = 0
    
    print(f"\nüîÑ Processing {len(law_file_paths)} law files...")
    
    for i, file_info in enumerate(tqdm(law_file_paths, desc="Processing law files")):
        file_path = file_info['path']
        category = file_info['category']
        file_name = file_info['file_name']
        
        # Ki·ªÉm tra xem file c√≥ trong th∆∞ m·ª•c lu·∫≠t kh√¥ng (ƒë√£ ƒë∆∞·ª£c l·ªçc trong find_law_files.py)
        
        print(f"\nüìÑ [{i+1}/{len(law_file_paths)}] Processing: {file_name}")
        print(f"   Category: {category}")
        print(f"   Path: {file_path}")
        
        if not os.path.exists(file_path):
            print(f"   ‚ùå File not found: {file_path}")
            failed_files += 1
            continue
        
        try:
            # B∆∞·ªõc 1: ƒê·ªçc file
            print(f"   üìñ Reading file...")
            law_text = read_docx(file_path)  # H√†m n√†y ƒë√£ x·ª≠ l√Ω c·∫£ .doc v√† .docx
            
            if not law_text or len(law_text.strip()) < 100:
                print(f"   ‚ö†Ô∏è File seems empty or too short: {len(law_text)} characters")
                print(f"   ‚è≠Ô∏è Skipping file (cannot read content)")
                failed_files += 1
                continue
            
            # B∆∞·ªõc 2: Chia th√†nh chunks
            print(f"   üî® Chunking document...")
            # T·∫°o law_id t·ª± ƒë·ªông t·ª´ t√™n file
            law_id = generate_law_id(file_name)
            print(f"   üìã Generated law_id: {law_id}")
            law_chunks = chunk_law_document(law_text, law_id=law_id, law_no="", law_title=file_name)
            
            if not law_chunks:
                print(f"   ‚ö†Ô∏è No chunks created from file")
                failed_files += 1
                continue
            
            # B∆∞·ªõc 3: Chu·∫©n b·ªã d·ªØ li·ªáu cho ƒë√°nh gi√°
            print(f"   üóÇÔ∏è Preparing chunks...")
            for j, chunk in enumerate(law_chunks):
                # Th√™m th√¥ng tin v·ªÅ file g·ªëc v√†o metadata
                chunk_metadata = chunk.get('metadata', {}).copy()
                chunk_metadata.update({
                    'source_file': file_path,
                    'source_category': category,
                    'source_file_name': file_name,
                    'chunk_index': j
                })
                
                # T·∫°o document theo format hn2014_chunks.json
                all_law_docs.append({
                    'id': chunk['id'],
                    'content': chunk['content'],
                    'metadata': chunk_metadata
                })
            
            print(f"   ‚úÖ Successfully processed {len(law_chunks)} chunks")
            successful_files += 1
            
        except Exception as e:
            print(f"   ‚ùå Error processing file: {e}")
            failed_files += 1
            continue
    
    print(f"\nüìä Processing Summary:")
    print(f"   ‚úÖ Successfully processed: {successful_files} files")
    print(f"   ‚ùå Failed to process: {failed_files} files")
    print(f"   üìÑ Total chunks created: {len(all_law_docs)}")
    
    if all_law_docs:
        print(f"   üìä Average chunk length: {np.mean([len(doc['content']) for doc in all_law_docs]):.0f} characters")
        
        # Th·ªëng k√™ theo category
        category_counts = {}
        for doc in all_law_docs:
            doc_category = doc.get('metadata', {}).get('source_category', 'Unknown')
            category_counts[doc_category] = category_counts.get(doc_category, 0) + 1
        
        print(f"\nüìà Chunk distribution by category:")
        for category, count in category_counts.items():
            print(f"   - {category}: {count} chunks")
    
    print(f"\n‚úÖ Successfully loaded {len(all_law_docs)} law document chunks from {successful_files} files!")
    return all_law_docs

    # else:
    #     # Load file c≈© t·ª´ LuatHonNhan
    #     print("üìö Loading law document from LuatHonNhan folder...")
        
    #     law_file_path = "LuatHonNhan/luat_hon_nhan_va_gia_dinh.docx"
    #     if os.path.exists(law_file_path):
    #         print(f"‚úÖ Found law file: {law_file_path}")
            
    #         # B∆∞·ªõc 1: ƒê·ªçc file docx
    #         print("\nüìñ Step 1: Reading DOCX file...")
    #         law_text = read_docx(law_file_path)
            
    #         # B∆∞·ªõc 2: Chia th√†nh chunks
    #         print("\nüî® Step 2: PROPER Advanced chunking with 2-pass validation...")
    #         law_chunks = advanced_chunk_law_document(law_text, max_length=600)

    #         # B∆∞·ªõc 2.1: (Tu·ª≥ ch·ªçn) Th·∫©m ƒë·ªãnh b·∫±ng Gemini
    #         if _env_truthy(os.getenv("CHUNK_AI_REVIEW", "false")):
    #             print("\nü§ñ AI Review: Calling Gemini to validate chunking...")
    #             # X√¢y d·ª±ng summary nh·∫π t·ª´ chunks (kh√¥ng c√≥ c·∫£nh b√°o n·ªôi b·ªô)
    #             try:
    #                 chapters_seen = []
    #                 seen_chapter_set = set()
    #                 citations = []
    #                 type_counts_tmp = {"article_intro": 0, "clause": 0, "point": 0}
    #                 seen_articles = set()
    #                 for c in law_chunks:
    #                     md = c.get("metadata", {})
    #                     ch = md.get("chapter")
    #                     if ch and ch not in seen_chapter_set:
    #                         seen_chapter_set.add(ch)
    #                         chapters_seen.append(ch)
    #                     cite = md.get("exact_citation")
    #                     if cite:
    #                         citations.append(cite)
    #                     t = c.get("type")
    #                     if t in type_counts_tmp:
    #                         type_counts_tmp[t] += 1
    #                     a_no = md.get("article_no")
    #                     if a_no is not None:
    #                         seen_articles.add(a_no)

    #                 summary = {
    #                     "chapters_seen": chapters_seen,
    #                     "articles": len(seen_articles),
    #                     "article_intro": type_counts_tmp.get("article_intro", 0),
    #                     "clauses": type_counts_tmp.get("clauses", 0) or type_counts_tmp.get("clause", 0),
    #                     "points": type_counts_tmp.get("points", 0) or type_counts_tmp.get("point", 0),
    #                     "citations": citations,
    #                     "warnings": [],
    #                     "halted_reason": None,
    #                     "total_chunks": len(law_chunks)
    #                 }

    #                 excerpts_len = int(os.getenv("CHUNK_REVIEW_EXCERPTS", "8000") or 8000)
    #                 payload = build_review_payload(law_chunks, summary, law_text, sample_excerpts_chars=max(2000, excerpts_len))
    #                 review = call_gemini_review(payload)
    #                 status = review.get("status", "issues_found")
    #                 confidence = review.get("confidence", 0.0)
    #                 issues = review.get("issues", []) or []
    #                 notes = review.get("notes", "")
    #                 print(f"- Gemini status: {status} | confidence: {confidence:.2f}")
    #                 if notes:
    #                     print(f"- Notes: {notes[:4000]}")
    #                 if issues:
    #                     print("\n===== Gemini Issues =====")
    #                     for i, it in enumerate(issues[:20], 1):
    #                         print(f"{i:02d}. [{it.get('severity','?')}] ({it.get('category','other')}) {it.get('citation') or it.get('id') or ''}")
    #                         print(f"    - {it.get('message','(no message)')}")
    #                         if it.get('suggestion'):
    #                             print(f"    ‚Üí Suggestion: {it['suggestion']}")

    #                 if _env_truthy(os.getenv("CHUNK_STRICT_OK_ONLY", "false")) and status != "ok":
    #                     print("‚ùå CHUNK_STRICT_OK_ONLY is set and Gemini did not return 'ok'. Aborting load.")
    #                     return []
    #             except Exception as e:
    #                 print(f"‚ö†Ô∏è Gemini review failed: {e}")
    #                 if _env_truthy(os.getenv("CHUNK_STRICT_OK_ONLY", "false")):
    #                     return []
            
    #         # B∆∞·ªõc 3: Chu·∫©n b·ªã d·ªØ li·ªáu cho ƒë√°nh gi√°
    #         print("\nüóÇÔ∏è Step 3: Preparing data for evaluation...")
    #         law_docs = []
    #         for i, chunk in enumerate(law_chunks):
    #             law_docs.append({
    #                 'id': i,
    #                 'title': chunk['title'],
    #                 'text': chunk['content'],
    #                 'length': len(chunk['content']),
    #                 'type': chunk['type'],
    #                 'metadata': chunk.get('metadata', {})
    #             })
            
    #         print(f"   ‚úÖ Processed {len(law_docs)} law document chunks")
    #         print(f"   üìä Average chunk length: {np.mean([doc['length'] for doc in law_docs]):.0f} characters")
            
    #         # Th·ªëng k√™ theo lo·∫°i
    #         type_counts = {}
    #         for doc in law_docs:
    #             doc_type = doc['type']
    #             type_counts[doc_type] = type_counts.get(doc_type, 0) + 1
            
    #         print(f"\nüìà Chunk distribution by type:")
    #         for chunk_type, count in type_counts.items():
    #             print(f"   - {chunk_type}: {count} chunks")
            
    #         print(f"\n‚úÖ Successfully loaded {len(law_docs)} law document chunks!")
    #         return law_docs
            
    #     else:
    #         print(f"‚ùå File {law_file_path} not found!")
    #         print(f"   Expected path: {os.path.abspath(law_file_path)}")
    #         return []

def get_models_to_evaluate():
    """Danh s√°ch c√°c m√¥ h√¨nh c·∫ßn ƒë√°nh gi√°"""
    return [
        {
            'name': 'minhquan6203/paraphrase-vietnamese-law',
            'type': 'transformers',
            'description': 'M√¥ h√¨nh Sentence Similarity ƒë√£ fine tune tr√™n b·ªô lu·∫≠t ph√°p Vi·ªát Nam',
            'max_length': 512
        },
        {
            'name': 'sentence-transformers/paraphrase-multilingual-mpnet-base-v2',
            'type': 'sentence_transformers',
            'description': 'M√¥ h√¨nh c∆° s·ªü ƒëa ng√¥n ng·ªØ (base model)',
            'max_length': 512
        },
        {
            'name': 'namnguyenba2003/Vietnamese_Law_Embedding_finetuned_v3_256dims',
            'type': 'transformers',
            'description': 'M√¥ h√¨nh embedding lu·∫≠t Vi·ªát Nam v·ªõi 256 dimensions',
            'max_length': 512
        },
        {
            'name': 'truro7/vn-law-embedding',
            'type': 'sentence_transformers',
            'description': 'M√¥ h√¨nh embedding lu·∫≠t Vi·ªát Nam b·∫±ng Sentence Transformers',
            'max_length': 512
        }
    ]

def load_question_benchmark(random_sample=50):
    """Load c√°c c√¢u h·ªèi t·ª´ file Excel ƒë·ªÉ l√†m benchmark queries"""
    try:
        with open("data_files/law_questions.json", 'r', encoding='utf-8') as f:
            questions = json.load(f)

        print(f"‚úÖ Loaded {len(questions)} benchmark questions from Excel files")

        # Random sample 50 questions
        if len(questions) > random_sample:
            import random
            questions_sample = random.sample(questions, random_sample)
            print(f"üé≤ Randomly selected {random_sample} questions for evaluation")
        else:
            questions_sample = questions
            print(f"üìä Using all {len(questions)} questions (less than {random_sample})")

        # Tr·∫£ v·ªÅ list c√°c query (ch·ªâ l·∫•y primary_query)
        queries = [q['primary_query'] for q in questions_sample]

        # Th·ªëng k√™ theo category
        category_stats = {}
        for q in questions_sample:
            cat = q.get('full_category', 'Unknown')
            category_stats[cat] = category_stats.get(cat, 0) + 1

        print(f"üìä Sample distribution by category:")
        for cat, count in sorted(category_stats.items()):
            print(f"   - {cat}: {count} questions")

        return queries, questions_sample

    except FileNotFoundError:
        print("‚ö†Ô∏è data_files/law_questions.json not found. Using default queries.")
        return get_default_benchmark_queries(), []
    except Exception as e:
        print(f"‚ö†Ô∏è Error loading questions: {e}. Using default queries.")
        return get_default_benchmark_queries(), []

def get_default_benchmark_queries():
    """C√°c query m·∫∑c ƒë·ªãnh n·∫øu kh√¥ng c√≥ file questions"""
    return [
        "Ng∆∞·ªùi b·ªã b·ªánh t√¢m th·∫ßn l√† ng∆∞·ªùi m·∫•t nƒÉng l·ª±c h√†nh vi d√¢n s·ª± l√† ƒë√∫ng hay sai?",
        "S·ª± th·ªèa thu·∫≠n c·ªßa c√°c b√™n kh√¥ng vi ph·∫°m ƒëi·ªÅu c·∫•m c·ªßa ph√°p lu·∫≠t, kh√¥ng tr√°i ƒë·∫°o ƒë·ª©c x√£ h·ªôi th√¨ ƒë∆∞·ª£c g·ªçi l√† h·ª£p ƒë·ªìng l√† ƒë√∫ng hay sai?",
        "Ti·ªÅn ph√∫ng vi·∫øng ƒë√°m ma c≈©ng thu·ªôc di s·∫£n th·ª´a k·∫ø c·ªßa ng∆∞·ªùi ch·∫øt ƒë·ªÉ l·∫°i l√† ƒë√∫ng hay sai?",
        "C√° nh√¢n ƒë∆∞·ª£c h∆∞·ªüng di s·∫£n th·ª´a k·∫ø ph·∫£i tr·∫£ n·ª£ thay cho ng∆∞·ªùi ƒë·ªÉ l·∫°i di s·∫£n l√† ƒë√∫ng hay sai?",
        "H·ª£p ƒë·ªìng v√¥ hi·ªáu l√† h·ª£p ƒë·ªìng vi ph·∫°m ph√°p lu·∫≠t l√† ƒë√∫ng hay sai?",
        "Hoa l·ª£i, l·ª£i t·ª©c ph√°t sinh t·ª´ t√†i s·∫£n ri√™ng c·ªßa m·ªôt b√™n v·ª£ ho·∫∑c ch·ªìng s·∫Ω l√† t√†i s·∫£n chung n·∫øu hoa l·ª£i, l·ª£i t·ª©c ƒë√≥ l√† ngu·ªìn s·ªëng duy nh·∫•t c·ªßa gia ƒë√¨nh.",
        "H·ªôi Li√™n hi·ªáp ph·ª• n·ªØ c√≥ quy·ªÅn y√™u c·∫ßu T√≤a √°n ra quy·∫øt ƒë·ªãnh h·ªßy k·∫øt h√¥n tr√°i ph√°p lu·∫≠t do vi ph·∫°m s·ª± t·ª± nguy·ªán.",
        "H√¥n nh√¢n ch·ªâ ch·∫•m d·ª©t khi m·ªôt b√™n v·ª£, ch·ªìng ch·∫øt.",
        "K·∫øt h√¥n c√≥ y·∫øu t·ªë n∆∞·ªõc ngo√†i c√≥ th·ªÉ ƒëƒÉng k√Ω t·∫°i UBND c·∫•p x√£.",
        "Khi cha m·∫π kh√¥ng th·ªÉ nu√¥i d∆∞·ª°ng, c·∫•p d∆∞·ª°ng ƒë∆∞·ª£c cho con, th√¨ √¥ng b√† ph·∫£i c√≥ nghƒ©a v·ª• nu√¥i d∆∞·ª°ng ho·∫∑c c·∫•p d∆∞·ª°ng cho ch√°u.",
        "Khi h√¥n nh√¢n ch·∫•m d·ª©t, m·ªçi quy·ªÅn v√† nghƒ©a v·ª• gi·ªØa nh·ªØng ng∆∞·ªùi ƒë√£ t·ª´ng l√† v·ª£ ch·ªìng c≈©ng ch·∫•m d·ª©t.",
        "Khi v·ª£ ch·ªìng ly h√¥n, con d∆∞·ªõi 36 th√°ng tu·ªïi ƒë∆∞·ª£c giao cho ng∆∞·ªùi v·ª£ tr·ª±c ti·∫øp nu√¥i d∆∞·ª°ng.",
        "Khi v·ª£ ho·∫∑c ch·ªìng th·ª±c hi·ªán nh·ªØng giao d·ªãch ph·ª•c v·ª• cho nhu c·∫ßu thi·∫øt y·∫øu c·ªßa gia ƒë√¨nh m√† kh√¥ng c√≥ s·ª± ƒë·ªìng √Ω c·ªßa b√™n kia th√¨ ng∆∞·ªùi th·ª±c hi·ªán giao d·ªãch ƒë√≥ ph·∫£i thanh to√°n b·∫±ng t√†i s·∫£n ri√™ng c·ªßa m√¨nh.",
        "Khi kh√¥ng s·ªëng chung c√πng v·ªõi cha m·∫π, con ƒë√£ th√†nh ni√™n c√≥ kh·∫£ nƒÉng lao ƒë·ªông ph·∫£i c·∫•p d∆∞·ª°ng cho cha m·∫π.",
        "Ch·ªâ UBND c·∫•p t·ªânh n∆°i c√¥ng d√¢n Vi·ªát Nam c∆∞ tr√∫ m·ªõi c√≥ th·∫©m quy·ªÅn ƒëƒÉng k√Ω vi·ªác k·∫øt h√¥n gi·ªØa c√¥ng d√¢n Vi·ªát Nam v·ªõi ng∆∞·ªùi n∆∞·ªõc ngo√†i.",
        "ƒêi·ªÅu ki·ªán k·∫øt h√¥n c·ªßa nam v√† n·ªØ theo ph√°p lu·∫≠t Vi·ªát Nam l√† g√¨?",
        "T√†i s·∫£n n√†o ƒë∆∞·ª£c coi l√† t√†i s·∫£n chung c·ªßa v·ª£ ch·ªìng?",
        "Th·ªß t·ª•c ly h√¥n t·∫°i t√≤a √°n ƒë∆∞·ª£c quy ƒë·ªãnh nh∆∞ th·∫ø n√†o?",
        "Quy·ªÅn v√† nghƒ©a v·ª• c·ªßa cha m·∫π ƒë·ªëi v·ªõi con ch∆∞a th√†nh ni√™n?",
        "Tr∆∞·ªùng h·ª£p n√†o v·ª£ ch·ªìng c√≥ th·ªÉ th·ªèa thu·∫≠n v·ªÅ ch·∫ø ƒë·ªô t√†i s·∫£n?"
    ]

def get_benchmark_queries():
    """C√°c query t·ª´ benchmark - ∆∞u ti√™n t·ª´ Excel, fallback v·ªÅ default"""
    # Note: This function is kept for backward compatibility
    # Main logic moved to load_question_benchmark() called directly in main()
    queries, questions_sample = load_question_benchmark()
    return queries

def search_top_k(query_embedding, doc_embeddings, k=15):
    """Deprecated: ƒë·ªÉ t∆∞∆°ng th√≠ch c≈©. Kh√¥ng c√≤n d√πng khi ƒë√£ chuy·ªÉn sang Qdrant."""
    similarities = cosine_similarity([query_embedding], doc_embeddings)[0]
    top_indices = np.argsort(similarities)[::-1][:k]
    top_scores = similarities[top_indices]
    return top_indices, top_scores

def calculate_metrics(scores, threshold_07=0.7, threshold_05=0.5):
    """T√≠nh to√°n c√°c metrics ƒë√°nh gi√° ch·∫•t l∆∞·ª£ng retrieval"""
    return {
        "max_score": float(np.max(scores)),
        "avg_top3": float(np.mean(scores[:3])) if len(scores) >= 3 else float(np.mean(scores)),
        "avg_top5": float(np.mean(scores[:5])) if len(scores) >= 5 else float(np.mean(scores)),
        "avg_top10": float(np.mean(scores[:10])) if len(scores) >= 10 else float(np.mean(scores)),
        "avg_all": float(np.mean(scores)),
        "scores_above_07": int(np.sum(scores >= threshold_07)),
        "scores_above_05": int(np.sum(scores >= threshold_05)),
        "min_score": float(np.min(scores))
    }

def display_search_results(query, law_docs, top_indices, top_scores, max_display=5):
    """Hi·ªÉn th·ªã k·∫øt qu·∫£ search m·ªôt c√°ch r√µ r√†ng"""
    print(f"üìù Query: {query}")
    
    # B·∫£o ƒë·∫£m s·∫Øp x·∫øp gi·∫£m d·∫ßn theo ƒëi·ªÉm s·ªë
    if not isinstance(top_scores, np.ndarray):
        top_scores = np.array(top_scores)
    if not isinstance(top_indices, np.ndarray):
        top_indices = np.array(top_indices)
    order = np.argsort(top_scores)[::-1]
    top_indices = top_indices[order]
    top_scores = top_scores[order]

    print(f"üéØ Top {min(max_display, len(top_indices))} Results:")
    
    for i in range(min(max_display, len(top_indices))):
        idx = int(top_indices[i])
        score = float(top_scores[i])
        doc = law_docs[idx]
        
        print(f"\n   {i+1}. Score: {score:.4f} | {doc['id']}")
        print(f"      Length: {len(doc['content'])} chars")
        print("      Content:")
        print(doc['content'])
        
        if doc.get('metadata') and doc['metadata']:
            # Ch·ªâ hi·ªÉn th·ªã m·ªôt s·ªë metadata quan tr·ªçng
            important_meta = {
                k: v for k, v in doc['metadata'].items()
                if k in ['exact_citation', 'chapter', 'article_no', 'clause_no', 'point_letter', 'source_file_name']
            }
            if important_meta:
                metadata_str = ", ".join([f"{k}: {v}" for k, v in important_meta.items()])
            print(f"      Metadata: {metadata_str}")

def evaluate_single_model(model_info, law_docs, queries, top_k=15, show_detailed_results=True, device="cuda"):
    """ƒê√°nh gi√° m·ªôt m√¥ h√¨nh embedding"""
    print(f"\n{'='*80}")
    print(f"üîç EVALUATING MODEL: {model_info['name']}")
    print(f"üìù Description: {model_info['description']}")
    print(f"üîß Type: {model_info['type']} | Max Length: {model_info['max_length']} tokens")
    print(f"{'='*80}")
    
    try:
        # B∆∞·ªõc 1: Chu·∫©n b·ªã texts
        doc_texts = [doc['content'] for doc in law_docs]
        print(f"\nüìö Step 1: Prepared {len(doc_texts)} document texts")
        
        # B∆∞·ªõc 2: Ki·ªÉm tra Qdrant collection tr∆∞·ªõc
        client = get_qdrant_client()
        collection_name = model_info['name'].replace('/', '_')
        existing = count_collection_points(client, collection_name)
        
        if existing >= len(law_docs):
            print(f"üü° Collection '{collection_name}' already has {existing} vectors (>= {len(law_docs)}). Skipping document encoding.")
        else:
            print(f"üü† Collection '{collection_name}' has {existing} vectors. Need to encode and upsert {len(law_docs)} vectors...")
            
            # B∆∞·ªõc 2a: Encode documents
            print(f"\nüî® Step 2a: Encoding documents...")
            if model_info['type'] == 'sentence_transformers':
                doc_embeddings = encode_with_sentence_transformers(
                    doc_texts, 
                    model_info['name'], 
                    batch_size=16,
                    device=device
                )
            else:
                doc_embeddings = encode_with_transformers(
                    doc_texts, 
                    model_info['name'], 
                    max_length=model_info['max_length'],
                    batch_size=16,
                    device=device
                )
            
            print(f"   ‚úÖ Document embeddings shape: {doc_embeddings.shape}")
            
            # B∆∞·ªõc 2b: L∆∞u v√†o Qdrant
            print(f"\nüíæ Step 2b: Storing embeddings in Qdrant...")
            ensure_collection(client, collection_name, vector_size=doc_embeddings.shape[1])
            upsert_embeddings_to_qdrant(client, collection_name, doc_embeddings, law_docs)
            
            # Gi·∫£i ph√≥ng RAM
            del doc_embeddings
            gc.collect()
        
        # B∆∞·ªõc 3: Encode queries
        print(f"\nüîç Step 3: Encoding queries...")
        if model_info['type'] == 'sentence_transformers':
            query_embeddings = encode_with_sentence_transformers(
                queries, 
                model_info['name'], 
                batch_size=16,
                device=device
            )
        else:
            query_embeddings = encode_with_transformers(
                queries, 
                model_info['name'], 
                max_length=model_info['max_length'],
                batch_size=16,
                device=device
            )
        
        print(f"   ‚úÖ Query embeddings shape: {query_embeddings.shape}")
        
        # B∆∞·ªõc 4: Evaluate t·ª´ng query
        print(f"\nüìä Step 4: Evaluating {len(queries)} queries...")
        query_results = []
        all_metrics = []
        
        for i, query in enumerate(queries):
            print(f"\n   üîç Query {i+1}/{len(queries)}")
            
            # Search top-k documents via Qdrant
            top_indices, top_scores = search_qdrant(
                client=client,
                collection_name=collection_name,
                query_embedding=query_embeddings[i],
                top_k=top_k
            )
            
            # Calculate metrics
            metrics = calculate_metrics(top_scores)
            all_metrics.append(metrics)
            
            # Store results
            query_result = {
                'query': query,
                'query_id': i,
                'top_indices': top_indices.tolist(),
                'top_scores': top_scores.tolist(),
                'metrics': metrics
            }
            query_results.append(query_result)
            
            # Show detailed results for first few queries
            if show_detailed_results and i < 3:
                display_search_results(query, law_docs, top_indices, top_scores, max_display=3)
                print(f"      üìà Metrics: Max={metrics['max_score']:.4f}, Avg_top5={metrics['avg_top5']:.4f}, Above_0.7={metrics['scores_above_07']}")
        
        # B∆∞·ªõc 5: Aggregate metrics
        print(f"\nüìà Step 5: Aggregating metrics...")
        
        # Calculate average metrics across all queries
        avg_metrics = {}
        metric_keys = all_metrics[0].keys()
        for key in metric_keys:
            if key.startswith('scores_above'):
                avg_metrics[f"avg_{key}"] = np.mean([m[key] for m in all_metrics])
            else:
                avg_metrics[f"avg_{key}"] = np.mean([m[key] for m in all_metrics])
        
        # Final result
        final_result = {
            'model_name': model_info['name'],
            'model_type': model_info['type'],
            'model_description': model_info['description'],
            'max_length': model_info['max_length'],
            'num_queries': len(queries),
            'num_documents': len(law_docs),
            'top_k': top_k,
            'query_results': query_results,
            'aggregated_metrics': avg_metrics,
            'evaluation_success': True
        }
        
        # Print summary
        print(f"\n‚úÖ EVALUATION COMPLETED SUCCESSFULLY!")
        print(f"   üìä Average Results:")
        print(f"      - Avg Max Score: {avg_metrics['avg_max_score']:.4f}")
        print(f"      - Avg Top-5 Score: {avg_metrics['avg_avg_top5']:.4f}")
        print(f"      - Avg Above 0.7: {avg_metrics['avg_scores_above_07']:.1f}")
        print(f"      - Avg Above 0.5: {avg_metrics['avg_scores_above_05']:.1f}")
        
        return final_result
        
    except Exception as e:
        print(f"\n‚ùå EVALUATION FAILED: {str(e)}")
        return {
            'model_name': model_info['name'],
            'model_type': model_info['type'],
            'error': str(e),
            'evaluation_success': False
        }

def run_evaluation_all_models(models_to_evaluate, law_docs, benchmark_queries, device="cuda"):
    """Ch·∫°y ƒë√°nh gi√° cho t·∫•t c·∫£ c√°c m√¥ h√¨nh"""
    print("üöÄ Starting evaluation for all models...")
    
    if not law_docs:
        print("‚ùå Error: No law documents loaded!")
        return []
    
    print(f"‚úÖ Ready to evaluate with:")
    print(f"   üìö Documents: {len(law_docs)} law chunks")
    print(f"   ‚ùì Queries: {len(benchmark_queries)} benchmark questions")
    print(f"   ü§ñ Models: {len(models_to_evaluate)} models to test")
    print(f"   üéØ Top-K: 15 results per query")
    
    evaluation_results = []
    successful_evaluations = 0
    failed_evaluations = 0
    
    for i, model_info in enumerate(models_to_evaluate):
        print(f"\n{'ü§ñ '*20}")
        print(f"ü§ñ EVALUATING MODEL {i+1}/{len(models_to_evaluate)}")
        print(f"{'ü§ñ '*20}")
        
        try:
            result = evaluate_single_model(
                model_info=model_info,
                law_docs=law_docs,
                queries=benchmark_queries,
                top_k=15,
                show_detailed_results=(i < 2),  # Show details for first 2 models only
                device=device
            )
            
            if result['evaluation_success']:
                evaluation_results.append(result)
                successful_evaluations += 1
                print(f"‚úÖ Model {i+1} evaluation completed successfully!")
            else:
                print(f"‚ùå Model {i+1} evaluation failed: {result.get('error', 'Unknown error')}")
                failed_evaluations += 1
            
            # Wait between models
            if i < len(models_to_evaluate) - 1:
                print(f"‚è≥ Waiting 2 seconds before next model...")
                time.sleep(2)
                
        except Exception as e:
            print(f"‚ùå Unexpected error evaluating model {i+1}: {str(e)}")
            failed_evaluations += 1
            continue
    
    # Final summary
    print(f"\n{'='*100}")
    print(f"üéâ EVALUATION SUMMARY")
    print(f"{'='*100}")
    print(f"‚úÖ Successful evaluations: {successful_evaluations}")
    print(f"‚ùå Failed evaluations: {failed_evaluations}")
    print(f"üìä Total models evaluated: {len(evaluation_results)}")
    
    if evaluation_results:
        print(f"\nüìà Quick Performance Preview:")
        # S·∫Øp x·∫øp theo avg_max_score gi·∫£m d·∫ßn
        preview_sorted = sorted(
            evaluation_results,
            key=lambda r: r['aggregated_metrics']['avg_max_score'],
            reverse=True
        )
        for result in preview_sorted:
            metrics = result['aggregated_metrics']
            model_name = result['model_name'].split('/')[-1]
            print(f"   ü§ñ {model_name}")
            print(f"      Max Score: {metrics['avg_max_score']:.4f} | Top-5: {metrics['avg_avg_top5']:.4f} | Above 0.7: {metrics['avg_scores_above_07']:.1f}")
    
    return evaluation_results

def generate_final_report(evaluation_results, law_docs, benchmark_queries, sample_questions=None):
    """T·∫°o b√°o c√°o chi ti·∫øt cu·ªëi c√πng"""
    print("üìä Generating detailed analysis and final report...")
    
    if not evaluation_results:
        print("‚ùå No evaluation results available!")
        return None
    
    print(f"\n{'='*100}")
    print(f"üìã COMPREHENSIVE EVALUATION REPORT")
    print(f"{'='*100}")
    
    # Sort models by average max score (best first)
    sorted_results = sorted(
        evaluation_results, 
        key=lambda x: x['aggregated_metrics']['avg_max_score'], 
        reverse=True
    )
    
    print(f"üìä DATASET INFORMATION:")
    print(f"   üìö Law Documents: {len(law_docs)} chunks from Lu·∫≠t H√¥n nh√¢n v√† Gia ƒë√¨nh")
    print(f"   ‚ùì Benchmark Queries: {len(benchmark_queries)} questions")
    print(f"   üîç Evaluation Method: Top-15 retrieval with cosine similarity")
    print(f"   üíæ Storage: Qdrant vector database")
    
    print(f"\nüèÜ RANKING BY PERFORMANCE:")
    print(f"   Metric: Average Max Score across all queries")
    
    # Create comparison table
    print(f"\n{'Rank':<4} {'Model':<45} {'Max Score':<10} {'Top-5':<8} {'‚â•0.7':<6} {'‚â•0.5':<6} {'Type':<12}")
    print(f"{'-'*95}")
    
    for i, result in enumerate(sorted_results):
        metrics = result['aggregated_metrics']
        model_name = result['model_name'].split('/')[-1][:40]
        model_type = result['model_type']
        
        print(f"{i+1:<4} {model_name:<45} {metrics['avg_max_score']:<10.4f} "
              f"{metrics['avg_avg_top5']:<8.4f} {metrics['avg_scores_above_07']:<6.1f} "
              f"{metrics['avg_scores_above_05']:<6.1f} {model_type:<12}")
    
    # Best model analysis
    best_model = sorted_results[0]
    print(f"\n‚≠ê RECOMMENDED MODEL:")
    print(f"   ü•á {best_model['model_name']}")
    print(f"   üìù {best_model['model_description']}")
    print(f"   üéØ Performance Highlights:")
    best_metrics = best_model['aggregated_metrics']
    print(f"      - Average Max Score: {best_metrics['avg_max_score']:.4f}")
    print(f"      - Average Top-5 Score: {best_metrics['avg_avg_top5']:.4f}")
    print(f"      - Queries with score ‚â• 0.7: {best_metrics['avg_scores_above_07']:.1f} per query")
    print(f"      - Queries with score ‚â• 0.5: {best_metrics['avg_scores_above_05']:.1f} per query")
    
    # Performance analysis
    print(f"\nüìà PERFORMANCE ANALYSIS:")
    
    # Calculate overall statistics
    all_max_scores = [r['aggregated_metrics']['avg_max_score'] for r in evaluation_results]
    all_top5_scores = [r['aggregated_metrics']['avg_avg_top5'] for r in evaluation_results]
    
    print(f"   üìä Overall Statistics:")
    print(f"      - Best Max Score: {max(all_max_scores):.4f}")
    print(f"      - Worst Max Score: {min(all_max_scores):.4f}")
    print(f"      - Average Max Score: {np.mean(all_max_scores):.4f}")
    print(f"      - Best Top-5 Score: {max(all_top5_scores):.4f}")
    print(f"      - Average Top-5 Score: {np.mean(all_top5_scores):.4f}")
    
    # Model type analysis
    transformers_models = [r for r in evaluation_results if r['model_type'] == 'transformers']
    sentence_transformers_models = [r for r in evaluation_results if r['model_type'] == 'sentence_transformers']
    
    if transformers_models and sentence_transformers_models:
        print(f"\nüîß MODEL TYPE COMPARISON:")
        
        trans_avg = np.mean([r['aggregated_metrics']['avg_max_score'] for r in transformers_models])
        sent_avg = np.mean([r['aggregated_metrics']['avg_max_score'] for r in sentence_transformers_models])
        
        print(f"   üî® Transformers models: {len(transformers_models)} models, avg score: {trans_avg:.4f}")
        print(f"   üì¶ Sentence-transformers: {len(sentence_transformers_models)} models, avg score: {sent_avg:.4f}")
        
        if trans_avg > sent_avg:
            print(f"   ‚úÖ Transformers models perform better on average (+{trans_avg - sent_avg:.4f})")
        else:
            print(f"   ‚úÖ Sentence-transformers models perform better on average (+{sent_avg - trans_avg:.4f})")
    
    # Recommendations
    print(f"\nüí° RECOMMENDATIONS:")
    print(f"   üéØ For Production Deployment:")
    print(f"      - Primary: {best_model['model_name']}")
    print(f"      - Type: {best_model['model_type']}")
    print(f"      - Max Length: {best_model['max_length']} tokens")
    
    if len(sorted_results) > 1:
        second_best = sorted_results[1]
        print(f"   ü•à Alternative Option:")
        print(f"      - {second_best['model_name']}")
        print(f"      - Performance difference: {best_metrics['avg_max_score'] - second_best['aggregated_metrics']['avg_max_score']:.4f}")
    
    print(f"\nüîç DETAILED QUERY ANALYSIS:")
    print(f"   üìù Sample Query Performance (Best Model):")
    
    # Show performance on first 3 queries for best model
    best_query_results = best_model['query_results'][:3]
    for i, qr in enumerate(best_query_results):
        print(f"\n   Query {i+1}: {qr['query'][:60]}...")
        print(f"      Max Score: {qr['metrics']['max_score']:.4f}")
        print(f"      Top-3 Average: {qr['metrics']['avg_top3']:.4f}")
        print(f"      Results ‚â• 0.7: {qr['metrics']['scores_above_07']}")

    # Top queries by total score (best model)
    print(f"\nüîù TOP 3 QUERIES BY TOTAL SCORE (Best Model)")
    # Compute total score = sum of retrieved top_k scores for each query
    query_totals = []
    for qr in best_model['query_results']:
        total = float(np.sum(qr.get('top_scores', [])))
        query_totals.append({
            'query_id': qr['query_id'],
            'query': qr['query'],
            'total_score': total,
            'top_indices': qr.get('top_indices', []),
            'top_scores': qr.get('top_scores', [])
        })
    # Sort and take top 3
    query_totals.sort(key=lambda x: x['total_score'], reverse=True)
    top3_queries = query_totals[:3]

    # Build detailed objects and print
    top_queries_detailed = []
    for rank, q in enumerate(top3_queries, start=1):
        print(f"\n   {rank}. Total Score: {q['total_score']:.4f}")
        print(f"      Query: {q['query']}")
        # Hi·ªÉn th·ªã th√¥ng tin question t·ª´ Excel n·∫øu c√≥
        question_info = {}
        if sample_questions:
            # Find matching question by query text to ensure correct mapping
            query_text = q['query'].strip()
            for sq in sample_questions:
                if sq['primary_query'].strip() == query_text:
                    question_info = {
                        'category': sq.get('full_category', ''),
                        'positive_answer': sq.get('positive', ''),
                        'negative_answer': sq.get('negative', ''),
                        'source_file': sq.get('source_file', '')
                    }
                    print(f"      Category: {question_info['category']}")
                    if question_info['positive_answer']:
                        print(f"      Expected Answer: {question_info['positive_answer'][:100]}...")
                    if question_info['negative_answer']:
                        print(f"      Negative Answer: {question_info['negative_answer'][:100]}...")
                    break

        # Print top 3 results for this query with FULL CONTENT
        top_n = min(3, len(q['top_indices']))
        answers = []
        for i in range(top_n):
            idx = int(q['top_indices'][i])
            score = float(q['top_scores'][i])
            if 0 <= idx < len(law_docs):
                doc = law_docs[idx]
                md = doc.get('metadata') or {}
                citation = md.get('exact_citation') or ''
                law_id = md.get('law_id', '')

                print(f"\n         üìÑ Rank {i+1}: Score {score:.4f} | Law: {law_id}")
                print(f"         üìù Citation: {citation}")
                print(f"         üìñ Content: {doc['content'][:300]}...")
                if len(doc['content']) > 300:
                    print(f"         ... ({len(doc['content'])} chars total)")

                # collect for JSON
                answers.append({
                    'rank': i+1,
                    'score': score,
                    'citation': citation,
                    'law_id': law_id,
                    'content': doc.get('content', ''),
                    'metadata': md
                })
            else:
                print(f"         - Rank {i+1}: {score:.4f} | [Index {idx} out of range]")
                answers.append({
                    'rank': i+1,
                    'score': score,
                    'citation': '',
                    'law_id': '',
                    'content': '',
                    'metadata': {},
                    'note': f'Index {idx} out of range'
                })
        # Th√™m th√¥ng tin question t·ª´ Excel (ƒë√£ ƒë∆∞·ª£c set ·ªü tr√™n)

        top_queries_detailed.append({
            'rank': rank,
            'query_id': q['query_id'],
            'query': q['query'],
            'total_score': q['total_score'],
            'question_info': question_info,
            'top_answers': answers
        })

    # Attach into the best model object for JSON output
    try:
        best_model['top_queries_by_total_score'] = top_queries_detailed
    except Exception:
        pass
    
    # Save summary
    evaluation_summary = {
        'best_model': best_model['model_name'],
        'best_score': best_metrics['avg_max_score'],
        'total_models_evaluated': len(evaluation_results),
        'total_queries': len(benchmark_queries),
        'total_documents': len(law_docs),
        'sorted_results': sorted_results,
        # attach preview for convenience
        'top_queries_preview': [
            {
                'query_id': q['query_id'],
                'query': q['query'],
                'total_score': q['total_score'],
                'top3': [
                    {
                        'doc_index': int(q['top_indices'][i]),
                        'score': float(q['top_scores'][i])
                    } for i in range(min(3, len(q['top_indices'])))
                ]
            } for q in top3_queries
        ],
        'top_queries_by_total_score': top_queries_detailed
    }
    
    print(f"\n{'='*100}")
    print(f"‚úÖ EVALUATION COMPLETED SUCCESSFULLY!")
    print(f"üìä Summary saved to evaluation_summary")
    print(f"üìã Full results available in evaluation_results")
    print(f"{'='*100}")
    
    # Export option
    print(f"\nüíæ EXPORT OPTIONS:")
    print(f"   To save results to JSON file:")
    print(f"   ‚Üí import json")
    print(f"   ‚Üí with open('results/embedding_evaluation_results.json', 'w', encoding='utf-8') as f:")
    print(f"   ‚Üí     json.dump(evaluation_results, f, ensure_ascii=False, indent=2)")
    
    print(f"\nüéâ Report generation completed!")
    
    return evaluation_summary

def test_single_query(best_model_name, models_to_evaluate, law_docs, device="cuda"):
    """Test v·ªõi m·ªôt query ƒë∆°n l·∫ª"""
    print("üß™ Quick test with single query...")
    
    test_query = "D·ª± √°n ƒë·∫ßu t∆∞ x√¢y d·ª±ng khu ƒë√¥ th·ªã ph·∫£i c√≥ c√¥ng nƒÉng h·ªón h·ª£p, ƒë·ªìng b·ªô h·∫° t·∫ßng h·∫° t·∫ßng k·ªπ thu·∫≠t, h·∫° t·∫ßng x√£ h·ªôi v√† nh√† ·ªü theo quy ho·∫°ch ƒë∆∞·ª£c ph√™ duy·ªát?"
    print(f"üìù Test Query: {test_query}")
    print(f"ü•á Using best model: {best_model_name}")
    
    # Find model info
    best_model_info = None
    for model in models_to_evaluate:
        if model['name'] == best_model_name:
            best_model_info = model
            break
    
    if best_model_info:
        print(f"üîç Testing single query with model `{best_model_info['name']}`...")
        
        # Encode query
        if best_model_info['type'] == 'sentence_transformers':
            test_query_embedding = encode_with_sentence_transformers([test_query], best_model_info['name'], device=device)[0]
        else:
            test_query_embedding = encode_with_transformers([test_query], best_model_info['name'], best_model_info['max_length'], device=device)[0]
        
        # Search tr·ª±c ti·∫øp tr√™n Qdrant (documents ƒë√£ ƒë∆∞·ª£c upsert khi evaluate)
        client = get_qdrant_client()
        collection_name = best_model_info['name'].replace('/', '_')
        top_indices, top_scores = search_qdrant(client, collection_name, test_query_embedding, top_k=10)
        
        # Display results
        display_search_results(test_query, law_docs, top_indices, top_scores, max_display=5)
        
        # Metrics
        metrics = calculate_metrics(top_scores)
        print(f"\nüìä Metrics for this query:")
        print(f"   - Max Score: {metrics['max_score']:.4f}")
        print(f"   - Top-5 Average: {metrics['avg_top5']:.4f}")
        print(f"   - Results ‚â• 0.7: {metrics['scores_above_07']}")
        print(f"   - Results ‚â• 0.5: {metrics['scores_above_05']}")
    else:
        print("‚ùå Could not find model info for testing")
    
    print(f"\n‚úÖ Single query test completed!")

def main():
    """H√†m ch√≠nh ch·∫°y to√†n b·ªô evaluation"""
    print("=" * 80)
    print("üî¨ ƒê√ÅNH GI√Å M√î H√åNH EMBEDDING CHO LU·∫¨T TI·∫æNG VI·ªÜT")
    print("=" * 80)
    
    # 1. Setup environment
    device = setup_environment()
    
    # 2. Load law documents
    law_docs = load_all_law_documents()
    if not law_docs:
        print("‚ùå Cannot proceed without law documents!")
        return
    
    # 3. Get models and queries
    models_to_evaluate = get_models_to_evaluate()
    benchmark_queries, sample_questions = load_question_benchmark()  # Get benchmark queries and sample questions
    
    print(f"\nü§ñ Prepared {len(models_to_evaluate)} models for evaluation:")
    for i, model in enumerate(models_to_evaluate):
        print(f"   {i+1}. {model['name']}")
        print(f"      Type: {model['type']} | Max Length: {model['max_length']} tokens")
        print(f"      Description: {model['description']}")
        print()
    
    print(f"üéØ All models support ‚â•512 tokens as required!")
    print(f"üíæ Embeddings will be stored in Qdrant vector database")
    
    print(f"\nPrepared {len(benchmark_queries)} benchmark queries from Excel files")
    print("Sample queries:")
    for i, query in enumerate(benchmark_queries[:5]):
        print(f"{i+1}. {query}")
    
    # 4. Run evaluation for all models
    evaluation_results = run_evaluation_all_models(models_to_evaluate, law_docs, benchmark_queries, device)
    
    # 5. Generate final report
    evaluation_summary = generate_final_report(evaluation_results, law_docs, benchmark_queries, sample_questions)
    
    # 6. Test single query with best model
    if evaluation_summary:
        test_single_query(evaluation_summary['best_model'], models_to_evaluate, law_docs, device)
    
    # 7. Save results to JSON
    if evaluation_results:
        try:
            # Save full results
            with open('results/embedding_evaluation_results.json', 'w', encoding='utf-8') as f:
                json.dump(evaluation_results, f, ensure_ascii=False, indent=2)
            print(f"\nüíæ Full results saved to: results/embedding_evaluation_results.json")

            # Also save clean top queries analysis
            top_queries_summary = []
            for model_result in evaluation_results:
                model_name = model_result['model_name']
                if 'top_queries_by_total_score' in model_result:
                    top_queries_summary.append({
                        "model_name": model_name,
                        "top_queries_by_total_score": model_result['top_queries_by_total_score']
                    })

            if top_queries_summary:
                with open('results/top_queries_analysis.json', 'w', encoding='utf-8') as f:
                    json.dump(top_queries_summary, f, ensure_ascii=False, indent=2)
                print(f"üíæ Top queries analysis saved to: results/top_queries_analysis.json")

        except Exception as e:
            print(f"\n‚ö†Ô∏è Could not save results to JSON: {e}")
    
    print(f"\nüéâ EVALUATION COMPLETED! üéâ")

if __name__ == "__main__":
    main()
