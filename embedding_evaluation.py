#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ÄÃ¡nh GiÃ¡ MÃ´ HÃ¬nh Embedding Cho Luáº­t Tiáº¿ng Viá»‡t
Chuyá»ƒn Ä‘á»•i tá»« notebook thÃ nh script Ä‘á»ƒ cháº¡y má»™t láº§n
"""

import warnings
warnings.filterwarnings('ignore')

import torch
import numpy as np
import pandas as pd
import os
import sys
import re
import json
import time
import gc
from tqdm import tqdm
from sklearn.metrics.pairwise import cosine_similarity
from qdrant_client import QdrantClient
from qdrant_client.http.models import Distance, VectorParams, PointStruct
from dotenv import load_dotenv
load_dotenv()

def check_version(package_name, expected_version=None):
    """Kiá»ƒm tra phiÃªn báº£n package"""
    try:
        import importlib.metadata
        version = importlib.metadata.version(package_name)
        if expected_version and version != expected_version:
            print(f"âš ï¸  {package_name}: expected {expected_version}, got {version}")
        else:
            print(f"âœ… {package_name}: {version}")
        return True, version
    except Exception as e:
        print(f"âŒ {package_name}: not found or error ({e})")
        return False, None

def setup_environment():
    """Thiáº¿t láº­p mÃ´i trÆ°á»ng vÃ  import cÃ¡c thÆ° viá»‡n cáº§n thiáº¿t"""
    print("ğŸ”„ Importing core libraries...")
    
    # Check critical versions
    check_version("huggingface_hub", "0.16.4")
    check_version("tokenizers", "0.13.3")
    check_version("transformers", "4.32.1")
    check_version("sentence-transformers", "2.2.2")
    
    # Import transformers
    print("\nğŸ”„ Importing transformers...")
    try:
        from transformers import AutoTokenizer, AutoModel
        print("âœ… transformers imported successfully")
        
        # Quick functionality test
        test_tokenizer = AutoTokenizer.from_pretrained("distilbert-base-uncased")
        test_tokens = test_tokenizer("test", return_tensors="pt")
        print("âœ… transformers functionality test passed")
        del test_tokenizer, test_tokens
        
    except Exception as e:
        print(f"âŒ transformers failed: {e}")
        raise
    
    # Import sentence-transformers
    print("\nğŸ”„ Importing sentence-transformers...")
    try:
        from sentence_transformers import SentenceTransformer
        print("âœ… sentence-transformers imported successfully")
        
        # Quick functionality test
        test_model = SentenceTransformer("all-MiniLM-L6-v2")
        test_embedding = test_model.encode(["test sentence"])
        print(f"âœ… sentence-transformers functionality test passed (embedding shape: {test_embedding.shape})")
        del test_model, test_embedding
        
    except Exception as e:
        print(f"âŒ sentence-transformers failed: {e}")
        raise
    
    # Import document processing
    print("\nğŸ”„ Importing document processing...")
    try:
        from docx import Document
        print("âœ… python-docx imported successfully")
    except ImportError:
        print("ğŸ“¦ Installing python-docx...")
        import subprocess
        subprocess.check_call([sys.executable, "-m", "pip", "install", "python-docx==0.8.11"])
        from docx import Document
        print("âœ… python-docx installed and imported")
    
    # Device configuration
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"\nğŸ–¥ï¸  Using device: {device}")
    if device == "cuda":
        print(f"   GPU: {torch.cuda.get_device_name()}")
        print(f"   GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB")
    
    print("\nğŸ‰ All libraries loaded successfully! Ready for evaluation.")
    return device

def get_qdrant_client():
    """Khá»Ÿi táº¡o Qdrant client tá»« biáº¿n mÃ´i trÆ°á»ng.
    """
    url = os.getenv("QDRANT_URL")
    api_key = os.getenv("QDRANT_API_KEY")

    try:
        client = QdrantClient(
            url=url, 
            api_key=api_key or None, 
            timeout=300.0,
            grpc_port=6334,
        )
        print("ğŸ—„ï¸  Qdrant connected successfully")
        return client
    except Exception as e:
        print(f"âŒ Cannot connect to Qdrant: {e}")
        raise

def ensure_collection(client: QdrantClient, collection_name: str, vector_size: int):
    """Táº¡o má»›i (recreate) collection vá»›i cáº¥u hÃ¬nh cosine vÃ  kÃ­ch thÆ°á»›c vector tÆ°Æ¡ng á»©ng."""
    client.recreate_collection(
        collection_name=collection_name,
        vectors_config=VectorParams(size=vector_size, distance=Distance.COSINE)
    )
    print(f"âœ… Collection ready: {collection_name} (dim={vector_size})")


def upsert_embeddings_to_qdrant(client: QdrantClient, collection_name: str, embeddings: np.ndarray, law_docs: list, batch_size=100):
    """Upsert toÃ n bá»™ embeddings vÃ  payload vÃ o Qdrant theo batch nhá»."""
    total_points = len(embeddings)
    print(f"ğŸ“¤ Upserting {total_points} vectors in batches of {batch_size}...")
    
    for i in range(0, total_points, batch_size):
        batch_end = min(i + batch_size, total_points)
        batch_points = []
        
        for idx in range(i, batch_end):
            payload = law_docs[idx]
            batch_points.append(PointStruct(id=idx, vector=embeddings[idx].tolist(), payload=payload))
        
        # Retry mechanism
        max_retries = 3
        for retry in range(max_retries):
            try:
                client.upsert(collection_name=collection_name, points=batch_points)
                print(f"   âœ… Batch {i//batch_size + 1}: upserted {len(batch_points)} vectors ({batch_end}/{total_points})")
                break
            except Exception as e:
                if retry < max_retries - 1:
                    print(f"   âš ï¸ Batch {i//batch_size + 1} failed (attempt {retry + 1}/{max_retries}): {e}")
                    print(f"   ğŸ”„ Retrying in 2 seconds...")
                    time.sleep(2)
                else:
                    print(f"   âŒ Batch {i//batch_size + 1} failed after {max_retries} attempts: {e}")
                    raise
    
    print(f"âœ… Successfully upserted {total_points} vectors into Qdrant collection '{collection_name}'")

def search_qdrant(client: QdrantClient, collection_name: str, query_embedding: np.ndarray, top_k: int):
    """Search top-k báº±ng Qdrant, tráº£ vá» (indices, scores)."""
    hits = client.search(
        collection_name=collection_name,
        query_vector=query_embedding.tolist(),
        limit=top_k
    )
    top_indices = [hit.id for hit in hits]
    top_scores = [float(hit.score) for hit in hits]
    return np.array(top_indices), np.array(top_scores, dtype=float)

def count_collection_points(client: QdrantClient, collection_name: str) -> int:
    try:
        ct = client.count(collection_name=collection_name, exact=True)
        return int(getattr(ct, 'count', 0))
    except Exception:
        return 0

# Roman numeral converter
ROMAN_MAP = {'I':1,'V':5,'X':10,'L':50,'C':100,'D':500,'M':1000}

def roman_to_int(s: str):
    """Chuyá»ƒn Ä‘á»•i sá»‘ La MÃ£ sang sá»‘ nguyÃªn"""
    s = s.upper().strip()
    if not s or any(ch not in ROMAN_MAP for ch in s):
        return None
    total = 0
    prev = 0
    for ch in reversed(s):
        val = ROMAN_MAP[ch]
        if val < prev:
            total -= val
        else:
            total += val
            prev = val
    return total

def mean_pooling(model_output, attention_mask):
    """Mean pooling Ä‘á»ƒ táº¡o sentence embeddings tá»« token embeddings"""
    token_embeddings = model_output[0]
    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()
    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)

def encode_with_transformers(texts, model_name, max_length=512, batch_size=32, device="cuda"):
    """Encode texts using transformers library vá»›i mean pooling"""
    from transformers import AutoTokenizer, AutoModel
    
    print(f"Loading model: {model_name}")
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModel.from_pretrained(model_name).to(device)
    model.eval()
    
    all_embeddings = []
    
    with torch.no_grad():
        for i in tqdm(range(0, len(texts), batch_size), desc="Encoding"):
            batch = texts[i:i+batch_size]
            
            # Tokenize
            encoded = tokenizer(
                batch, 
                padding=True, 
                truncation=True, 
                max_length=max_length,
                return_tensors='pt'
            ).to(device)
            
            # Get model output
            model_output = model(**encoded)
            
            # Mean pooling
            sentence_embeddings = mean_pooling(model_output, encoded['attention_mask'])
            
            # Normalize embeddings
            sentence_embeddings = torch.nn.functional.normalize(sentence_embeddings, p=2, dim=1)
            
            # Move to CPU and convert to numpy
            embeddings = sentence_embeddings.cpu().numpy()
            all_embeddings.append(embeddings)
            
            # Clear GPU memory
            del encoded, model_output, sentence_embeddings, embeddings
            torch.cuda.empty_cache() if device == "cuda" else None
    
    # Delete model to free memory
    del model, tokenizer
    torch.cuda.empty_cache() if device == "cuda" else None
    
    # Combine all embeddings
    final_embeddings = np.vstack(all_embeddings)
    print(f"   âœ… Generated {final_embeddings.shape[0]} embeddings")
    
    return final_embeddings

def encode_with_sentence_transformers(texts, model_name, batch_size=32, device="cuda"):
    """Encode texts using sentence-transformers library"""
    from sentence_transformers import SentenceTransformer
    
    print(f"Loading model: {model_name}")
    model = SentenceTransformer(model_name)
    model.to(device)
    
    # Encode all texts
    embeddings = model.encode(
        texts,
        batch_size=batch_size,
        show_progress_bar=True,
        convert_to_numpy=True,
        normalize_embeddings=True
    )
    
    # Delete model to free memory
    del model
    torch.cuda.empty_cache() if device == "cuda" else None
    
    print(f"   âœ… Generated {embeddings.shape[0]} embeddings")
    
    return embeddings

def read_docx(file_path):
    """Äá»c file docx vÃ  tráº£ vá» text (há»— trá»£ cáº£ .doc vÃ  .docx)"""
    print(f"   Reading file: {file_path}")
    
    try:
        # Thá»­ Ä‘á»c báº±ng python-docx trÆ°á»›c (cho file .docx thá»±c sá»±)
        from docx import Document
        doc = Document(file_path)
        text = "\n".join((p.text or "").strip() for p in doc.paragraphs)
        print(f"   âœ… Successfully read {len(text):,} characters using python-docx")
        return text
    except Exception as e1:
        print(f"   âš ï¸ python-docx failed: {e1}")
        
        # Náº¿u file cÃ³ extension .docx nhÆ°ng thá»±c táº¿ lÃ  .doc, thá»­ dÃ¹ng docx2txt
        try:
            import docx2txt
            text = docx2txt.process(file_path)
            if text and len(text.strip()) > 0:
                print(f"   âœ… Successfully read {len(text):,} characters using docx2txt")
                return text
            else:
                print(f"   âŒ docx2txt returned empty content")
                return ""
        except Exception as e2:
            print(f"   âŒ docx2txt also failed: {e2}")
            return ""

def normalize_lines(text: str):
    """Chuáº©n hÃ³a cÃ¡c dÃ²ng text - loáº¡i bá» whitespace thá»«a"""
    lines = [re.sub(r'\s+$', '', ln) for ln in text.splitlines()]
    print(f"   âœ… Normalized {len(lines):,} lines")
    return lines

# ===================== GEMINI EVALUATION AGENT (optional) =====================
# Env controls:
#   CHUNK_AI_REVIEW=true|1        â†’ enable Gemini review for chunks
#   CHUNK_STRICT_OK_ONLY=true|1   â†’ only proceed when Gemini returns status=ok
#   GEMINI_API_KEY                â†’ required when review is enabled

GEMINI_MODEL_NAME = "gemini-2.5-flash"

GEMINI_PROMPT = """Báº¡n lÃ  chuyÃªn gia phÃ¡p Ä‘iá»ƒn hoÃ¡ & kiá»ƒm thá»­ dá»¯ liá»‡u luáº­t.
TÃ´i gá»­i cho báº¡n:
1) Summary thá»‘ng kÃª & cáº£nh bÃ¡o tá»« bá»™ chunking.
2) Má»™t sá»‘ "excerpts" (trÃ­ch Ä‘oáº¡n nguyÃªn vÄƒn) Ä‘áº¡i diá»‡n.
3) Danh má»¥c chunks (id + metadata + content rÃºt gá»n).

Nhiá»‡m vá»¥:
- PHÃT HIá»†N Báº¤T THÆ¯á»œNG (anomalies) trong chunking, vÃ­ dá»¥:
  * Sai thá»© tá»±/chÆ°a â€œstrictâ€ (nháº£y cÃ³c ChÆ°Æ¡ng/Äiá»u/Khoáº£n/Äiá»ƒm).
  * Nháº­n diá»‡n nháº§m â€œKhoáº£nâ€ (khÃ´ng pháº£i dáº¡ng `1.`) hoáº·c â€œÄiá»ƒmâ€ (khÃ´ng pháº£i `a)`).
  * KhÃ´ng tiÃªm intro khoáº£n vÃ o Ä‘iá»ƒm khi Ä‘Ã£ cÃ³ chuá»—i Ä‘iá»ƒm.
  * Thiáº¿u/bá» sÃ³t ná»™i dung so vá»›i excerpts.
  * Metadata khÃ´ng khá»›p: article_no/clause_no/point_letter/exact_citation.
  * ÄÃ³ng má»Ÿ chuá»—i Ä‘iá»ƒm sai (báº¯t Ä‘áº§u khÃ´ng pháº£i â€œa)â€, chÃ¨n ná»™i dung thÆ°á»ng vÃ o giá»¯a).
  * Ná»™i dung â€œÄiá»uâ€ khÃ´ng cÃ³ khoáº£n nhÆ°ng khÃ´ng sinh chunk intro.
- Gá»¢I Ã Sá»¬A: chá»‰ rÃµ vá»‹ trÃ­ (id / exact_citation), mÃ´ táº£ váº¥n Ä‘á», cÃ¡ch kháº¯c phá»¥c.
- Náº¿u KHÃ”NG tháº¥y váº¥n Ä‘á», xÃ¡c nháº­n â€œokâ€ vÃ  nÃªu ngáº¯n gá»n cÆ¡ sá»Ÿ káº¿t luáº­n.

HÃ£y TRáº¢ Lá»œI CHá»ˆ á»Ÿ dáº¡ng JSON theo schema:
{
  "status": "ok" | "issues_found",
  "confidence": 0.0-1.0,
  "issues": [
    {
      "id": "chuá»—i id chunk hoáº·c mÃ´ táº£ vá»‹ trÃ­",
      "citation": "Äiá»u ... khoáº£n ... Ä‘iá»ƒm ...",
      "severity": "low|medium|high",
      "category": "ordering|regex|metadata|omission|points_chain|format|other",
      "message": "MÃ´ táº£ ngáº¯n gá»n váº¥n Ä‘á»",
      "suggestion": "CÃ¡ch sá»­a ngáº¯n gá»n"
    }
  ],
  "notes": "Ghi chÃº ngáº¯n (tuá»³ chá»n)"
}
Tráº£ JSON há»£p lá»‡. KhÃ´ng giáº£i thÃ­ch ngoÃ i JSON.
"""

def _shorten_text(s: str, max_len: int = 500) -> str:
    if len(s) <= max_len:
        return s
    head = s[: max_len // 2].rstrip()
    tail = s[- max_len // 2 :].lstrip()
    return head + "\n...\n" + tail

def build_review_payload(chunks, summary, raw_text: str, sample_excerpts_chars: int = 8000):
    n = len(raw_text)
    if n <= sample_excerpts_chars:
        excerpts = raw_text
    else:
        k = sample_excerpts_chars // 3
        excerpts = raw_text[:k] + "\n...\n" + raw_text[n//2 - k//2 : n//2 + k//2] + "\n...\n" + raw_text[-k:]

    def lite(c):
        return {
            "id": c.get("metadata", {}).get("exact_citation", ""),
            "metadata": c.get("metadata"),
            "content_preview": _shorten_text(c.get("content", ""), 700)
        }

    chunks_lite = [lite(c) for c in chunks]
    return {
        "summary": summary,
        "excerpts": excerpts,
        "chunks_preview": chunks_lite[:1200]
    }

def _env_truthy(v: str) -> bool:
    return str(v or "").strip().lower() in {"1", "true", "yes", "y", "on"}

def call_gemini_review(payload: dict, api_key: str = None) -> dict:
    api_key = api_key or os.getenv("GEMINI_API_KEY", "").strip()
    if not api_key:
        raise RuntimeError("Thiáº¿u GEMINI_API_KEY trong mÃ´i trÆ°á»ng (.env).")
    try:
        # import at call time
        import google.generativeai as genai
    except Exception as e:
        raise RuntimeError(f"Thiáº¿u thÆ° viá»‡n google-generativeai: {e}")

    genai.configure(api_key=api_key)
    generation_config = {
        "temperature": 0.2,
        "top_p": 0.9,
        "top_k": 40,
        "response_mime_type": "application/json",
    }
    model = genai.GenerativeModel(GEMINI_MODEL_NAME, generation_config=generation_config)
    prompt_parts = [
        {"role": "user", "parts": [{"text": GEMINI_PROMPT}]},
        {"role": "user", "parts": [{"text": json.dumps(payload, ensure_ascii=False)}]},
    ]
    resp = model.generate_content(prompt_parts)
    raw = getattr(resp, "text", None) or (resp.candidates and resp.candidates[0].content.parts[0].text)
    if not raw:
        raise RuntimeError("Gemini khÃ´ng tráº£ ra ná»™i dung.")
    try:
        data = json.loads(raw)
        if not isinstance(data, dict):
            raise ValueError("JSON khÃ´ng pháº£i object")
        data.setdefault("status", "issues_found")
        data.setdefault("issues", [])
        data.setdefault("confidence", 0.0)
        return data
    except Exception as e:
        return {
            "status": "issues_found",
            "confidence": 0.0,
            "issues": [{
                "id": "PARSER",
                "citation": "",
                "severity": "high",
                "category": "other",
                "message": f"KhÃ´ng parse Ä‘Æ°á»£c JSON tá»« Gemini: {e}",
                "suggestion": "Cháº¡y láº¡i hoáº·c giáº£m excerpts."
            }],
            "notes": raw[:2000]
        }

def chunk_law_document(text, law_id="LAW", law_no="", law_title=""):
    """Chia vÄƒn báº£n luáº­t thÃ nh chunks theo Ä‘á»‹nh dáº¡ng hn2014_chunks.json"""
    print("   ğŸ” Chunking law document with strict parsing...")

    lines = normalize_lines(text)

    # Regex patterns
    ARTICLE_RE = re.compile(r'^Äiá»u\s+(\d+)\s*[\.:]?\s*(.*)$', re.UNICODE)
    CHAPTER_RE = re.compile(r'^ChÆ°Æ¡ng\s+([IVXLCDM]+)\s*(.*)$', re.UNICODE | re.IGNORECASE)
    SECTION_RE = re.compile(r'^Má»¥c\s+(\d+)\s*[:\-]?\s*(.*)$', re.UNICODE | re.IGNORECASE)
    CLAUSE_RE = re.compile(r'^\s*(\d+)\.\s*(.*)$', re.UNICODE)
    POINT_RE = re.compile(r'^\s*([a-zA-ZÄ‘Ä])\)\s+(.*)$', re.UNICODE)

    # PASS 1: Pre-scan
    chapters_nums, articles_nums = [], []
    chapters_labels, article_titles_seen = [], []

    for line in lines:
        if not line:
            continue
        m_ch = CHAPTER_RE.match(line)
        if m_ch:
            n = roman_to_int(m_ch.group(1))
            if n:
                chapters_nums.append(n)
                title = (m_ch.group(2) or "").strip()
                chapters_labels.append(
                    f"ChÆ°Æ¡ng {m_ch.group(1).strip()}" + (f" â€“ {title}" if title else "")
                )
            continue
        m_art = ARTICLE_RE.match(line)
        if m_art:
            articles_nums.append(int(m_art.group(1)))
            continue

    chapters_set, articles_set = set(chapters_nums), set(articles_nums)

    # PASS 2: Chunking
    chunks = []
    chapter_label = None
    section_label = None
    article_no = None
    article_title = ""
    expecting_article_title = False
    article_intro_buf = ""
    article_has_any_chunk = False
    clause_no = None
    clause_buf = ""
    clause_intro_current = None
    in_points = False
    point_letter = None
    point_buf = ""
    expected_chapter = None
    expected_article = None
    seeking_article = False

    def build_article_header(article_no: int, article_title: str) -> str:
        t = (article_title or "").strip()
        return f"Äiá»u {article_no}" + (f" {t}" if t else "")

    def flush_article_intro():
        nonlocal article_intro_buf, article_has_any_chunk
        content = article_intro_buf.strip()
        if not content:
            return
        cid = f"{law_id}-D{article_no}"
        exact = f"Äiá»u {article_no}"
        meta = {
            "law_no": law_no,
            "law_title": law_title,
            "law_id": law_id,
            "chapter": chapter_label,
            "section": section_label,
            "article_no": article_no,
            "article_title": article_title,
            "exact_citation": exact
        }
        title_line = f"Äiá»u {article_no}. {article_title}".strip() if article_title else f"Äiá»u {article_no}"
        chunks.append({
            "id": cid,
            "content": f"{title_line}\n{content}",
            "metadata": meta
        })
        article_intro_buf = ""
        article_has_any_chunk = True

    def flush_clause():
        nonlocal clause_buf
        content = clause_buf.strip()
        if not content:
            return
        cid = f"{law_id}-D{article_no}-K{clause_no}"
        exact = f"Äiá»u {article_no} khoáº£n {clause_no}"
        meta = {
            "law_no": law_no,
            "law_title": law_title,
            "law_id": law_id,
            "chapter": chapter_label,
            "section": section_label,
            "article_no": article_no,
            "article_title": article_title,
            "clause_no": clause_no,
            "exact_citation": exact
        }
        art_hdr = build_article_header(article_no, article_title)
        full_content = f"{art_hdr} Khoáº£n {clause_no}. {content}"
        chunks.append({
            "id": cid,
            "content": full_content,
            "metadata": meta
        })

    def flush_point():
        nonlocal point_buf
        content = point_buf.strip()
        if not content:
            return
        letter = point_letter.lower()
        cid = f"{law_id}-D{article_no}-K{clause_no}-{letter}"
        exact = f"Äiá»u {article_no} khoáº£n {clause_no} Ä‘iá»ƒm {letter})"
        meta = {
            "law_no": law_no,
            "law_title": law_title,
            "law_id": law_id,
            "chapter": chapter_label,
            "section": section_label,
            "article_no": article_no,
            "article_title": article_title,
            "clause_no": clause_no,
            "point_letter": letter,
            "exact_citation": exact
        }
        if clause_intro_current:
            meta["clause_intro"] = clause_intro_current

        art_hdr = build_article_header(article_no, article_title)
        if clause_intro_current:
            intro = clause_intro_current.rstrip().rstrip(':')
            full_content = f"{art_hdr} Khoáº£n {clause_no} {intro}, Ä‘iá»ƒm {letter}): {content}"
        else:
            full_content = f"{art_hdr} Khoáº£n {clause_no}, Ä‘iá»ƒm {letter}) {content}"

        chunks.append({
            "id": cid,
            "content": full_content,
            "metadata": meta
        })

    def close_clause():
        nonlocal clause_no, clause_buf, in_points, point_letter, point_buf, article_has_any_chunk, clause_intro_current
        if clause_no is None:
            return
        if in_points and point_letter:
            flush_point()
        elif clause_buf.strip():
            flush_clause()
        article_has_any_chunk = True
        clause_no, clause_buf, in_points, point_letter, point_buf = None, "", False, None, ""
        clause_intro_current = None

    def close_article_if_needed():
        nonlocal article_intro_buf, article_has_any_chunk
        if not article_has_any_chunk and article_intro_buf.strip():
            flush_article_intro()

    print(f"   ğŸ“„ Processing {len(lines):,} lines...")

    for line in lines:
        if not line:
            continue

        if seeking_article:
            m_art_seek = ARTICLE_RE.match(line)
            if m_art_seek:
                a_no = int(m_art_seek.group(1))
                if a_no == expected_article:
                    seeking_article = False
                    close_clause()
                    if article_no is not None:
                        close_article_if_needed()
                    article_no = a_no
                    article_title = (m_art_seek.group(2) or "").strip()
                    if not article_title:
                        expecting_article_title = True
                    expected_article = a_no + 1
                    clause_no = None
                    clause_buf = ""
                    in_points = False
                    point_letter = None
                    point_buf = ""
                    clause_intro_current = None
                continue
            continue

        if expecting_article_title:
            if not any(regex.match(line) for regex in [CHAPTER_RE, SECTION_RE, CLAUSE_RE, POINT_RE, ARTICLE_RE]):
                article_title = line
                expecting_article_title = False
                continue
            else:
                expecting_article_title = False

        # CHÆ¯Æ NG
        m_ch = CHAPTER_RE.match(line)
        if m_ch:
            close_clause()
            if article_no is not None:
                close_article_if_needed()
            article_no = None
            article_title = ""
            article_intro_buf = ""
            expecting_article_title = False

            roman = m_ch.group(1).strip()
            ch_num = roman_to_int(roman) or 0
            ch_title = (m_ch.group(2) or "").strip()
            lbl = f"ChÆ°Æ¡ng {roman}" + (f" â€“ {ch_title}" if ch_title else "")

            if expected_chapter is None:
                expected_chapter = ch_num + 1
            elif ch_num == expected_chapter:
                expected_chapter = ch_num + 1
            elif ch_num > expected_chapter:
                if expected_chapter not in chapters_set:
                    break
                continue
            else:
                continue

            chapter_label = lbl
            section_label = None
            continue

        # Má»¤C
        m_sec = SECTION_RE.match(line)
        if m_sec:
            close_clause()
            if article_no is not None:
                close_article_if_needed()
            article_no = None
            article_title = ""
            article_intro_buf = ""
            expecting_article_title = False

            sec_no = m_sec.group(1).strip()
            sec_title = (m_sec.group(2) or "").strip()
            section_label = f"Má»¥c {sec_no}" + (f" â€“ {sec_title}" if sec_title else "")
            continue

        # ÄIá»€U
        m_art = ARTICLE_RE.match(line)
        if m_art:
            a_no = int(m_art.group(1))
            a_title = (m_art.group(2) or "").strip()

            if expected_article is None:
                expected_article = a_no + 1
                close_clause()
                if article_no is not None:
                    close_article_if_needed()
                article_no = a_no
                article_title = a_title
                if not article_title:
                    expecting_article_title = True
                clause_no = None
                clause_buf = ""
                in_points = False
                point_letter = None
                point_buf = ""
                clause_intro_current = None
                continue
            elif a_no == expected_article:
                expected_article = a_no + 1
                close_clause()
                if article_no is not None:
                    close_article_if_needed()
                article_no = a_no
                article_title = a_title
                if not article_title:
                    expecting_article_title = True
                clause_no = None
                clause_buf = ""
                in_points = False
                point_letter = None
                point_buf = ""
                clause_intro_current = None
                continue
            elif a_no > expected_article:
                if expected_article not in articles_set:
                    break
                else:
                    seeking_article = True
                    continue
            else:
                continue

        if article_no is None:
            continue

        # KHOáº¢N
        m_k = CLAUSE_RE.match(line)
        if m_k and m_k.group(1).isdigit():
            if article_intro_buf.strip():
                flush_article_intro()
                article_has_any_chunk = True
            close_clause()
            clause_no = int(m_k.group(1))
            clause_buf = (m_k.group(2) or "").strip()
            in_points = False
            point_letter = None
            point_buf = ""
            clause_intro_current = None
            continue

        # ÄIá»‚M
        m_p = POINT_RE.match(line)
        if m_p and clause_no is not None:
            letter = m_p.group(1).lower()
            text = (m_p.group(2) or "").strip()

            if not in_points:
                if letter != 'a':
                    clause_buf += ("\n" if clause_buf else "") + f"{letter}) {text}"
                    continue
                clause_intro_current = clause_buf.strip() if clause_buf.strip() else None
                clause_buf = ""
                in_points = True
                point_letter = letter
                point_buf = text
                continue

            if point_letter:
                flush_point()
            in_points = True
            point_letter = letter
            point_buf = text
            continue

        # Ná»™i dung kÃ©o dÃ i
        if clause_no is not None:
            if in_points and point_letter:
                point_buf += ("\n" if point_buf else "") + line
            else:
                clause_buf += ("\n" if clause_buf else "") + line
        else:
            article_intro_buf += ("\n" if article_intro_buf else "") + line

    # Káº¿t thÃºc
    close_clause()
    if article_no is not None:
        close_article_if_needed()

    # Filter chunks
    valid_chunks = []
    for chunk in chunks:
        content = chunk['content'].strip()
        if len(content) > 50:
            valid_chunks.append(chunk)

    print(f"   âœ… Created {len(valid_chunks)} chunks")
    return valid_chunks

def generate_law_id(file_name: str) -> str:
    """Tá»± Ä‘á»™ng sinh law_id tá»« tÃªn file"""
    # Loáº¡i bá» extension vÃ  chuáº©n hÃ³a
    name = file_name.replace('.docx', '').replace('.doc', '').strip()

    # Tá»« Ä‘iá»ƒn mapping cho cÃ¡c loáº¡i luáº­t phá»• biáº¿n
    law_mappings = {
        'kinh doanh báº¥t Ä‘á»™ng sáº£n': 'LKBDS',
        'nhÃ  á»Ÿ': 'LNHAO',
        'Ä‘áº¥t Ä‘ai': 'LDATDAI',
        'Ä‘áº§u tÆ°': 'LDAUTU',
        'Ä‘áº§u tÆ° cÃ´ng': 'LDAUTUCONG',
        'Ä‘áº§u tÆ° theo phÆ°Æ¡ng thá»©c Ä‘á»‘i tÃ¡c cÃ´ng tÆ°': 'LDAUTUPPPCT',
        'thuáº¿ sá»­ dá»¥ng Ä‘áº¥t nÃ´ng nghiá»‡p': 'LTSDDNONGNGHIEP',
        'thuáº¿ sá»­ dá»¥ng Ä‘áº¥t phi nÃ´ng nghiá»‡p': 'LTSDDPHINONGNGHIEP',
        'xÃ¢y dá»±ng': 'LXAYDUNG',
        'hÃ´n nhÃ¢n vÃ  gia Ä‘Ã¬nh': 'LHNVDG',
    }

    # Chuáº©n hÃ³a tÃªn Ä‘á»ƒ matching
    name_lower = name.lower()

    # TÃ¬m mapping phÃ¹ há»£p
    for key, value in law_mappings.items():
        if key in name_lower:
            return value

    # Xá»­ lÃ½ cÃ¡c trÆ°á»ng há»£p Ä‘áº·c biá»‡t
    if 'luáº­t sá»‘' in name_lower and 'qh' in name_lower:
        # Luáº­t sá»‘ XX_YYYY_QHZZ -> LXAYDUNG (luáº­t xÃ¢y dá»±ng)
        if 'xÃ¢y dá»±ng' in name_lower:
            return 'LXAYDUNG'
        # CÃ¡c luáº­t khÃ¡c cÃ³ thá»ƒ thÃªm mapping

    if 'vÄƒn báº£n há»£p nháº¥t' in name_lower:
        # VÄƒn báº£n há»£p nháº¥t Luáº­t XXX -> LDAUTU
        if 'Ä‘áº§u tÆ°' in name_lower:
            return 'LDAUTU'

    # Xá»­ lÃ½ cÃ¡c file VBHN (VÄƒn báº£n há»£p nháº¥t) cÃ³ thá»ƒ lÃ  luáº­t xÃ¢y dá»±ng
    if name_lower.startswith('vbhn') or 'vbhn' in name_lower:
        # ThÆ°á»ng lÃ  luáº­t xÃ¢y dá»±ng
        return 'LXAYDUNG'

    # Xá»­ lÃ½ luáº­t sá»‘ khÃ´ng cÃ³ tá»« khÃ³a rÃµ rÃ ng
    if 'luáº­t sá»‘' in name_lower:
        # CÃ³ thá»ƒ lÃ  luáº­t xÃ¢y dá»±ng náº¿u khÃ´ng match gÃ¬ khÃ¡c
        return 'LXAYDUNG'

    # Náº¿u khÃ´ng tÃ¬m tháº¥y, táº¡o ID tá»« chá»¯ cÃ¡i Ä‘áº§u cá»§a cÃ¡c tá»« quan trá»ng
    words = name.split()

    # Lá»c bá» cÃ¡c tá»« khÃ´ng quan trá»ng
    stop_words = {'sá»‘', 'vÃ ', 'theo', 'phÆ°Æ¡ng', 'thá»©c', 'Ä‘á»‘i', 'tÃ¡c', 'cÃ´ng', 'tÆ°', 'luáº­t', 'vÄƒn', 'báº£n', 'há»£p', 'nháº¥t', 'nÄƒm', 'qÄ‘', 'tt', 'bh', 'vbh', 'vbhn', 'vpqh'}

    important_words = []
    for word in words:
        word_lower = word.lower()
        # Bá» qua sá»‘, tá»« dá»«ng, vÃ  tá»« quÃ¡ ngáº¯n
        if len(word) <= 1 or word_lower in stop_words or word.isdigit():
            continue
        # Bá» qua cÃ¡c pattern sá»‘ nhÆ° 2020_QH14
        if '_' in word and any(part.isdigit() for part in word.split('_')):
            continue
        important_words.append(word)

    if important_words:
        # Láº¥y chá»¯ cÃ¡i Ä‘áº§u cá»§a 2-4 tá»« quan trá»ng Ä‘áº§u tiÃªn
        first_letters = ''.join(w[0].upper() for w in important_words[:4])
        result = f"L{first_letters}"
        # Giá»›i háº¡n Ä‘á»™ dÃ i
        return result[:8]  # Tá»‘i Ä‘a 8 kÃ½ tá»±

    # Fallback cuá»‘i cÃ¹ng
    first_letters = ''.join(w[0].upper() for w in words[:3] if len(w) > 1 and not w.isdigit())
    return f"L{first_letters[:6]}"  # Giá»›i háº¡n 6 kÃ½ tá»±


def load_all_law_documents():
    """Load vÃ  chunk táº¥t cáº£ vÄƒn báº£n luáº­t tá»« thÆ° má»¥c law_content"""
    print("ğŸ“š Loading ALL law documents from law_content folder...")

    # Load danh sÃ¡ch file tá»« JSON
    law_file_paths = []
    try:
        with open("data_files/law_file_paths.json", 'r', encoding='utf-8') as f:
            law_file_paths = json.load(f)
        print(f"âœ… Loaded {len(law_file_paths)} law file paths from JSON")
    except FileNotFoundError:
        print("âŒ data_files/law_file_paths.json not found! Please run find_law_files.py first.")
        print("ğŸ”„ Running find_law_files.py to generate the file...")
        try:
            import subprocess
            result = subprocess.run([sys.executable, "find_law_files.py"],
                                  capture_output=True, text=True, cwd=os.getcwd())
            if result.returncode == 0:
                print("âœ… Successfully generated data_files/law_file_paths.json")
                with open("data_files/law_file_paths.json", 'r', encoding='utf-8') as f:
                    law_file_paths = json.load(f)
                print(f"âœ… Loaded {len(law_file_paths)} law file paths from generated JSON")
            else:
                print(f"âŒ Failed to generate data_files/law_file_paths.json: {result.stderr}")
                return []
        except Exception as e:
            print(f"âŒ Error running find_law_files.py: {e}")
            return []
    except Exception as e:
        print(f"âŒ Error loading data_files/law_file_paths.json: {e}")
        return []
    
    all_law_docs = []
    successful_files = 0
    failed_files = 0
    
    print(f"\nğŸ”„ Processing {len(law_file_paths)} law files...")
    
    for i, file_info in enumerate(tqdm(law_file_paths, desc="Processing law files")):
        file_path = file_info['path']
        category = file_info['category']
        file_name = file_info['file_name']
        
        # Kiá»ƒm tra xem file cÃ³ trong thÆ° má»¥c luáº­t khÃ´ng (Ä‘Ã£ Ä‘Æ°á»£c lá»c trong find_law_files.py)
        
        print(f"\nğŸ“„ [{i+1}/{len(law_file_paths)}] Processing: {file_name}")
        print(f"   Category: {category}")
        print(f"   Path: {file_path}")
        
        if not os.path.exists(file_path):
            print(f"   âŒ File not found: {file_path}")
            failed_files += 1
            continue
        
        try:
            # BÆ°á»›c 1: Äá»c file
            print(f"   ğŸ“– Reading file...")
            law_text = read_docx(file_path)  # HÃ m nÃ y Ä‘Ã£ xá»­ lÃ½ cáº£ .doc vÃ  .docx
            
            if not law_text or len(law_text.strip()) < 100:
                print(f"   âš ï¸ File seems empty or too short: {len(law_text)} characters")
                print(f"   â­ï¸ Skipping file (cannot read content)")
                failed_files += 1
                continue
            
            # BÆ°á»›c 2: Chia thÃ nh chunks
            print(f"   ğŸ”¨ Chunking document...")
            # Táº¡o law_id tá»± Ä‘á»™ng tá»« tÃªn file
            law_id = generate_law_id(file_name)
            print(f"   ğŸ“‹ Generated law_id: {law_id}")
            law_chunks = chunk_law_document(law_text, law_id=law_id, law_no="", law_title=file_name)

            if not law_chunks:
                print(f"   âš ï¸ No chunks created from file")
                failed_files += 1
                continue

            # BÆ°á»›c 3: Chuáº©n bá»‹ dá»¯ liá»‡u cho Ä‘Ã¡nh giÃ¡
            print(f"   ğŸ—‚ï¸ Preparing chunks...")
            for j, chunk in enumerate(law_chunks):
                # ThÃªm thÃ´ng tin vá» file gá»‘c vÃ o metadata
                chunk_metadata = chunk.get('metadata', {}).copy()
                chunk_metadata.update({
                    'source_file': file_path,
                    'source_category': category,
                    'source_file_name': file_name,
                    'chunk_index': j
                })

                # Táº¡o document theo format hn2014_chunks.json
                all_law_docs.append({
                    'id': chunk['id'],
                    'content': chunk['content'],
                    'metadata': chunk_metadata
                })
            
            print(f"   âœ… Successfully processed {len(law_chunks)} chunks")
            successful_files += 1
            
        except Exception as e:
            print(f"   âŒ Error processing file: {e}")
            failed_files += 1
            continue
    
    print(f"\nğŸ“Š Processing Summary:")
    print(f"   âœ… Successfully processed: {successful_files} files")
    print(f"   âŒ Failed to process: {failed_files} files")
    print(f"   ğŸ“„ Total chunks created: {len(all_law_docs)}")
    
    if all_law_docs:
        print(f"   ğŸ“Š Average chunk length: {np.mean([len(doc['content']) for doc in all_law_docs]):.0f} characters")
        
        # Thá»‘ng kÃª theo category
        category_counts = {}
        for doc in all_law_docs:
            doc_category = doc.get('metadata', {}).get('source_category', 'Unknown')
            category_counts[doc_category] = category_counts.get(doc_category, 0) + 1

        print(f"\nğŸ“ˆ Chunk distribution by category:")
        for category, count in category_counts.items():
            print(f"   - {category}: {count} chunks")
    
    print(f"\nâœ… Successfully loaded {len(all_law_docs)} law document chunks from {successful_files} files!")
    return all_law_docs

    # else:
    #     # Load file cÅ© tá»« LuatHonNhan
    #     print("ğŸ“š Loading law document from LuatHonNhan folder...")
        
    #     law_file_path = "LuatHonNhan/luat_hon_nhan_va_gia_dinh.docx"
    #     if os.path.exists(law_file_path):
    #         print(f"âœ… Found law file: {law_file_path}")
            
    #         # BÆ°á»›c 1: Äá»c file docx
    #         print("\nğŸ“– Step 1: Reading DOCX file...")
    #         law_text = read_docx(law_file_path)
            
    #         # BÆ°á»›c 2: Chia thÃ nh chunks
    #         print("\nğŸ”¨ Step 2: PROPER Advanced chunking with 2-pass validation...")
    #         law_chunks = advanced_chunk_law_document(law_text, max_length=600)

    #         # BÆ°á»›c 2.1: (Tuá»³ chá»n) Tháº©m Ä‘á»‹nh báº±ng Gemini
    #         if _env_truthy(os.getenv("CHUNK_AI_REVIEW", "false")):
    #             print("\nğŸ¤– AI Review: Calling Gemini to validate chunking...")
    #             # XÃ¢y dá»±ng summary nháº¹ tá»« chunks (khÃ´ng cÃ³ cáº£nh bÃ¡o ná»™i bá»™)
    #             try:
    #                 chapters_seen = []
    #                 seen_chapter_set = set()
    #                 citations = []
    #                 type_counts_tmp = {"article_intro": 0, "clause": 0, "point": 0}
    #                 seen_articles = set()
    #                 for c in law_chunks:
    #                     md = c.get("metadata", {})
    #                     ch = md.get("chapter")
    #                     if ch and ch not in seen_chapter_set:
    #                         seen_chapter_set.add(ch)
    #                         chapters_seen.append(ch)
    #                     cite = md.get("exact_citation")
    #                     if cite:
    #                         citations.append(cite)
    #                     t = c.get("type")
    #                     if t in type_counts_tmp:
    #                         type_counts_tmp[t] += 1
    #                     a_no = md.get("article_no")
    #                     if a_no is not None:
    #                         seen_articles.add(a_no)

    #                 summary = {
    #                     "chapters_seen": chapters_seen,
    #                     "articles": len(seen_articles),
    #                     "article_intro": type_counts_tmp.get("article_intro", 0),
    #                     "clauses": type_counts_tmp.get("clauses", 0) or type_counts_tmp.get("clause", 0),
    #                     "points": type_counts_tmp.get("points", 0) or type_counts_tmp.get("point", 0),
    #                     "citations": citations,
    #                     "warnings": [],
    #                     "halted_reason": None,
    #                     "total_chunks": len(law_chunks)
    #                 }

    #                 excerpts_len = int(os.getenv("CHUNK_REVIEW_EXCERPTS", "8000") or 8000)
    #                 payload = build_review_payload(law_chunks, summary, law_text, sample_excerpts_chars=max(2000, excerpts_len))
    #                 review = call_gemini_review(payload)
    #                 status = review.get("status", "issues_found")
    #                 confidence = review.get("confidence", 0.0)
    #                 issues = review.get("issues", []) or []
    #                 notes = review.get("notes", "")
    #                 print(f"- Gemini status: {status} | confidence: {confidence:.2f}")
    #                 if notes:
    #                     print(f"- Notes: {notes[:4000]}")
    #                 if issues:
    #                     print("\n===== Gemini Issues =====")
    #                     for i, it in enumerate(issues[:20], 1):
    #                         print(f"{i:02d}. [{it.get('severity','?')}] ({it.get('category','other')}) {it.get('citation') or it.get('id') or ''}")
    #                         print(f"    - {it.get('message','(no message)')}")
    #                         if it.get('suggestion'):
    #                             print(f"    â†’ Suggestion: {it['suggestion']}")

    #                 if _env_truthy(os.getenv("CHUNK_STRICT_OK_ONLY", "false")) and status != "ok":
    #                     print("âŒ CHUNK_STRICT_OK_ONLY is set and Gemini did not return 'ok'. Aborting load.")
    #                     return []
    #             except Exception as e:
    #                 print(f"âš ï¸ Gemini review failed: {e}")
    #                 if _env_truthy(os.getenv("CHUNK_STRICT_OK_ONLY", "false")):
    #                     return []
            
    #         # BÆ°á»›c 3: Chuáº©n bá»‹ dá»¯ liá»‡u cho Ä‘Ã¡nh giÃ¡
    #         print("\nğŸ—‚ï¸ Step 3: Preparing data for evaluation...")
    #         law_docs = []
    #         for i, chunk in enumerate(law_chunks):
    #             law_docs.append({
    #                 'id': i,
    #                 'title': chunk['title'],
    #                 'text': chunk['content'],
    #                 'length': len(chunk['content']),
    #                 'type': chunk['type'],
    #                 'metadata': chunk.get('metadata', {})
    #             })
            
    #         print(f"   âœ… Processed {len(law_docs)} law document chunks")
    #         print(f"   ğŸ“Š Average chunk length: {np.mean([doc['length'] for doc in law_docs]):.0f} characters")
            
    #         # Thá»‘ng kÃª theo loáº¡i
    #         type_counts = {}
    #         for doc in law_docs:
    #             doc_type = doc['type']
    #             type_counts[doc_type] = type_counts.get(doc_type, 0) + 1
            
    #         print(f"\nğŸ“ˆ Chunk distribution by type:")
    #         for chunk_type, count in type_counts.items():
    #             print(f"   - {chunk_type}: {count} chunks")
            
    #         print(f"\nâœ… Successfully loaded {len(law_docs)} law document chunks!")
    #         return law_docs
            
    #     else:
    #         print(f"âŒ File {law_file_path} not found!")
    #         print(f"   Expected path: {os.path.abspath(law_file_path)}")
    #         return []

def get_models_to_evaluate():
    """Danh sÃ¡ch cÃ¡c mÃ´ hÃ¬nh cáº§n Ä‘Ã¡nh giÃ¡"""
    return [
        {
            'name': 'minhquan6203/paraphrase-vietnamese-law',
            'type': 'transformers',
            'description': 'MÃ´ hÃ¬nh Sentence Similarity Ä‘Ã£ fine tune trÃªn bá»™ luáº­t phÃ¡p Viá»‡t Nam',
            'max_length': 512
        },
        {
            'name': 'sentence-transformers/paraphrase-multilingual-mpnet-base-v2',
            'type': 'sentence_transformers',
            'description': 'MÃ´ hÃ¬nh cÆ¡ sá»Ÿ Ä‘a ngÃ´n ngá»¯ (base model)',
            'max_length': 512
        },
        {
            'name': 'namnguyenba2003/Vietnamese_Law_Embedding_finetuned_v3_256dims',
            'type': 'transformers',
            'description': 'MÃ´ hÃ¬nh embedding luáº­t Viá»‡t Nam vá»›i 256 dimensions',
            'max_length': 512
        },
        {
            'name': 'truro7/vn-law-embedding',
            'type': 'sentence_transformers',
            'description': 'MÃ´ hÃ¬nh embedding luáº­t Viá»‡t Nam báº±ng Sentence Transformers',
            'max_length': 512
        }
    ]

def load_question_benchmark(random_sample=50):
    """Load cÃ¡c cÃ¢u há»i tá»« file Excel Ä‘á»ƒ lÃ m benchmark queries"""
    try:
        with open("data_files/law_questions.json", 'r', encoding='utf-8') as f:
            questions = json.load(f)

        print(f"âœ… Loaded {len(questions)} benchmark questions from Excel files")

        # Random sample 50 questions
        if len(questions) > random_sample:
            import random
            questions_sample = random.sample(questions, random_sample)
            print(f"ğŸ² Randomly selected {random_sample} questions for evaluation")
        else:
            questions_sample = questions
            print(f"ğŸ“Š Using all {len(questions)} questions (less than {random_sample})")

        # Tráº£ vá» list cÃ¡c query (chá»‰ láº¥y primary_query)
        queries = [q['primary_query'] for q in questions_sample]

        # Thá»‘ng kÃª theo category
        category_stats = {}
        for q in questions_sample:
            cat = q.get('full_category', 'Unknown')
            category_stats[cat] = category_stats.get(cat, 0) + 1

        print(f"ğŸ“Š Sample distribution by category:")
        for cat, count in sorted(category_stats.items()):
            print(f"   - {cat}: {count} questions")

        return queries, questions_sample

    except FileNotFoundError:
        print("âš ï¸ data_files/law_questions.json not found. Using default queries.")
        return get_default_benchmark_queries(), []
    except Exception as e:
        print(f"âš ï¸ Error loading questions: {e}. Using default queries.")
        return get_default_benchmark_queries(), []

def get_default_benchmark_queries():
    """CÃ¡c query máº·c Ä‘á»‹nh náº¿u khÃ´ng cÃ³ file questions"""
    return [
        "NgÆ°á»i bá»‹ bá»‡nh tÃ¢m tháº§n lÃ  ngÆ°á»i máº¥t nÄƒng lá»±c hÃ nh vi dÃ¢n sá»± lÃ  Ä‘Ãºng hay sai?",
        "Sá»± thá»a thuáº­n cá»§a cÃ¡c bÃªn khÃ´ng vi pháº¡m Ä‘iá»u cáº¥m cá»§a phÃ¡p luáº­t, khÃ´ng trÃ¡i Ä‘áº¡o Ä‘á»©c xÃ£ há»™i thÃ¬ Ä‘Æ°á»£c gá»i lÃ  há»£p Ä‘á»“ng lÃ  Ä‘Ãºng hay sai?",
        "Tiá»n phÃºng viáº¿ng Ä‘Ã¡m ma cÅ©ng thuá»™c di sáº£n thá»«a káº¿ cá»§a ngÆ°á»i cháº¿t Ä‘á»ƒ láº¡i lÃ  Ä‘Ãºng hay sai?",
        "CÃ¡ nhÃ¢n Ä‘Æ°á»£c hÆ°á»Ÿng di sáº£n thá»«a káº¿ pháº£i tráº£ ná»£ thay cho ngÆ°á»i Ä‘á»ƒ láº¡i di sáº£n lÃ  Ä‘Ãºng hay sai?",
        "Há»£p Ä‘á»“ng vÃ´ hiá»‡u lÃ  há»£p Ä‘á»“ng vi pháº¡m phÃ¡p luáº­t lÃ  Ä‘Ãºng hay sai?",
        "Hoa lá»£i, lá»£i tá»©c phÃ¡t sinh tá»« tÃ i sáº£n riÃªng cá»§a má»™t bÃªn vá»£ hoáº·c chá»“ng sáº½ lÃ  tÃ i sáº£n chung náº¿u hoa lá»£i, lá»£i tá»©c Ä‘Ã³ lÃ  nguá»“n sá»‘ng duy nháº¥t cá»§a gia Ä‘Ã¬nh.",
        "Há»™i LiÃªn hiá»‡p phá»¥ ná»¯ cÃ³ quyá»n yÃªu cáº§u TÃ²a Ã¡n ra quyáº¿t Ä‘á»‹nh há»§y káº¿t hÃ´n trÃ¡i phÃ¡p luáº­t do vi pháº¡m sá»± tá»± nguyá»‡n.",
        "HÃ´n nhÃ¢n chá»‰ cháº¥m dá»©t khi má»™t bÃªn vá»£, chá»“ng cháº¿t.",
        "Káº¿t hÃ´n cÃ³ yáº¿u tá»‘ nÆ°á»›c ngoÃ i cÃ³ thá»ƒ Ä‘Äƒng kÃ½ táº¡i UBND cáº¥p xÃ£.",
        "Khi cha máº¹ khÃ´ng thá»ƒ nuÃ´i dÆ°á»¡ng, cáº¥p dÆ°á»¡ng Ä‘Æ°á»£c cho con, thÃ¬ Ã´ng bÃ  pháº£i cÃ³ nghÄ©a vá»¥ nuÃ´i dÆ°á»¡ng hoáº·c cáº¥p dÆ°á»¡ng cho chÃ¡u.",
        "Khi hÃ´n nhÃ¢n cháº¥m dá»©t, má»i quyá»n vÃ  nghÄ©a vá»¥ giá»¯a nhá»¯ng ngÆ°á»i Ä‘Ã£ tá»«ng lÃ  vá»£ chá»“ng cÅ©ng cháº¥m dá»©t.",
        "Khi vá»£ chá»“ng ly hÃ´n, con dÆ°á»›i 36 thÃ¡ng tuá»•i Ä‘Æ°á»£c giao cho ngÆ°á»i vá»£ trá»±c tiáº¿p nuÃ´i dÆ°á»¡ng.",
        "Khi vá»£ hoáº·c chá»“ng thá»±c hiá»‡n nhá»¯ng giao dá»‹ch phá»¥c vá»¥ cho nhu cáº§u thiáº¿t yáº¿u cá»§a gia Ä‘Ã¬nh mÃ  khÃ´ng cÃ³ sá»± Ä‘á»“ng Ã½ cá»§a bÃªn kia thÃ¬ ngÆ°á»i thá»±c hiá»‡n giao dá»‹ch Ä‘Ã³ pháº£i thanh toÃ¡n báº±ng tÃ i sáº£n riÃªng cá»§a mÃ¬nh.",
        "Khi khÃ´ng sá»‘ng chung cÃ¹ng vá»›i cha máº¹, con Ä‘Ã£ thÃ nh niÃªn cÃ³ kháº£ nÄƒng lao Ä‘á»™ng pháº£i cáº¥p dÆ°á»¡ng cho cha máº¹.",
        "Chá»‰ UBND cáº¥p tá»‰nh nÆ¡i cÃ´ng dÃ¢n Viá»‡t Nam cÆ° trÃº má»›i cÃ³ tháº©m quyá»n Ä‘Äƒng kÃ½ viá»‡c káº¿t hÃ´n giá»¯a cÃ´ng dÃ¢n Viá»‡t Nam vá»›i ngÆ°á»i nÆ°á»›c ngoÃ i.",
        "Äiá»u kiá»‡n káº¿t hÃ´n cá»§a nam vÃ  ná»¯ theo phÃ¡p luáº­t Viá»‡t Nam lÃ  gÃ¬?",
        "TÃ i sáº£n nÃ o Ä‘Æ°á»£c coi lÃ  tÃ i sáº£n chung cá»§a vá»£ chá»“ng?",
        "Thá»§ tá»¥c ly hÃ´n táº¡i tÃ²a Ã¡n Ä‘Æ°á»£c quy Ä‘á»‹nh nhÆ° tháº¿ nÃ o?",
        "Quyá»n vÃ  nghÄ©a vá»¥ cá»§a cha máº¹ Ä‘á»‘i vá»›i con chÆ°a thÃ nh niÃªn?",
        "TrÆ°á»ng há»£p nÃ o vá»£ chá»“ng cÃ³ thá»ƒ thá»a thuáº­n vá» cháº¿ Ä‘á»™ tÃ i sáº£n?"
    ]

def get_benchmark_queries():
    """CÃ¡c query tá»« benchmark - Æ°u tiÃªn tá»« Excel, fallback vá» default"""
    # Note: This function is kept for backward compatibility
    # Main logic moved to load_question_benchmark() called directly in main()
    queries, questions_sample = load_question_benchmark()
    return queries

def search_top_k(query_embedding, doc_embeddings, k=15):
    """Deprecated: Ä‘á»ƒ tÆ°Æ¡ng thÃ­ch cÅ©. KhÃ´ng cÃ²n dÃ¹ng khi Ä‘Ã£ chuyá»ƒn sang Qdrant."""
    similarities = cosine_similarity([query_embedding], doc_embeddings)[0]
    top_indices = np.argsort(similarities)[::-1][:k]
    top_scores = similarities[top_indices]
    return top_indices, top_scores

def calculate_metrics(scores, threshold_07=0.7, threshold_05=0.5):
    """TÃ­nh toÃ¡n cÃ¡c metrics Ä‘Ã¡nh giÃ¡ cháº¥t lÆ°á»£ng retrieval"""
    return {
        "max_score": float(np.max(scores)),
        "avg_top3": float(np.mean(scores[:3])) if len(scores) >= 3 else float(np.mean(scores)),
        "avg_top5": float(np.mean(scores[:5])) if len(scores) >= 5 else float(np.mean(scores)),
        "avg_top10": float(np.mean(scores[:10])) if len(scores) >= 10 else float(np.mean(scores)),
        "avg_all": float(np.mean(scores)),
        "scores_above_07": int(np.sum(scores >= threshold_07)),
        "scores_above_05": int(np.sum(scores >= threshold_05)),
        "min_score": float(np.min(scores))
    }

def display_search_results(query, law_docs, top_indices, top_scores, max_display=5):
    """Hiá»ƒn thá»‹ káº¿t quáº£ search má»™t cÃ¡ch rÃµ rÃ ng"""
    print(f"ğŸ“ Query: {query}")

    # Báº£o Ä‘áº£m sáº¯p xáº¿p giáº£m dáº§n theo Ä‘iá»ƒm sá»‘
    if not isinstance(top_scores, np.ndarray):
        top_scores = np.array(top_scores)
    if not isinstance(top_indices, np.ndarray):
        top_indices = np.array(top_indices)
    order = np.argsort(top_scores)[::-1]
    top_indices = top_indices[order]
    top_scores = top_scores[order]

    print(f"ğŸ¯ Top {min(max_display, len(top_indices))} Results:")

    for i in range(min(max_display, len(top_indices))):
        idx = int(top_indices[i])
        score = float(top_scores[i])
        doc = law_docs[idx]

        print(f"\n   {i+1}. Score: {score:.4f} | {doc['id']}")
        print(f"      Length: {len(doc['content'])} chars")
        print("      Content:")
        print(doc['content'])

        if doc.get('metadata') and doc['metadata']:
            # Chá»‰ hiá»ƒn thá»‹ má»™t sá»‘ metadata quan trá»ng
            important_meta = {
                k: v for k, v in doc['metadata'].items()
                if k in ['exact_citation', 'chapter', 'article_no', 'clause_no', 'point_letter', 'source_file_name']
            }
            if important_meta:
                metadata_str = ", ".join([f"{k}: {v}" for k, v in important_meta.items()])
                print(f"      Metadata: {metadata_str}")

def evaluate_single_model(model_info, law_docs, queries, top_k=15, show_detailed_results=True, device="cuda"):
    """ÄÃ¡nh giÃ¡ má»™t mÃ´ hÃ¬nh embedding"""
    print(f"\n{'='*80}")
    print(f"ğŸ” EVALUATING MODEL: {model_info['name']}")
    print(f"ğŸ“ Description: {model_info['description']}")
    print(f"ğŸ”§ Type: {model_info['type']} | Max Length: {model_info['max_length']} tokens")
    print(f"{'='*80}")
    
    try:
        # BÆ°á»›c 1: Chuáº©n bá»‹ texts
        doc_texts = [doc['content'] for doc in law_docs]
        print(f"\nğŸ“š Step 1: Prepared {len(doc_texts)} document texts")
        
        # BÆ°á»›c 2: Kiá»ƒm tra Qdrant collection trÆ°á»›c
        client = get_qdrant_client()
        collection_name = model_info['name'].replace('/', '_')
        existing = count_collection_points(client, collection_name)
        
        if existing >= len(law_docs):
            print(f"ğŸŸ¡ Collection '{collection_name}' already has {existing} vectors (>= {len(law_docs)}). Skipping document encoding.")
        else:
            print(f"ğŸŸ  Collection '{collection_name}' has {existing} vectors. Need to encode and upsert {len(law_docs)} vectors...")
            
            # BÆ°á»›c 2a: Encode documents
            print(f"\nğŸ”¨ Step 2a: Encoding documents...")
            if model_info['type'] == 'sentence_transformers':
                doc_embeddings = encode_with_sentence_transformers(
                    doc_texts, 
                    model_info['name'], 
                    batch_size=16,
                    device=device
                )
            else:
                doc_embeddings = encode_with_transformers(
                    doc_texts, 
                    model_info['name'], 
                    max_length=model_info['max_length'],
                    batch_size=16,
                    device=device
                )
            
            print(f"   âœ… Document embeddings shape: {doc_embeddings.shape}")
            
            # BÆ°á»›c 2b: LÆ°u vÃ o Qdrant
            print(f"\nğŸ’¾ Step 2b: Storing embeddings in Qdrant...")
            ensure_collection(client, collection_name, vector_size=doc_embeddings.shape[1])
            upsert_embeddings_to_qdrant(client, collection_name, doc_embeddings, law_docs)
            
            # Giáº£i phÃ³ng RAM
            del doc_embeddings
            gc.collect()
        
        # BÆ°á»›c 3: Encode queries
        print(f"\nğŸ” Step 3: Encoding queries...")
        if model_info['type'] == 'sentence_transformers':
            query_embeddings = encode_with_sentence_transformers(
                queries, 
                model_info['name'], 
                batch_size=16,
                device=device
            )
        else:
            query_embeddings = encode_with_transformers(
                queries, 
                model_info['name'], 
                max_length=model_info['max_length'],
                batch_size=16,
                device=device
            )
        
        print(f"   âœ… Query embeddings shape: {query_embeddings.shape}")
        
        # BÆ°á»›c 4: Evaluate tá»«ng query
        print(f"\nğŸ“Š Step 4: Evaluating {len(queries)} queries...")
        query_results = []
        all_metrics = []
        
        for i, query in enumerate(queries):
            print(f"\n   ğŸ” Query {i+1}/{len(queries)}")
            
            # Search top-k documents via Qdrant
            top_indices, top_scores = search_qdrant(
                client=client,
                collection_name=collection_name,
                query_embedding=query_embeddings[i],
                top_k=top_k
            )
            
            # Calculate metrics
            metrics = calculate_metrics(top_scores)
            all_metrics.append(metrics)
            
            # Store results
            query_result = {
                'query': query,
                'query_id': i,
                'top_indices': top_indices.tolist(),
                'top_scores': top_scores.tolist(),
                'metrics': metrics
            }
            query_results.append(query_result)
            
            # Show detailed results for first few queries
            if show_detailed_results and i < 3:
                display_search_results(query, law_docs, top_indices, top_scores, max_display=3)
                print(f"      ğŸ“ˆ Metrics: Max={metrics['max_score']:.4f}, Avg_top5={metrics['avg_top5']:.4f}, Above_0.7={metrics['scores_above_07']}")
        
        # BÆ°á»›c 5: Aggregate metrics
        print(f"\nğŸ“ˆ Step 5: Aggregating metrics...")
        
        # Calculate average metrics across all queries
        avg_metrics = {}
        metric_keys = all_metrics[0].keys()
        for key in metric_keys:
            if key.startswith('scores_above'):
                avg_metrics[f"avg_{key}"] = np.mean([m[key] for m in all_metrics])
            else:
                avg_metrics[f"avg_{key}"] = np.mean([m[key] for m in all_metrics])
        
        # Final result
        final_result = {
            'model_name': model_info['name'],
            'model_type': model_info['type'],
            'model_description': model_info['description'],
            'max_length': model_info['max_length'],
            'num_queries': len(queries),
            'num_documents': len(law_docs),
            'top_k': top_k,
            'query_results': query_results,
            'aggregated_metrics': avg_metrics,
            'evaluation_success': True
        }
        
        # Print summary
        print(f"\nâœ… EVALUATION COMPLETED SUCCESSFULLY!")
        print(f"   ğŸ“Š Average Results:")
        print(f"      - Avg Max Score: {avg_metrics['avg_max_score']:.4f}")
        print(f"      - Avg Top-5 Score: {avg_metrics['avg_avg_top5']:.4f}")
        print(f"      - Avg Above 0.7: {avg_metrics['avg_scores_above_07']:.1f}")
        print(f"      - Avg Above 0.5: {avg_metrics['avg_scores_above_05']:.1f}")
        
        return final_result
        
    except Exception as e:
        print(f"\nâŒ EVALUATION FAILED: {str(e)}")
        return {
            'model_name': model_info['name'],
            'model_type': model_info['type'],
            'error': str(e),
            'evaluation_success': False
        }

def run_evaluation_all_models(models_to_evaluate, law_docs, benchmark_queries, device="cuda"):
    """Cháº¡y Ä‘Ã¡nh giÃ¡ cho táº¥t cáº£ cÃ¡c mÃ´ hÃ¬nh"""
    print("ğŸš€ Starting evaluation for all models...")
    
    if not law_docs:
        print("âŒ Error: No law documents loaded!")
        return []
    
    print(f"âœ… Ready to evaluate with:")
    print(f"   ğŸ“š Documents: {len(law_docs)} law chunks")
    print(f"   â“ Queries: {len(benchmark_queries)} benchmark questions")
    print(f"   ğŸ¤– Models: {len(models_to_evaluate)} models to test")
    print(f"   ğŸ¯ Top-K: 15 results per query")
    
    evaluation_results = []
    successful_evaluations = 0
    failed_evaluations = 0
    
    for i, model_info in enumerate(models_to_evaluate):
        print(f"\n{'ğŸ¤– '*20}")
        print(f"ğŸ¤– EVALUATING MODEL {i+1}/{len(models_to_evaluate)}")
        print(f"{'ğŸ¤– '*20}")
        
        try:
            result = evaluate_single_model(
                model_info=model_info,
                law_docs=law_docs,
                queries=benchmark_queries,
                top_k=15,
                show_detailed_results=(i < 2),  # Show details for first 2 models only
                device=device
            )
            
            if result['evaluation_success']:
                evaluation_results.append(result)
                successful_evaluations += 1
                print(f"âœ… Model {i+1} evaluation completed successfully!")
            else:
                print(f"âŒ Model {i+1} evaluation failed: {result.get('error', 'Unknown error')}")
                failed_evaluations += 1
            
            # Wait between models
            if i < len(models_to_evaluate) - 1:
                print(f"â³ Waiting 2 seconds before next model...")
                time.sleep(2)
                
        except Exception as e:
            print(f"âŒ Unexpected error evaluating model {i+1}: {str(e)}")
            failed_evaluations += 1
            continue
    
    # Final summary
    print(f"\n{'='*100}")
    print(f"ğŸ‰ EVALUATION SUMMARY")
    print(f"{'='*100}")
    print(f"âœ… Successful evaluations: {successful_evaluations}")
    print(f"âŒ Failed evaluations: {failed_evaluations}")
    print(f"ğŸ“Š Total models evaluated: {len(evaluation_results)}")
    
    if evaluation_results:
        print(f"\nğŸ“ˆ Quick Performance Preview:")
        # Sáº¯p xáº¿p theo avg_max_score giáº£m dáº§n
        preview_sorted = sorted(
            evaluation_results,
            key=lambda r: r['aggregated_metrics']['avg_max_score'],
            reverse=True
        )
        for result in preview_sorted:
            metrics = result['aggregated_metrics']
            model_name = result['model_name'].split('/')[-1]
            print(f"   ğŸ¤– {model_name}")
            print(f"      Max Score: {metrics['avg_max_score']:.4f} | Top-5: {metrics['avg_avg_top5']:.4f} | Above 0.7: {metrics['avg_scores_above_07']:.1f}")
    
    return evaluation_results

def generate_final_report(evaluation_results, law_docs, benchmark_queries, sample_questions=None):
    """Táº¡o bÃ¡o cÃ¡o chi tiáº¿t cuá»‘i cÃ¹ng"""
    print("ğŸ“Š Generating detailed analysis and final report...")
    
    if not evaluation_results:
        print("âŒ No evaluation results available!")
        return None
    
    print(f"\n{'='*100}")
    print(f"ğŸ“‹ COMPREHENSIVE EVALUATION REPORT")
    print(f"{'='*100}")
    
    # Sort models by average max score (best first)
    sorted_results = sorted(
        evaluation_results, 
        key=lambda x: x['aggregated_metrics']['avg_max_score'], 
        reverse=True
    )
    
    print(f"ğŸ“Š DATASET INFORMATION:")
    print(f"   ğŸ“š Law Documents: {len(law_docs)} chunks from Luáº­t HÃ´n nhÃ¢n vÃ  Gia Ä‘Ã¬nh")
    print(f"   â“ Benchmark Queries: {len(benchmark_queries)} questions")
    print(f"   ğŸ” Evaluation Method: Top-15 retrieval with cosine similarity")
    print(f"   ğŸ’¾ Storage: Qdrant vector database")
    
    print(f"\nğŸ† RANKING BY PERFORMANCE:")
    print(f"   Metric: Average Max Score across all queries")
    
    # Create comparison table
    print(f"\n{'Rank':<4} {'Model':<45} {'Max Score':<10} {'Top-5':<8} {'â‰¥0.7':<6} {'â‰¥0.5':<6} {'Type':<12}")
    print(f"{'-'*95}")
    
    for i, result in enumerate(sorted_results):
        metrics = result['aggregated_metrics']
        model_name = result['model_name'].split('/')[-1][:40]
        model_type = result['model_type']
        
        print(f"{i+1:<4} {model_name:<45} {metrics['avg_max_score']:<10.4f} "
              f"{metrics['avg_avg_top5']:<8.4f} {metrics['avg_scores_above_07']:<6.1f} "
              f"{metrics['avg_scores_above_05']:<6.1f} {model_type:<12}")
    
    # Best model analysis
    best_model = sorted_results[0]
    print(f"\nâ­ RECOMMENDED MODEL:")
    print(f"   ğŸ¥‡ {best_model['model_name']}")
    print(f"   ğŸ“ {best_model['model_description']}")
    print(f"   ğŸ¯ Performance Highlights:")
    best_metrics = best_model['aggregated_metrics']
    print(f"      - Average Max Score: {best_metrics['avg_max_score']:.4f}")
    print(f"      - Average Top-5 Score: {best_metrics['avg_avg_top5']:.4f}")
    print(f"      - Queries with score â‰¥ 0.7: {best_metrics['avg_scores_above_07']:.1f} per query")
    print(f"      - Queries with score â‰¥ 0.5: {best_metrics['avg_scores_above_05']:.1f} per query")
    
    # Performance analysis
    print(f"\nğŸ“ˆ PERFORMANCE ANALYSIS:")
    
    # Calculate overall statistics
    all_max_scores = [r['aggregated_metrics']['avg_max_score'] for r in evaluation_results]
    all_top5_scores = [r['aggregated_metrics']['avg_avg_top5'] for r in evaluation_results]
    
    print(f"   ğŸ“Š Overall Statistics:")
    print(f"      - Best Max Score: {max(all_max_scores):.4f}")
    print(f"      - Worst Max Score: {min(all_max_scores):.4f}")
    print(f"      - Average Max Score: {np.mean(all_max_scores):.4f}")
    print(f"      - Best Top-5 Score: {max(all_top5_scores):.4f}")
    print(f"      - Average Top-5 Score: {np.mean(all_top5_scores):.4f}")
    
    # Model type analysis
    transformers_models = [r for r in evaluation_results if r['model_type'] == 'transformers']
    sentence_transformers_models = [r for r in evaluation_results if r['model_type'] == 'sentence_transformers']
    
    if transformers_models and sentence_transformers_models:
        print(f"\nğŸ”§ MODEL TYPE COMPARISON:")
        
        trans_avg = np.mean([r['aggregated_metrics']['avg_max_score'] for r in transformers_models])
        sent_avg = np.mean([r['aggregated_metrics']['avg_max_score'] for r in sentence_transformers_models])
        
        print(f"   ğŸ”¨ Transformers models: {len(transformers_models)} models, avg score: {trans_avg:.4f}")
        print(f"   ğŸ“¦ Sentence-transformers: {len(sentence_transformers_models)} models, avg score: {sent_avg:.4f}")
        
        if trans_avg > sent_avg:
            print(f"   âœ… Transformers models perform better on average (+{trans_avg - sent_avg:.4f})")
        else:
            print(f"   âœ… Sentence-transformers models perform better on average (+{sent_avg - trans_avg:.4f})")
    
    # Recommendations
    print(f"\nğŸ’¡ RECOMMENDATIONS:")
    print(f"   ğŸ¯ For Production Deployment:")
    print(f"      - Primary: {best_model['model_name']}")
    print(f"      - Type: {best_model['model_type']}")
    print(f"      - Max Length: {best_model['max_length']} tokens")
    
    if len(sorted_results) > 1:
        second_best = sorted_results[1]
        print(f"   ğŸ¥ˆ Alternative Option:")
        print(f"      - {second_best['model_name']}")
        print(f"      - Performance difference: {best_metrics['avg_max_score'] - second_best['aggregated_metrics']['avg_max_score']:.4f}")
    
    print(f"\nğŸ” DETAILED QUERY ANALYSIS:")
    print(f"   ğŸ“ Sample Query Performance (Best Model):")
    
    # Show performance on first 3 queries for best model
    best_query_results = best_model['query_results'][:3]
    for i, qr in enumerate(best_query_results):
        print(f"\n   Query {i+1}: {qr['query'][:60]}...")
        print(f"      Max Score: {qr['metrics']['max_score']:.4f}")
        print(f"      Top-3 Average: {qr['metrics']['avg_top3']:.4f}")
        print(f"      Results â‰¥ 0.7: {qr['metrics']['scores_above_07']}")

    # Top queries by total score (best model)
    print(f"\nğŸ” TOP 3 QUERIES BY TOTAL SCORE (Best Model)")
    # Compute total score = sum of retrieved top_k scores for each query
    query_totals = []
    for qr in best_model['query_results']:
        total = float(np.sum(qr.get('top_scores', [])))
        query_totals.append({
            'query_id': qr['query_id'],
            'query': qr['query'],
            'total_score': total,
            'top_indices': qr.get('top_indices', []),
            'top_scores': qr.get('top_scores', [])
        })
    # Sort and take top 3
    query_totals.sort(key=lambda x: x['total_score'], reverse=True)
    top3_queries = query_totals[:3]

    # Build detailed objects and print
    top_queries_detailed = []
    for rank, q in enumerate(top3_queries, start=1):
        print(f"\n   {rank}. Total Score: {q['total_score']:.4f}")
        print(f"      Query: {q['query']}")
        # Hiá»ƒn thá»‹ thÃ´ng tin question tá»« Excel náº¿u cÃ³
        question_info = {}
        if sample_questions:
            # Find matching question by query text to ensure correct mapping
            query_text = q['query'].strip()
            for sq in sample_questions:
                if sq['primary_query'].strip() == query_text:
                    question_info = {
                        'category': sq.get('full_category', ''),
                        'positive_answer': sq.get('positive', ''),
                        'negative_answer': sq.get('negative', ''),
                        'source_file': sq.get('source_file', '')
                    }
                    print(f"      Category: {question_info['category']}")
                    if question_info['positive_answer']:
                        print(f"      Expected Answer: {question_info['positive_answer'][:100]}...")
                    if question_info['negative_answer']:
                        print(f"      Negative Answer: {question_info['negative_answer'][:100]}...")
                    break

        # Print top 3 results for this query with FULL CONTENT
        top_n = min(3, len(q['top_indices']))
        answers = []
        for i in range(top_n):
            idx = int(q['top_indices'][i])
            score = float(q['top_scores'][i])
            if 0 <= idx < len(law_docs):
                doc = law_docs[idx]
                md = doc.get('metadata') or {}
                citation = md.get('exact_citation') or ''
                law_id = md.get('law_id', '')

                print(f"\n         ğŸ“„ Rank {i+1}: Score {score:.4f} | Law: {law_id}")
                print(f"         ğŸ“ Citation: {citation}")
                print(f"         ğŸ“– Content: {doc['content'][:300]}...")
                if len(doc['content']) > 300:
                    print(f"         ... ({len(doc['content'])} chars total)")

                # collect for JSON
                answers.append({
                    'rank': i+1,
                    'score': score,
                    'citation': citation,
                    'law_id': law_id,
                    'content': doc.get('content', ''),
                    'metadata': md
                })
            else:
                print(f"         - Rank {i+1}: {score:.4f} | [Index {idx} out of range]")
                answers.append({
                    'rank': i+1,
                    'score': score,
                    'citation': '',
                    'law_id': '',
                    'content': '',
                    'metadata': {},
                    'note': f'Index {idx} out of range'
                })
        # ThÃªm thÃ´ng tin question tá»« Excel (Ä‘Ã£ Ä‘Æ°á»£c set á»Ÿ trÃªn)

        top_queries_detailed.append({
            'rank': rank,
            'query_id': q['query_id'],
            'query': q['query'],
            'total_score': q['total_score'],
            'question_info': question_info,
            'top_answers': answers
        })

    # Attach into the best model object for JSON output
    try:
        best_model['top_queries_by_total_score'] = top_queries_detailed
    except Exception:
        pass
    
    # Save summary
    evaluation_summary = {
        'best_model': best_model['model_name'],
        'best_score': best_metrics['avg_max_score'],
        'total_models_evaluated': len(evaluation_results),
        'total_queries': len(benchmark_queries),
        'total_documents': len(law_docs),
        'sorted_results': sorted_results,
        # attach preview for convenience
        'top_queries_preview': [
            {
                'query_id': q['query_id'],
                'query': q['query'],
                'total_score': q['total_score'],
                'top3': [
                    {
                        'doc_index': int(q['top_indices'][i]),
                        'score': float(q['top_scores'][i])
                    } for i in range(min(3, len(q['top_indices'])))
                ]
            } for q in top3_queries
        ],
        'top_queries_by_total_score': top_queries_detailed
    }
    
    print(f"\n{'='*100}")
    print(f"âœ… EVALUATION COMPLETED SUCCESSFULLY!")
    print(f"ğŸ“Š Summary saved to evaluation_summary")
    print(f"ğŸ“‹ Full results available in evaluation_results")
    print(f"{'='*100}")
    
    # Export option
    print(f"\nğŸ’¾ EXPORT OPTIONS:")
    print(f"   To save results to JSON file:")
    print(f"   â†’ import json")
    print(f"   â†’ with open('results/embedding_evaluation_results.json', 'w', encoding='utf-8') as f:")
    print(f"   â†’     json.dump(evaluation_results, f, ensure_ascii=False, indent=2)")
    
    print(f"\nğŸ‰ Report generation completed!")
    
    return evaluation_summary

def test_single_query(best_model_name, models_to_evaluate, law_docs, device="cuda"):
    """Test vá»›i má»™t query Ä‘Æ¡n láº»"""
    print("ğŸ§ª Quick test with single query...")
    
    test_query = "Dá»± Ã¡n Ä‘áº§u tÆ° xÃ¢y dá»±ng khu Ä‘Ã´ thá»‹ pháº£i cÃ³ cÃ´ng nÄƒng há»—n há»£p, Ä‘á»“ng bá»™ háº¡ táº§ng háº¡ táº§ng ká»¹ thuáº­t, háº¡ táº§ng xÃ£ há»™i vÃ  nhÃ  á»Ÿ theo quy hoáº¡ch Ä‘Æ°á»£c phÃª duyá»‡t?"
    print(f"ğŸ“ Test Query: {test_query}")
    print(f"ğŸ¥‡ Using best model: {best_model_name}")
    
    # Find model info
    best_model_info = None
    for model in models_to_evaluate:
        if model['name'] == best_model_name:
            best_model_info = model
            break
    
    if best_model_info:
        print(f"ğŸ” Testing single query with model `{best_model_info['name']}`...")
        
        # Encode query
        if best_model_info['type'] == 'sentence_transformers':
            test_query_embedding = encode_with_sentence_transformers([test_query], best_model_info['name'], device=device)[0]
        else:
            test_query_embedding = encode_with_transformers([test_query], best_model_info['name'], best_model_info['max_length'], device=device)[0]
        
        # Search trá»±c tiáº¿p trÃªn Qdrant (documents Ä‘Ã£ Ä‘Æ°á»£c upsert khi evaluate)
        client = get_qdrant_client()
        collection_name = best_model_info['name'].replace('/', '_')
        top_indices, top_scores = search_qdrant(client, collection_name, test_query_embedding, top_k=10)
        
        # Display results
        display_search_results(test_query, law_docs, top_indices, top_scores, max_display=5)
        
        # Metrics
        metrics = calculate_metrics(top_scores)
        print(f"\nğŸ“Š Metrics for this query:")
        print(f"   - Max Score: {metrics['max_score']:.4f}")
        print(f"   - Top-5 Average: {metrics['avg_top5']:.4f}")
        print(f"   - Results â‰¥ 0.7: {metrics['scores_above_07']}")
        print(f"   - Results â‰¥ 0.5: {metrics['scores_above_05']}")
    else:
        print("âŒ Could not find model info for testing")
    
    print(f"\nâœ… Single query test completed!")

def main():
    """HÃ m chÃ­nh cháº¡y toÃ n bá»™ evaluation"""
    print("=" * 80)
    print("ğŸ”¬ ÄÃNH GIÃ MÃ” HÃŒNH EMBEDDING CHO LUáº¬T TIáº¾NG VIá»†T")
    print("=" * 80)
    
    # 1. Setup environment
    device = setup_environment()
    
    # 2. Load law documents
    law_docs = load_all_law_documents()
    if not law_docs:
        print("âŒ Cannot proceed without law documents!")
        return
    
    # 3. Get models and queries
    models_to_evaluate = get_models_to_evaluate()
    benchmark_queries, sample_questions = load_question_benchmark()  # Get benchmark queries and sample questions

    print(f"\nğŸ¤– Prepared {len(models_to_evaluate)} models for evaluation:")
    for i, model in enumerate(models_to_evaluate):
        print(f"   {i+1}. {model['name']}")
        print(f"      Type: {model['type']} | Max Length: {model['max_length']} tokens")
        print(f"      Description: {model['description']}")
        print()

    print(f"ğŸ¯ All models support â‰¥512 tokens as required!")
    print(f"ğŸ’¾ Embeddings will be stored in Qdrant vector database")

    print(f"\nPrepared {len(benchmark_queries)} benchmark queries from Excel files")
    print("Sample queries:")
    for i, query in enumerate(benchmark_queries[:5]):
        print(f"{i+1}. {query}")
    
    # 4. Run evaluation for all models
    evaluation_results = run_evaluation_all_models(models_to_evaluate, law_docs, benchmark_queries, device)
    
    # 5. Generate final report
    evaluation_summary = generate_final_report(evaluation_results, law_docs, benchmark_queries, sample_questions)
    
    # 6. Test single query with best model
    if evaluation_summary:
        test_single_query(evaluation_summary['best_model'], models_to_evaluate, law_docs, device)
    
    # 7. Save results to JSON
    if evaluation_results:
        try:
            # Save full results
            with open('results/embedding_evaluation_results.json', 'w', encoding='utf-8') as f:
                json.dump(evaluation_results, f, ensure_ascii=False, indent=2)
            print(f"\nğŸ’¾ Full results saved to: results/embedding_evaluation_results.json")

            # Also save clean top queries analysis
            top_queries_summary = []
            for model_result in evaluation_results:
                model_name = model_result['model_name']
                if 'top_queries_by_total_score' in model_result:
                    top_queries_summary.append({
                        "model_name": model_name,
                        "top_queries_by_total_score": model_result['top_queries_by_total_score']
                    })

            if top_queries_summary:
                with open('results/top_queries_analysis.json', 'w', encoding='utf-8') as f:
                    json.dump(top_queries_summary, f, ensure_ascii=False, indent=2)
                print(f"ğŸ’¾ Top queries analysis saved to: results/top_queries_analysis.json")

        except Exception as e:
            print(f"\nâš ï¸ Could not save results to JSON: {e}")
    
    print(f"\nğŸ‰ EVALUATION COMPLETED! ğŸ‰")

if __name__ == "__main__":
    main()
