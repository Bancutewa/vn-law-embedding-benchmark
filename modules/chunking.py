#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Vietnamese Law Document Chunking Module
T√°ch vƒÉn b·∫£n lu·∫≠t th√†nh chunks theo c·∫•u tr√∫c ph√°p ƒëi·ªÉn Vi·ªát Nam
"""

import re
import os
from typing import List, Dict, Any, Optional

# Roman numeral converter
ROMAN_MAP = {'I':1,'V':5,'X':10,'L':50,'C':100,'D':500,'M':1000}

def roman_to_int(s: str) -> Optional[int]:
    """Chuy·ªÉn ƒë·ªïi s·ªë La M√£ sang s·ªë nguy√™n"""
    s = s.upper().strip()
    if not s or any(ch not in ROMAN_MAP for ch in s):
        return None
    total = 0
    prev = 0
    for ch in reversed(s):
        val = ROMAN_MAP[ch]
        if val < prev:
            total -= val
        else:
            total += val
            prev = val
    return total

# ===== Heuristic intro ƒêi·ªÅu d√†nh cho c√°c kho·∫£n =====
INTRO_CUE_PAT = re.compile(r'(sau ƒë√¢y|bao g·ªìm|g·ªìm c√°c|quy ƒë·ªãnh nh∆∞ sau)\s*:\s*$', re.IGNORECASE | re.UNICODE)
def is_intro_text_for_clauses(text: str) -> bool:
    """
    Heuristic: intro ƒêi·ªÅu √°p d·ª•ng cho c√°c kho·∫£n n·∫øu:
      - K·∫øt th√∫c b·∫±ng ':' HO·∫∂C
      - Ch·ª©a c√°c c·ª•m ph·ªï bi·∫øn ('sau ƒë√¢y:', 'bao g·ªìm:', 'quy ƒë·ªãnh nh∆∞ sau:'...)
    """
    if not text:
        return False
    t = text.strip()
    if t.endswith(':'):
        return True
    if INTRO_CUE_PAT.search(t):
        return True
    return False

def chapter_to_int(s: str) -> Optional[int]:
    """Chuy·ªÉn ƒë·ªïi s·ªë Ch∆∞∆°ng (La M√£ ho·∫∑c ·∫¢ R·∫≠p) sang s·ªë nguy√™n"""
    s = s.strip().upper()
    if not s:
        return None

    # Th·ª≠ chuy·ªÉn ƒë·ªïi s·ªë La M√£ tr∆∞·ªõc
    roman_num = roman_to_int(s)
    if roman_num is not None:
        return roman_num

    # N·∫øu kh√¥ng ph·∫£i s·ªë La M√£, th·ª≠ chuy·ªÉn ƒë·ªïi s·ªë ·∫¢ R·∫≠p
    try:
        arabic_num = int(s)
        return arabic_num
    except ValueError:
        return None

def read_docx(file_path: str) -> str:
    """ƒê·ªçc file docx v√† tr·∫£ v·ªÅ text (h·ªó tr·ª£ c·∫£ .doc v√† .docx)"""
    print(f"   Reading file: {file_path}")

    try:
        # Th·ª≠ ƒë·ªçc b·∫±ng python-docx tr∆∞·ªõc (cho file .docx th·ª±c s·ª±)
        from docx import Document
        doc = Document(file_path)
        text = "\n".join((p.text or "").strip() for p in doc.paragraphs)
        print(f"   ‚úÖ Successfully read {len(text):,} characters using python-docx")
        return text
    except Exception as e1:
        print(f"   ‚ö†Ô∏è python-docx failed: {e1}")

        # N·∫øu file c√≥ extension .docx nh∆∞ng th·ª±c t·∫ø l√† .doc, th·ª≠ d√πng docx2txt
        try:
            import docx2txt
            text = docx2txt.process(file_path)
            if text and len(text.strip()) > 0:
                print(f"   ‚úÖ Successfully read {len(text):,} characters using docx2txt")
                return text
            else:
                print(f"   ‚ùå docx2txt returned empty content")
                return ""
        except Exception as e2:
            print(f"   ‚ùå docx2txt also failed: {e2}")
            return ""

def normalize_lines(text: str) -> List[str]:
    """Normalize lines for more robust header matching:
    - NFC unicode normalization (fix decomposed diacritics like `√™ÃÄ`)
    - replace common no-break spaces with normal spaces
    - strip trailing whitespace and remove BOM
    """
    if text is None:
        return []
    # Normalize unicode to composed form so regex matches decorated characters
    import unicodedata
    text = unicodedata.normalize("NFC", text)
    lines = text.splitlines()
    out: List[str] = []
    for ln in lines:
        if ln is None:
            continue
        # Replace common non-breaking / narrow no-break spaces
        ln = ln.replace('\u00A0', ' ').replace('\u202F', ' ').replace('\u2009', ' ')
        # Remove BOM if present
        ln = ln.replace('\ufeff', '')
        # Trim trailing whitespace
        ln = re.sub(r'\s+$', '', ln)
        out.append(ln)
    print(f"   ‚úÖ Normalized {len(out):,} lines with Unicode NFC normalization")
    return out

def generate_law_id(file_name: str) -> str:
    """T·ª± ƒë·ªông sinh law_id t·ª´ t√™n file"""
    # Lo·∫°i b·ªè extension v√† chu·∫©n h√≥a
    name = file_name.replace('.docx', '').replace('.doc', '').strip()

    # T·ª´ ƒëi·ªÉn mapping cho c√°c lo·∫°i lu·∫≠t ph·ªï bi·∫øn
    law_mappings = {
        'kinh doanh b·∫•t ƒë·ªông s·∫£n': 'LKBDS',
        'nh√† ·ªü': 'LNHAO',
        'ƒë·∫•t ƒëai': 'LDATDAI',
        'ƒë·∫ßu t∆∞': 'LDAUTU',
        'ƒë·∫ßu t∆∞ c√¥ng': 'LDAUTUCONG',
        'ƒë·∫ßu t∆∞ theo ph∆∞∆°ng th·ª©c ƒë·ªëi t√°c c√¥ng t∆∞': 'LDAUTUPPPCT',
        'thu·∫ø s·ª≠ d·ª•ng ƒë·∫•t n√¥ng nghi·ªáp': 'LTSDDNONGNGHIEP',
        'thu·∫ø s·ª≠ d·ª•ng ƒë·∫•t phi n√¥ng nghi·ªáp': 'LTSDDPHINONGNGHIEP',
        'x√¢y d·ª±ng': 'LXAYDUNG',
        'h√¥n nh√¢n v√† gia ƒë√¨nh': 'LHNVDG',
    }

    # Chu·∫©n h√≥a t√™n ƒë·ªÉ matching
    name_lower = name.lower()

    # T√¨m mapping ph√π h·ª£p
    for key, value in law_mappings.items():
        if key in name_lower:
            return value

    # X·ª≠ l√Ω c√°c tr∆∞·ªùng h·ª£p ƒë·∫∑c bi·ªát
    if 'lu·∫≠t s·ªë' in name_lower and 'qh' in name_lower:
        # Lu·∫≠t s·ªë XX_YYYY_QHZZ -> LXAYDUNG (lu·∫≠t x√¢y d·ª±ng)
        if 'x√¢y d·ª±ng' in name_lower:
            return 'LXAYDUNG'
        # C√°c lu·∫≠t kh√°c c√≥ th·ªÉ th√™m mapping

    if 'vƒÉn b·∫£n h·ª£p nh·∫•t' in name_lower:
        # VƒÉn b·∫£n h·ª£p nh·∫•t Lu·∫≠t XXX -> LDAUTU
        if 'ƒë·∫ßu t∆∞' in name_lower:
            return 'LDAUTU'

    # X·ª≠ l√Ω c√°c file VBHN (VƒÉn b·∫£n h·ª£p nh·∫•t) c√≥ th·ªÉ l√† lu·∫≠t x√¢y d·ª±ng
    if name_lower.startswith('vbhn') or 'vbhn' in name_lower:
        # Th∆∞·ªùng l√† lu·∫≠t x√¢y d·ª±ng
        return 'LXAYDUNG'

    # X·ª≠ l√Ω lu·∫≠t s·ªë kh√¥ng c√≥ t·ª´ kh√≥a r√µ r√†ng
    if 'lu·∫≠t s·ªë' in name_lower:
        # C√≥ th·ªÉ l√† lu·∫≠t x√¢y d·ª±ng n·∫øu kh√¥ng match g√¨ kh√°c
        return 'LXAYDUNG'

    # N·∫øu kh√¥ng t√¨m th·∫•y, t·∫°o ID t·ª´ ch·ªØ c√°i ƒë·∫ßu c·ªßa c√°c t·ª´ quan tr·ªçng
    words = name.split()

    # L·ªçc b·ªè c√°c t·ª´ kh√¥ng quan tr·ªçng
    stop_words = {'s·ªë', 'v√†', 'theo', 'ph∆∞∆°ng', 'th·ª©c', 'ƒë·ªëi', 't√°c', 'c√¥ng', 't∆∞', 'lu·∫≠t', 'vƒÉn', 'b·∫£n', 'h·ª£p', 'nh·∫•t', 'nƒÉm', 'qƒë', 'tt', 'bh', 'vbh', 'vbhn', 'vpqh'}

    important_words = []
    for word in words:
        word_lower = word.lower()
        # B·ªè qua s·ªë, t·ª´ d·ª´ng, v√† t·ª´ qu√° ng·∫Øn
        if len(word) <= 1 or word_lower in stop_words or word.isdigit():
            continue
        # B·ªè qua c√°c pattern s·ªë nh∆∞ 2020_QH14
        if '_' in word and any(part.isdigit() for part in word.split('_')):
            continue
        important_words.append(word)

    if important_words:
        # L·∫•y ch·ªØ c√°i ƒë·∫ßu c·ªßa 2-4 t·ª´ quan tr·ªçng ƒë·∫ßu ti√™n
        first_letters = ''.join(w[0].upper() for w in important_words[:4])
        result = f"L{first_letters}"
        # Gi·ªõi h·∫°n ƒë·ªô d√†i
        return result[:8]  # T·ªëi ƒëa 8 k√Ω t·ª±

    # Fallback cu·ªëi c√πng
    first_letters = ''.join(w[0].upper() for w in words[:3] if len(w) > 1 and not w.isdigit())
    return f"L{first_letters[:6]}"  # Gi·ªõi h·∫°n 6 k√Ω t·ª±

def chunk_law_document(text: str, law_id: str = "LAW", law_no: str = "", law_title: str = "") -> List[Dict[str, Any]]:
    """Chia vƒÉn b·∫£n lu·∫≠t th√†nh chunks theo ƒë·ªãnh d·∫°ng hn2014_chunks.json"""
    print("   üîç Chunking law document with strict parsing...")

    lines = normalize_lines(text)

    # ===== Regex (ƒë·∫ßu d√≤ng) =====
    ARTICLE_RE = re.compile(r'^ƒêi·ªÅu\s+(\d+)\s*[\.:]?\s*(.*)$', re.UNICODE)
    CHAPTER_RE = re.compile(r'^Ch∆∞∆°ng\s+([IVXLCDM]+|\d+)\s*:?\s*(.*)$', re.UNICODE | re.IGNORECASE)
    CLAUSE_RE = re.compile(r'^\s*(\d+)\.\s*(.*)$', re.UNICODE)  # Kho·∫£n: PH·∫¢I "1."
    POINT_RE  = re.compile(r'^\s*([a-zA-Zƒëƒê])[\)\.]\s+(.*)$', re.UNICODE)  # ƒêi·ªÉm: "a)" ho·∫∑c "a."

    # ===== PASS 1: Pre-scan CH∆Ø∆†NG/ƒêI·ªÄU (ƒë·∫ßu d√≤ng) =====
    chapters_nums, articles_nums = [], []
    chapters_labels, article_titles_seen = [], []
    expecting_chapter_title = False
    roman_current = None

    for raw in lines:
        line = raw
        if not line:
            continue
        if expecting_chapter_title:
            if not (CHAPTER_RE.match(line) or CLAUSE_RE.match(line) or POINT_RE.match(line) or ARTICLE_RE.match(line)):
                ch_title = line.strip()
                lbl = f"Ch∆∞∆°ng {roman_current} ‚Äì {ch_title}"
                chapters_labels[-1] = lbl  # c·∫≠p nh·∫≠t label cu·ªëi
                expecting_chapter_title = False
                continue
            else:
                expecting_chapter_title = False
        m_ch = CHAPTER_RE.match(line)
        if m_ch:
            n = roman_to_int(m_ch.group(1))
            if n:
                chapters_nums.append(n)
                title = (m_ch.group(2) or "").strip()
                lbl = f"Ch∆∞∆°ng {m_ch.group(1).strip()}" + (f" ‚Äì {title}" if title else "")
                chapters_labels.append(lbl)
                if not title:
                    expecting_chapter_title = True
                    roman_current = m_ch.group(1).strip()
                else:
                    expecting_chapter_title = False
            continue
        m_art = ARTICLE_RE.match(line)
        if m_art:
            num = int(m_art.group(1))
            articles_nums.append(num)
            article_titles_seen.append((m_art.group(2) or "").strip())
            continue

    chapters_set, articles_set = set(chapters_nums), set(articles_nums)

    # ===== PASS 2: Chunk strict =====
    chunks = []
    chapter_label: Optional[str] = None
    expecting_chapter_title: bool = False
    roman_current: Optional[str] = None
    chapter_number: Optional[int] = None

    article_no: Optional[int] = None
    article_title: str = ""
    expecting_article_title: bool = False

    article_intro_buf: str = ""
    article_has_any_chunk: bool = False

    clause_no: Optional[int] = None
    clause_buf: str = ""
    # intro c·ªßa kho·∫£n ‚Üí ti√™m v√†o m·ªçi ƒëi·ªÉm c·ªßa kho·∫£n
    clause_intro_current: Optional[str] = None
    # intro c·ªßa ƒëi·ªÅu ‚Üí ti√™m v√†o m·ªçi kho·∫£n c·ªßa ƒëi·ªÅu
    article_clause_intro_current: Optional[str] = None

    in_points: bool = False
    point_letter: Optional[str] = None
    point_buf: str = ""

    expected_chapter: Optional[int] = None
    expected_article: Optional[int] = None
    seeking_article: bool = False

    def build_article_header(article_no: int, article_title: str) -> str:
        t = (article_title or "").strip()
        return f"ƒêi·ªÅu {article_no}" + (f" {t}" if t else "")

    def flush_article_intro():
        nonlocal article_intro_buf, article_has_any_chunk
        content = article_intro_buf.strip()
        if not content:
            return
        cid = f"{law_id}-D{article_no}"
        exact = f"ƒêi·ªÅu {article_no}"
        meta = {
            "law_no": law_no,
            "law_title": law_title,
            "law_id": law_id,
            "chapter": chapter_label,
            "article_no": article_no,
            "article_title": article_title,
            "exact_citation": exact
        }
        title_line = f"ƒêi·ªÅu {article_no}. {article_title}".strip() if article_title else f"ƒêi·ªÅu {article_no}"
        chunks.append({
            "id": cid,
            "content": f"{title_line}\n{content}",
            "metadata": meta
        })
        article_intro_buf = ""
        article_has_any_chunk = True

    def flush_clause():
        nonlocal clause_buf
        content = (clause_buf or "").strip()
        if not content:
            return
        cid = f"{law_id}-D{article_no}-K{clause_no}"
        exact = f"ƒêi·ªÅu {article_no} kho·∫£n {clause_no}"
        meta = {
            "law_no": law_no,
            "law_title": law_title,
            "law_id": law_id,
            "chapter": chapter_label,
            "article_no": article_no,
            "article_title": article_title,
            "clause_no": clause_no,
            "exact_citation": exact,
            "chapter_number": chapter_number
        }
        art_hdr = build_article_header(article_no, article_title)

        # TI√äM intro ƒêi·ªÅu v√†o kho·∫£n (n·∫øu c√≥)
        if article_clause_intro_current:
            intro = article_clause_intro_current.rstrip().rstrip(':') + ':'
            full_content = f"{art_hdr} Kho·∫£n {clause_no}. {intro}\n{content}"
            meta["clause_intro"] = article_clause_intro_current
        else:
            full_content = f"{art_hdr} Kho·∫£n {clause_no}. {content}"

        chunks.append({"id": cid, "content": full_content, "metadata": meta})

    def flush_point():
        nonlocal point_buf
        content = (point_buf or "").strip()
        if not content:
            return
        if clause_intro_current:
            intro = clause_intro_current.rstrip().rstrip(':') + ':'
            content = f"{intro}\n{content}"
        letter = point_letter.lower()
        cid = f"{law_id}-D{article_no}-K{clause_no}-{letter}"
        exact = f"ƒêi·ªÅu {article_no} kho·∫£n {clause_no} ƒëi·ªÉm {letter}."
        meta = {
            "law_no": law_no,
            "law_title": law_title,
            "law_id": law_id,
            "chapter": chapter_label,
            "article_no": article_no,
            "article_title": article_title,
            "clause_no": clause_no,
            "point_letter": letter,
            "exact_citation": exact,
            "chapter_number": chapter_number
        }
        if clause_intro_current:
            meta["clause_intro"] = clause_intro_current
        art_hdr = build_article_header(article_no, article_title)

        if clause_intro_current:
            intro = clause_intro_current.rstrip().rstrip(':')
            full_content = f"{art_hdr} Kho·∫£n {clause_no}. {intro}, ƒëi·ªÉm {letter}.\n{content}"
        else:
            full_content = f"{art_hdr} Kho·∫£n {clause_no}, ƒëi·ªÉm {letter}. {content}"

        chunks.append({"id": cid, "content": full_content, "metadata": meta})

    def close_clause():
        nonlocal clause_no, clause_buf, in_points, point_letter, point_buf, article_has_any_chunk, clause_intro_current
        if clause_no is None:
            return
        if in_points and point_letter:
            flush_point()
        elif clause_buf.strip():
            flush_clause()
        article_has_any_chunk = True
        clause_no, clause_buf, in_points, point_letter, point_buf = None, "", False, None, ""
        clause_intro_current = None

    def close_article_if_needed():
        nonlocal article_intro_buf, article_has_any_chunk
        if not article_has_any_chunk and article_intro_buf.strip():
            flush_article_intro()

    print(f"   üìÑ Processing {len(lines):,} lines...")

    for line in lines:
        if not line:
            continue

        if seeking_article:
            m_art_seek = ARTICLE_RE.match(line)
            if m_art_seek:
                a_no = int(m_art_seek.group(1))
                if a_no == expected_article:
                    seeking_article = False
                    close_clause()
                    if article_no is not None:
                        close_article_if_needed()
                    article_no = a_no
                    article_title = (m_art_seek.group(2) or "").strip()
                    if not article_title:
                        expecting_article_title = True
                    expected_article = a_no + 1
                    clause_no = None
                    clause_buf = ""
                    in_points = False
                    point_letter = None
                    point_buf = ""
                    clause_intro_current = None
                    continue
            continue

        if expecting_article_title:
            if not any(regex.match(line) for regex in [CHAPTER_RE, CLAUSE_RE, POINT_RE, ARTICLE_RE]):
                article_title = line
                expecting_article_title = False
                continue
            else:
                expecting_article_title = False

        if expecting_chapter_title:
            if not (CHAPTER_RE.match(line) or CLAUSE_RE.match(line) or POINT_RE.match(line) or ARTICLE_RE.match(line)):
                ch_title = line.strip()
                lbl = f"Ch∆∞∆°ng {roman_current} ‚Äì {ch_title}"
                chapter_label = lbl
                expecting_chapter_title = False
                continue
            else:
                expecting_chapter_title = False

        # CH∆Ø∆†NG
        m_ch = CHAPTER_RE.match(line)
        if m_ch:
            close_clause()
            if article_no is not None:
                close_article_if_needed()
            article_no = None
            article_title = ""
            article_intro_buf = ""
            expecting_article_title = False
            article_clause_intro_current = None

            roman = m_ch.group(1).strip()
            ch_num = roman_to_int(roman) or 0
            ch_title = (m_ch.group(2) or "").strip()
            lbl = f"Ch∆∞∆°ng {roman}" + (f" ‚Äì {ch_title}" if ch_title else "")

            if not ch_title:
                expecting_chapter_title = True
                roman_current = roman  # l∆∞u l·∫°i ƒë·ªÉ d√πng sau
                chapter_number = ch_num  # l∆∞u chapter_number
            else:
                chapter_label = lbl
                chapter_number = ch_num  # l∆∞u chapter_number
                expecting_chapter_title = False

            if expected_chapter is None:
                expected_chapter = ch_num + 1
            else:
                if ch_num == expected_chapter:
                    expected_chapter = ch_num + 1
                elif ch_num > expected_chapter:
                    if expected_chapter not in chapters_set:
                        break
                    continue
                else:
                    continue

            continue

        # ƒêI·ªÄU
        m_art = ARTICLE_RE.match(line)
        if m_art:
            a_no = int(m_art.group(1))
            a_title = (m_art.group(2) or "").strip()

            if expected_article is None:
                expected_article = a_no + 1
                close_clause()
                if article_no is not None:
                    close_article_if_needed()
                article_no = a_no
                article_title = a_title
                if not article_title:
                    expecting_article_title = True
                clause_no = None
                clause_buf = ""
                in_points = False
                point_letter = None
                point_buf = ""
                clause_intro_current = None
                article_clause_intro_current = None
                continue
            elif a_no == expected_article:
                expected_article = a_no + 1
                close_clause()
                if article_no is not None:
                    close_article_if_needed()
                article_no = a_no
                article_title = a_title
                if not article_title:
                    expecting_article_title = True
                clause_no = None
                clause_buf = ""
                in_points = False
                point_letter = None
                point_buf = ""
                clause_intro_current = None
                article_clause_intro_current = None
                continue
            elif a_no > expected_article:
                if expected_article not in articles_set:
                    break
                else:
                    seeking_article = True
                    continue
            else:
                continue

        if article_no is None:
            continue

        # KHO·∫¢N
        m_k = CLAUSE_RE.match(line)
        if m_k and m_k.group(1).isdigit():
            # N·∫øu ƒêi·ªÅu c√≥ intro 'gi·ªõi thi·ªáu' ‚Üí l∆∞u l·∫°i ƒë·ªÉ ti√™m cho m·ªçi kho·∫£n, KH√îNG flush chunk intro ƒêi·ªÅu
            if article_intro_buf.strip() and is_intro_text_for_clauses(article_intro_buf):
                article_clause_intro_current = article_intro_buf.strip()
                article_intro_buf = ""
            elif article_intro_buf.strip():
                # Intro kh√¥ng ph·∫£i d·∫°ng 'gi·ªõi thi·ªáu' ‚Üí t·∫°o chunk ƒêi·ªÅu intro nh∆∞ c≈©
                flush_article_intro()
                article_has_any_chunk = True

            close_clause()
            clause_no = int(m_k.group(1))
            clause_buf = (m_k.group(2) or "").strip()
            in_points = False
            point_letter = None
            point_buf = ""
            clause_intro_current = None
            continue

        # ƒêI·ªÇM ‚Äî chu·ªói ƒëi·ªÉm ch·ªâ b·∫Øt ƒë·∫ßu n·∫øu m·ªü b·∫±ng a)
        m_p = POINT_RE.match(line)
        if m_p and clause_no is not None:
            letter = m_p.group(1).lower()
            text = (m_p.group(2) or "").strip()

            if not in_points:
                if letter != 'a':
                    # kh√¥ng coi l√† ƒëi·ªÉm ‚Üí nh·∫≠p v√†o n·ªôi dung kho·∫£n
                    clause_buf += ("\n" if clause_buf else "") + f"{letter}. {text}"
                    continue
                # b·∫Øt ƒë·∫ßu chu·ªói ƒëi·ªÉm v·ªõi 'a)' ‚Äî KH√îNG flush chunk "Kho·∫£n X"
                # l∆∞u intro kho·∫£n ƒë·ªÉ ti√™m v√†o M·ªåI ƒëi·ªÉm
                clause_intro_current = clause_buf.strip() if clause_buf.strip() else None
                clause_buf = ""
                in_points = True
                point_letter = letter
                point_buf = text
                continue

            # ƒëang trong chu·ªói ƒëi·ªÉm: flush ƒëi·ªÉm tr∆∞·ªõc, m·ªü ƒëi·ªÉm m·ªõi
            if point_letter:
                flush_point()
            in_points = True
            point_letter = letter
            point_buf = text
            continue

        # N·ªôi dung k√©o d√†i
        if clause_no is not None:
            if in_points and point_letter:
                point_buf += ("\n" if point_buf else "") + line
            else:
                clause_buf += ("\n" if clause_buf else "") + line
        else:
            article_intro_buf += ("\n" if article_intro_buf else "") + line

    # K·∫øt th√∫c
    close_clause()
    if article_no is not None:
        close_article_if_needed()

    # Filter chunks
    valid_chunks = []
    for chunk in chunks:
        content = chunk['content'].strip()
        if len(content) > 50:
            valid_chunks.append(chunk)

    print(f"   ‚úÖ Created {len(valid_chunks)} chunks")
    return valid_chunks
