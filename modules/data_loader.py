#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Data Loading Module
Load vÃ  xá»­ lÃ½ documents vÃ  benchmark queries
"""

import json
import os
import sys
from typing import List, Dict, Any, Tuple, Optional
from tqdm import tqdm

from .chunking import read_docx, chunk_law_document, generate_law_id

def load_all_law_documents() -> List[Dict[str, Any]]:
    """Load vÃ  chunk táº¥t cáº£ vÄƒn báº£n luáº­t tá»« thÆ° má»¥c law_content"""
    print("ğŸ“š Loading ALL law documents from law_content folder...")

    # Load danh sÃ¡ch file tá»« JSON
    law_file_paths = []
    try:
        with open("data_files/law_file_paths.json", 'r', encoding='utf-8') as f:
            law_file_paths = json.load(f)
        print(f"âœ… Loaded {len(law_file_paths)} law file paths from JSON")
    except FileNotFoundError:
        print("âŒ data_files/law_file_paths.json not found! Please run find_law_files.py first.")
        print("ğŸ”„ Running find_law_files.py to generate the file...")
        try:
            import subprocess
            result = subprocess.run([sys.executable, "find_law_files.py"],
                                  capture_output=True, text=True, cwd=os.getcwd())
            if result.returncode == 0:
                print("âœ… Successfully generated data_files/law_file_paths.json")
                with open("data_files/law_file_paths.json", 'r', encoding='utf-8') as f:
                    law_file_paths = json.load(f)
                print(f"âœ… Loaded {len(law_file_paths)} law file paths from generated JSON")
            else:
                print(f"âŒ Failed to generate data_files/law_file_paths.json: {result.stderr}")
                return []
        except Exception as e:
            print(f"âŒ Error running find_law_files.py: {e}")
            return []
    except Exception as e:
        print(f"âŒ Error loading data_files/law_file_paths.json: {e}")
        return []

    all_law_docs = []
    successful_files = 0
    failed_files = 0

    print(f"\nğŸ”„ Processing {len(law_file_paths)} law files...")

    for i, file_info in enumerate(tqdm(law_file_paths, desc="Processing law files")):
        file_path = file_info['path']
        category = file_info['category']
        file_name = file_info['file_name']

        print(f"\nğŸ“„ [{i+1}/{len(law_file_paths)}] Processing: {file_name}")
        print(f"   Category: {category}")
        print(f"   Path: {file_path}")

        if not os.path.exists(file_path):
            print(f"   âŒ File not found: {file_path}")
            failed_files += 1
            continue

        try:
            # BÆ°á»›c 1: Äá»c file
            print(f"   ğŸ“– Reading file...")
            law_text = read_docx(file_path)  # HÃ m nÃ y Ä‘Ã£ xá»­ lÃ½ cáº£ .doc vÃ  .docx

            if not law_text or len(law_text.strip()) < 100:
                print(f"   âš ï¸ File seems empty or too short: {len(law_text)} characters")
                print(f"   â­ï¸ Skipping file (cannot read content)")
                failed_files += 1
                continue

            # BÆ°á»›c 2: Chia thÃ nh chunks
            print(f"   ğŸ”¨ Chunking document...")
            # Táº¡o law_id tá»± Ä‘á»™ng tá»« tÃªn file
            law_id = generate_law_id(file_name)
            print(f"   ğŸ“‹ Generated law_id: {law_id}")
            law_chunks = chunk_law_document(law_text, law_id=law_id, law_no="", law_title=file_name)

            if not law_chunks:
                print(f"   âš ï¸ No chunks created from file")
                failed_files += 1
                continue

            # BÆ°á»›c 3: Chuáº©n bá»‹ dá»¯ liá»‡u cho Ä‘Ã¡nh giÃ¡
            print(f"   ğŸ—‚ï¸ Preparing chunks...")
            for j, chunk in enumerate(law_chunks):
                # ThÃªm thÃ´ng tin vá» file gá»‘c vÃ o metadata
                chunk_metadata = chunk.get('metadata', {}).copy()
                chunk_metadata.update({
                    'source_file': file_path,
                    'source_category': category,
                    'source_file_name': file_name,
                    'chunk_index': j
                })

                # Táº¡o document theo format hn2014_chunks.json
                all_law_docs.append({
                    'id': chunk['id'],
                    'content': chunk['content'],
                    'metadata': chunk_metadata
                })

            print(f"   âœ… Successfully processed {len(law_chunks)} chunks")
            successful_files += 1

        except Exception as e:
            print(f"   âŒ Error processing file: {e}")
            failed_files += 1
            continue

    print(f"\nğŸ“Š Processing Summary:")
    print(f"   âœ… Successfully processed: {successful_files} files")
    print(f"   âŒ Failed to process: {failed_files} files")
    print(f"   ğŸ“„ Total chunks created: {len(all_law_docs)}")

    if all_law_docs:
        import numpy as np
        print(f"   ğŸ“Š Average chunk length: {np.mean([len(doc['content']) for doc in all_law_docs]):.0f} characters")

        # Thá»‘ng kÃª theo category
        category_counts = {}
        for doc in all_law_docs:
            doc_category = doc.get('metadata', {}).get('source_category', 'Unknown')
            category_counts[doc_category] = category_counts.get(doc_category, 0) + 1

        print(f"\nğŸ“ˆ Chunk distribution by category:")
        for category, count in category_counts.items():
            print(f"   - {category}: {count} chunks")

    print(f"\nâœ… Successfully loaded {len(all_law_docs)} law document chunks from {successful_files} files!")
    return all_law_docs

def load_question_benchmark(random_sample: int = 50) -> Tuple[List[str], List[Dict[str, Any]]]:
    """Load cÃ¡c cÃ¢u há»i tá»« file Excel Ä‘á»ƒ lÃ m benchmark queries"""
    try:
        with open("data_files/law_questions.json", 'r', encoding='utf-8') as f:
            questions = json.load(f)

        print(f"âœ… Loaded {len(questions)} benchmark questions from Excel files")

        # Random sample 50 questions
        if len(questions) > random_sample:
            import random
            questions_sample = random.sample(questions, random_sample)
            print(f"ğŸ² Randomly selected {random_sample} questions for evaluation")
        else:
            questions_sample = questions
            print(f"ğŸ“Š Using all {len(questions)} questions (less than {random_sample})")

        # Tráº£ vá» list cÃ¡c query (chá»‰ láº¥y primary_query)
        queries = [q['primary_query'] for q in questions_sample]

        # Thá»‘ng kÃª theo category
        category_stats = {}
        for q in questions_sample:
            cat = q.get('full_category', 'Unknown')
            category_stats[cat] = category_stats.get(cat, 0) + 1

        print(f"ğŸ“Š Sample distribution by category:")
        for cat, count in sorted(category_stats.items()):
            print(f"   - {cat}: {count} questions")

        return queries, questions_sample

    except FileNotFoundError:
        print("âš ï¸ data_files/law_questions.json not found. Using default queries.")
        return get_default_benchmark_queries(), []
    except Exception as e:
        print(f"âš ï¸ Error loading questions: {e}. Using default queries.")
        return get_default_benchmark_queries(), []

def get_default_benchmark_queries() -> List[str]:
    """CÃ¡c query máº·c Ä‘á»‹nh náº¿u khÃ´ng cÃ³ file questions"""
    return [
        "NgÆ°á»i bá»‹ bá»‡nh tÃ¢m tháº§n lÃ  ngÆ°á»i máº¥t nÄƒng lá»±c hÃ nh vi dÃ¢n sá»± lÃ  Ä‘Ãºng hay sai?",
        "Sá»± thá»a thuáº­n cá»§a cÃ¡c bÃªn khÃ´ng vi pháº¡m Ä‘iá»u cáº¥m cá»§a phÃ¡p luáº­t, khÃ´ng trÃ¡i Ä‘áº¡o Ä‘á»©c xÃ£ há»™i thÃ¬ Ä‘Æ°á»£c gá»i lÃ  há»£p Ä‘á»“ng lÃ  Ä‘Ãºng hay sai?",
        "Tiá»n phÃºng viáº¿ng Ä‘Ã¡m ma cÅ©ng thuá»™c di sáº£n thá»«a káº¿ cá»§a ngÆ°á»i cháº¿t Ä‘á»ƒ láº¡i lÃ  Ä‘Ãºng hay sai?",
        "CÃ¡ nhÃ¢n Ä‘Æ°á»£c hÆ°á»Ÿng di sáº£n thá»«a káº¿ pháº£i tráº£ ná»£ thay cho ngÆ°á»i Ä‘á»ƒ láº¡i di sáº£n lÃ  Ä‘Ãºng hay sai?",
        "Há»£p Ä‘á»“ng vÃ´ hiá»‡u lÃ  há»£p Ä‘á»“ng vi pháº¡m phÃ¡p luáº­t lÃ  Ä‘Ãºng hay sai?",
        "Hoa lá»£i, lá»£i tá»©c phÃ¡t sinh tá»« tÃ i sáº£n riÃªng cá»§a má»™t bÃªn vá»£ hoáº·c chá»“ng sáº½ lÃ  tÃ i sáº£n chung náº¿u hoa lá»£i, lá»£i tá»©c Ä‘Ã³ lÃ  nguá»“n sá»‘ng duy nháº¥t cá»§a gia Ä‘Ã¬nh.",
        "Há»™i LiÃªn hiá»‡p phá»¥ ná»¯ cÃ³ quyá»n yÃªu cáº§u TÃ²a Ã¡n ra quyáº¿t Ä‘á»‹nh há»§y káº¿t hÃ´n trÃ¡i phÃ¡p luáº­t do vi pháº¡m sá»± tá»± nguyá»‡n.",
        "HÃ´n nhÃ¢n chá»‰ cháº¥m dá»©t khi má»™t bÃªn vá»£, chá»“ng cháº¿t.",
        "Káº¿t hÃ´n cÃ³ yáº¿u tá»‘ nÆ°á»›c ngoÃ i cÃ³ thá»ƒ Ä‘Äƒng kÃ½ táº¡i UBND cáº¥p xÃ£.",
        "Khi cha máº¹ khÃ´ng thá»ƒ nuÃ´i dÆ°á»¡ng, cáº¥p dÆ°á»¡ng Ä‘Æ°á»£c cho con, thÃ¬ Ã´ng bÃ  pháº£i cÃ³ nghÄ©a vá»¥ nuÃ´i dÆ°á»¡ng hoáº·c cáº¥p dÆ°á»¡ng cho chÃ¡u.",
        "Khi hÃ´n nhÃ¢n cháº¥m dá»©t, má»i quyá»n vÃ  nghÄ©a vá»¥ giá»¯a nhá»¯ng ngÆ°á»i Ä‘Ã£ tá»«ng lÃ  vá»£ chá»“ng cÅ©ng cháº¥m dá»©t.",
        "Khi vá»£ chá»“ng ly hÃ´n, con dÆ°á»›i 36 thÃ¡ng tuá»•i Ä‘Æ°á»£c giao cho ngÆ°á»i vá»£ trá»±c tiáº¿p nuÃ´i dÆ°á»¡ng.",
        "Khi vá»£ hoáº·c chá»“ng thá»±c hiá»‡n nhá»¯ng giao dá»‹ch phá»¥c vá»¥ cho nhu cáº§u thiáº¿t yáº¿u cá»§a gia Ä‘Ã¬nh mÃ  khÃ´ng cÃ³ sá»± Ä‘á»“ng Ã½ cá»§a bÃªn kia thÃ¬ ngÆ°á»i thá»±c hiá»‡n giao dá»‹ch Ä‘Ã³ pháº£i thanh toÃ¡n báº±ng tÃ i sáº£n riÃªng cá»§a mÃ¬nh.",
        "Khi khÃ´ng sá»‘ng chung cÃ¹ng vá»›i cha máº¹, con Ä‘Ã£ thÃ nh niÃªn cÃ³ kháº£ nÄƒng lao Ä‘á»™ng pháº£i cáº¥p dÆ°á»¡ng cho cha máº¹.",
        "Chá»‰ UBND cáº¥p tá»‰nh nÆ¡i cÃ´ng dÃ¢n Viá»‡t Nam cÆ° trÃº má»›i cÃ³ tháº©m quyá»n Ä‘Äƒng kÃ½ viá»‡c káº¿t hÃ´n giá»¯a cÃ´ng dÃ¢n Viá»‡t Nam vá»›i ngÆ°á»i nÆ°á»›c ngoÃ i.",
        "Äiá»u kiá»‡n káº¿t hÃ´n cá»§a nam vÃ  ná»¯ theo phÃ¡p luáº­t Viá»‡t Nam lÃ  gÃ¬?",
        "TÃ i sáº£n nÃ o Ä‘Æ°á»£c coi lÃ  tÃ i sáº£n chung cá»§a vá»£ chá»“ng?",
        "Thá»§ tá»¥c ly hÃ´n táº¡i tÃ²a Ã¡n Ä‘Æ°á»£c quy Ä‘á»‹nh nhÆ° tháº¿ nÃ o?",
        "Quyá»n vÃ  nghÄ©a vá»¥ cá»§a cha máº¹ Ä‘á»‘i vá»›i con chÆ°a thÃ nh niÃªn?",
        "TrÆ°á»ng há»£p nÃ o vá»£ chá»“ng cÃ³ thá»ƒ thá»a thuáº­n vá» cháº¿ Ä‘á»™ tÃ i sáº£n?"
    ]

def save_chunks_to_json(chunks: List[Dict[str, Any]], output_path: str) -> None:
    """LÆ°u chunks vÃ o file JSON"""
    safe_path = output_path.encode('ascii', 'ignore').decode('ascii') or output_path
    print(f"Saving {len(chunks)} chunks to {safe_path}")
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(chunks, f, ensure_ascii=False, indent=2)
    safe_path = output_path.encode('ascii', 'ignore').decode('ascii') or output_path
    print(f"Saved {len(chunks)} chunks to {safe_path}")

def load_chunks_from_json(input_path: str) -> List[Dict[str, Any]]:
    """Load chunks tá»« file JSON"""
    safe_path = input_path.encode('ascii', 'ignore').decode('ascii') or input_path
    print(f"Loading chunks from {safe_path}")
    with open(input_path, 'r', encoding='utf-8') as f:
        chunks = json.load(f)
    safe_path = input_path.encode('ascii', 'ignore').decode('ascii') or input_path
    print(f"Loaded {len(chunks)} chunks from {safe_path}")
    return chunks
