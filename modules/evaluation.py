#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Evaluation Module
Logic Ä‘Ã¡nh giÃ¡ cháº¥t lÆ°á»£ng cÃ¡c embedding models
"""

import numpy as np
from typing import List, Dict, Any, Optional
from tqdm import tqdm

from .qdrant_manager import get_qdrant_client, ensure_collection, upsert_embeddings_to_qdrant, search_qdrant, count_collection_points
from .embedding_models import encode_texts, get_embedding_dimension
from .data_loader import display_search_results

def calculate_metrics(scores: np.ndarray, threshold_07: float = 0.7, threshold_05: float = 0.5) -> Dict[str, float]:
    """TÃ­nh toÃ¡n cÃ¡c metrics Ä‘Ã¡nh giÃ¡ cháº¥t lÆ°á»£ng retrieval"""
    return {
        "max_score": float(np.max(scores)),
        "avg_top3": float(np.mean(scores[:3])) if len(scores) >= 3 else float(np.mean(scores)),
        "avg_top5": float(np.mean(scores[:5])) if len(scores) >= 5 else float(np.mean(scores)),
        "avg_top10": float(np.mean(scores[:10])) if len(scores) >= 10 else float(np.mean(scores)),
        "avg_all": float(np.mean(scores)),
        "scores_above_07": int(np.sum(scores >= threshold_07)),
        "scores_above_05": int(np.sum(scores >= threshold_05)),
        "min_score": float(np.min(scores))
    }

def evaluate_single_model(model_info: Dict[str, Any], law_docs: List[Dict[str, Any]], queries: List[str], top_k: int = 15, show_detailed_results: bool = True, device: str = "cuda") -> Dict[str, Any]:
    """ÄÃ¡nh giÃ¡ má»™t mÃ´ hÃ¬nh embedding"""
    print(f"\n{'='*80}")
    print(f"ğŸ” EVALUATING MODEL: {model_info['name']}")
    print(f"ğŸ“ Description: {model_info['description']}")
    print(f"ğŸ”§ Type: {model_info['type']} | Max Length: {model_info['max_length']} tokens")
    print(f"{'='*80}")

    try:
        # BÆ°á»›c 1: Chuáº©n bá»‹ texts
        doc_texts = [doc['content'] for doc in law_docs]
        print(f"\nğŸ“š Step 1: Prepared {len(doc_texts)} document texts")

        # BÆ°á»›c 2: Kiá»ƒm tra Qdrant collection trÆ°á»›c
        client = get_qdrant_client()
        collection_name = model_info['name'].replace('/', '_')
        existing = count_collection_points(client, collection_name)

        if existing >= len(law_docs):
            print(f"ğŸŸ¡ Collection '{collection_name}' already has {existing} vectors (>= {len(law_docs)}). Skipping document encoding.")
        else:
            print(f"ğŸŸ  Collection '{collection_name}' has {existing} vectors. Need to encode and upsert {len(law_docs)} vectors...")

            # BÆ°á»›c 2a: Encode documents
            print(f"\nğŸ”¨ Step 2a: Encoding documents...")
            doc_embeddings = encode_texts(doc_texts, model_info, device=device)
            print(f"   âœ… Document embeddings shape: {doc_embeddings.shape}")

            # BÆ°á»›c 2b: LÆ°u vÃ o Qdrant
            print(f"\nğŸ’¾ Step 2b: Storing embeddings in Qdrant...")
            vector_size = doc_embeddings.shape[1]
            ensure_collection(client, collection_name, vector_size)
            upsert_embeddings_to_qdrant(client, collection_name, doc_embeddings, law_docs)

            # Giáº£i phÃ³ng RAM
            del doc_embeddings

        # BÆ°á»›c 3: Encode queries
        print(f"\nğŸ” Step 3: Encoding queries...")
        query_embeddings = encode_texts(queries, model_info, device=device)
        print(f"   âœ… Query embeddings shape: {query_embeddings.shape}")

        # BÆ°á»›c 4: Evaluate tá»«ng query
        print(f"\nğŸ“Š Step 4: Evaluating {len(queries)} queries...")
        query_results = []
        all_metrics = []

        for i, query in enumerate(queries):
            print(f"\n   ğŸ” Query {i+1}/{len(queries)}")

            # Search top-k documents via Qdrant
            top_indices, top_scores = search_qdrant(
                client=client,
                collection_name=collection_name,
                query_embedding=query_embeddings[i],
                top_k=top_k
            )

            # Calculate metrics
            metrics = calculate_metrics(top_scores)
            all_metrics.append(metrics)

            # Store results
            query_result = {
                'query': query,
                'query_id': i,
                'top_indices': top_indices.tolist(),
                'top_scores': top_scores.tolist(),
                'metrics': metrics
            }
            query_results.append(query_result)

            # Show detailed results for first few queries
            if show_detailed_results and i < 3:
                display_search_results(query, law_docs, top_indices, top_scores, max_display=3)
                print(f"      ğŸ“ˆ Metrics: Max={metrics['max_score']:.4f}, Avg_top5={metrics['avg_top5']:.4f}, Above_0.7={metrics['scores_above_07']}")

        # BÆ°á»›c 5: Aggregate metrics
        print(f"\nğŸ“ˆ Step 5: Aggregating metrics...")

        # Calculate average metrics across all queries
        avg_metrics = {}
        metric_keys = all_metrics[0].keys()
        for key in metric_keys:
            if key.startswith('scores_above'):
                avg_metrics[f"avg_{key}"] = np.mean([m[key] for m in all_metrics])
            else:
                avg_metrics[f"avg_{key}"] = np.mean([m[key] for m in all_metrics])

        # Final result
        final_result = {
            'model_name': model_info['name'],
            'model_type': model_info['type'],
            'model_description': model_info['description'],
            'max_length': model_info['max_length'],
            'num_queries': len(queries),
            'num_documents': len(law_docs),
            'top_k': top_k,
            'query_results': query_results,
            'aggregated_metrics': avg_metrics,
            'evaluation_success': True
        }

        # Print summary
        print(f"\nâœ… EVALUATION COMPLETED SUCCESSFULLY!")
        print(f"   ğŸ“Š Average Results:")
        print(f"      - Avg Max Score: {avg_metrics['avg_max_score']:.4f}")
        print(f"      - Avg Top-5 Score: {avg_metrics['avg_avg_top5']:.4f}")
        print(f"      - Avg Above 0.7: {avg_metrics['avg_scores_above_07']:.1f}")
        print(f"      - Avg Above 0.5: {avg_metrics['avg_scores_above_05']:.1f}")

        return final_result

    except Exception as e:
        print(f"\nâŒ EVALUATION FAILED: {str(e)}")
        return {
            'model_name': model_info['name'],
            'model_type': model_info['type'],
            'error': str(e),
            'evaluation_success': False
        }

def run_evaluation_all_models(models_to_evaluate: List[Dict[str, Any]], law_docs: List[Dict[str, Any]], benchmark_queries: List[str], device: str = "cuda") -> List[Dict[str, Any]]:
    """Cháº¡y Ä‘Ã¡nh giÃ¡ cho táº¥t cáº£ cÃ¡c mÃ´ hÃ¬nh"""
    print("ğŸš€ Starting evaluation for all models...")

    if not law_docs:
        print("âŒ Error: No law documents loaded!")
        return []

    print(f"âœ… Ready to evaluate with:")
    print(f"   ğŸ“š Documents: {len(law_docs)} law chunks")
    print(f"   â“ Queries: {len(benchmark_queries)} benchmark questions")
    print(f"   ğŸ¤– Models: {len(models_to_evaluate)} models to test")
    print(f"   ğŸ¯ Top-K: 15 results per query")

    evaluation_results = []
    successful_evaluations = 0
    failed_evaluations = 0

    for i, model_info in enumerate(models_to_evaluate):
        print(f"\n{'ğŸ¤– '*20}")
        print(f"ğŸ¤– EVALUATING MODEL {i+1}/{len(models_to_evaluate)}")
        print(f"{'ğŸ¤– '*20}")

        try:
            result = evaluate_single_model(
                model_info=model_info,
                law_docs=law_docs,
                queries=benchmark_queries,
                top_k=15,
                show_detailed_results=(i < 2),  # Show details for first 2 models only
                device=device
            )

            if result['evaluation_success']:
                evaluation_results.append(result)
                successful_evaluations += 1
                print(f"âœ… Model {i+1} evaluation completed successfully!")
            else:
                print(f"âŒ Model {i+1} evaluation failed: {result.get('error', 'Unknown error')}")
                failed_evaluations += 1

            # Wait between models
            if i < len(models_to_evaluate) - 1:
                print(f"â³ Waiting 2 seconds before next model...")
                import time
                time.sleep(2)

        except Exception as e:
            print(f"âŒ Unexpected error evaluating model {i+1}: {str(e)}")
            failed_evaluations += 1
            continue

    # Final summary
    print(f"\n{'='*100}")
    print(f"ğŸ‰ EVALUATION SUMMARY")
    print(f"{'='*100}")
    print(f"âœ… Successful evaluations: {successful_evaluations}")
    print(f"âŒ Failed evaluations: {failed_evaluations}")
    print(f"ğŸ“Š Total models evaluated: {len(evaluation_results)}")

    if evaluation_results:
        print(f"\nğŸ“ˆ Quick Performance Preview:")
        # Sáº¯p xáº¿p theo avg_max_score giáº£m dáº§n
        preview_sorted = sorted(
            evaluation_results,
            key=lambda r: r['aggregated_metrics']['avg_max_score'],
            reverse=True
        )
        for result in preview_sorted:
            metrics = result['aggregated_metrics']
            model_name = result['model_name'].split('/')[-1]
            print(f"   ğŸ¤– {model_name}")
            print(f"      Max Score: {metrics['avg_max_score']:.4f} | Top-5: {metrics['avg_avg_top5']:.4f} | Above 0.7: {metrics['avg_scores_above_07']:.1f}")

    return evaluation_results
